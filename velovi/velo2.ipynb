{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chels/anaconda3/envs/thesis/lib/python3.8/site-packages/scvi_colab/_core.py:41: UserWarning: \n",
      "                Not currently in Google Colab environment.\n",
      "\n",
      "                Please run with `run_outside_colab=True` to override.\n",
      "\n",
      "                Returning with no further action.\n",
      "                \n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet scvi-colab\n",
    "from scvi_colab import install\n",
    "install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/home/chels/anaconda3/envs/thesis/lib/python3.8/site-packages/pytorch_lightning/utilities/warnings.py:53: LightningDeprecationWarning: pytorch_lightning.utilities.warnings.rank_zero_deprecation has been deprecated in v1.6 and will be removed in v1.8. Use the equivalent function from the pytorch_lightning.utilities.rank_zero module instead.\n",
      "  new_rank_zero_deprecation(\n",
      "/home/chels/anaconda3/envs/thesis/lib/python3.8/site-packages/pytorch_lightning/utilities/warnings.py:58: LightningDeprecationWarning: The `pytorch_lightning.loggers.base.rank_zero_experiment` is deprecated in v1.7 and will be removed in v1.9. Please use `pytorch_lightning.loggers.logger.rank_zero_experiment` instead.\n",
      "  return new_rank_zero_deprecation(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scvelo as scv\n",
    "import torch\n",
    "from velovi import preprocess_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Main module.\"\"\"\n",
    "from typing import Callable, Iterable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scvi._compat import Literal\n",
    "from scvi.module.base import BaseModuleClass, LossRecorder, auto_move_data\n",
    "from scvi.nn import Encoder, FCLayers\n",
    "from torch import nn as nn\n",
    "from torch.distributions import Categorical, Dirichlet, MixtureSameFamily, Normal\n",
    "from torch.distributions import kl_divergence as kl\n",
    "from scvi.distributions import NegativeBinomial\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "from functools import partial\n",
    "from typing import Iterable, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "from anndata import AnnData\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import ttest_ind\n",
    "from scvi._compat import Literal\n",
    "from scvi._utils import _doc_params\n",
    "from scvi.data import AnnDataManager\n",
    "from scvi.data.fields import LayerField\n",
    "from scvi.dataloaders import AnnDataLoader, DataSplitter\n",
    "from scvi.model._utils import scrna_raw_counts_properties\n",
    "from scvi.model.base import BaseModelClass, UnsupervisedTrainingMixin, VAEMixin\n",
    "from scvi.model.base._utils import _de_core\n",
    "from scvi.train import TrainingPlan, TrainRunner\n",
    "from scvi.utils._docstrings import doc_differential_expression, setup_anndata_dsp\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from velovi import REGISTRY_KEYS\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def one_hot_encoder(idx, n_cls):\n",
    "    assert torch.max(idx).item() < n_cls\n",
    "    if idx.dim() == 1:\n",
    "        idx = idx.unsqueeze(1)\n",
    "    onehot = torch.zeros(idx.size(0), n_cls)\n",
    "    onehot = onehot.to(idx.device)\n",
    "    onehot.scatter_(1, idx.long(), 1)\n",
    "    return onehot\n",
    "\n",
    "class MaskedLinear(nn.Linear):\n",
    "    def __init__(self, n_in,  n_out, mask, bias=True):\n",
    "        # mask should have the same dimensions as the transposed linear weight\n",
    "        # n_input x n_output_nodes\n",
    "        if n_in != mask.shape[0] or n_out != mask.shape[1]:\n",
    "            raise ValueError('Incorrect shape of the mask.')\n",
    "\n",
    "        super().__init__(n_in, n_out, bias)\n",
    "\n",
    "        self.register_buffer('mask', mask.t())\n",
    "\n",
    "        # zero out the weights for group lasso\n",
    "        # gradient descent won't change these zero weights\n",
    "        self.weight.data*=self.mask\n",
    "\n",
    "    def forward(self, input):\n",
    "        return nn.functional.linear(input, self.weight*self.mask, self.bias)\n",
    "\n",
    "class MaskedCondLayers(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in: int,\n",
    "        n_out: int,\n",
    "        n_cond: int,\n",
    "        bias: bool,\n",
    "        n_ext: int = 0,\n",
    "        n_ext_m: int = 0,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        ext_mask: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_cond = n_cond\n",
    "        self.n_ext = n_ext\n",
    "        self.n_ext_m = n_ext_m\n",
    "\n",
    "        self.expr_L = nn.Linear(n_in, n_out, bias=bias)\n",
    "\n",
    "        # if mask is None:\n",
    "        #     self.expr_L = nn.Linear(n_in, n_out, bias=bias)\n",
    "        # else:\n",
    "        #     self.expr_L = MaskedLinear(n_in, n_out, mask, bias=bias)\n",
    "\n",
    "        # if self.n_cond != 0:\n",
    "        #     self.cond_L = nn.Linear(self.n_cond, n_out, bias=False)\n",
    "\n",
    "        # if self.n_ext != 0:\n",
    "        #     self.ext_L = nn.Linear(self.n_ext, n_out, bias=False)\n",
    "\n",
    "        # if self.n_ext_m != 0:\n",
    "        #     if ext_mask is not None:\n",
    "        #         self.ext_L_m = MaskedLinear(self.n_ext_m, n_out, ext_mask, bias=False)\n",
    "        #     else:\n",
    "        #         self.ext_L_m = nn.Linear(self.n_ext_m, n_out, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # if self.n_cond == 0:\n",
    "        #     expr, cond = x, None\n",
    "        # else:\n",
    "        #     expr, cond = torch.split(x, [x.shape[1] - self.n_cond, self.n_cond], dim=1)\n",
    "\n",
    "        # if self.n_ext == 0:\n",
    "        #     ext = None\n",
    "        # else:\n",
    "        #     expr, ext = torch.split(expr, [expr.shape[1] - self.n_ext, self.n_ext], dim=1)\n",
    "\n",
    "        # if self.n_ext_m == 0:\n",
    "        #     ext_m = None\n",
    "        # else:\n",
    "        #     expr, ext_m = torch.split(expr, [expr.shape[1] - self.n_ext_m, self.n_ext_m], dim=1)\n",
    "\n",
    "        expr=x\n",
    "\n",
    "        out = self.expr_L(expr)\n",
    "        # if ext is not None:\n",
    "        #     out = out + self.ext_L(ext)\n",
    "        # if ext_m is not None:\n",
    "        #     out = out + self.ext_L_m(ext_m)\n",
    "        # if cond is not None:\n",
    "        #     out = out + self.cond_L(cond)\n",
    "        return out\n",
    "\n",
    "\n",
    "# class MaskedLinearDecoder(nn.Module):\n",
    "#     def __init__(self, in_dim, out_dim, n_cond, mask, ext_mask, recon_loss,\n",
    "#                  last_layer=None, n_ext=0, n_ext_m=0):\n",
    "#         super().__init__()\n",
    "\n",
    "#         if recon_loss == \"mse\":\n",
    "#             if last_layer == \"softmax\":\n",
    "#                 raise ValueError(\"Can't specify softmax last layer with mse loss.\")\n",
    "#             last_layer = \"identity\" if last_layer is None else last_layer\n",
    "#         elif recon_loss == \"nb\":\n",
    "#             last_layer = \"softmax\" if last_layer is None else last_layer\n",
    "#         else:\n",
    "#             raise ValueError(\"Unrecognized loss.\")\n",
    "\n",
    "#         print(\"GP Decoder Architecture:\")\n",
    "#         print(\"\\tMasked linear layer in, ext_m, ext, cond, out: \", in_dim, n_ext_m, n_ext, n_cond, out_dim)\n",
    "#         if mask is not None:\n",
    "#             print('\\twith hard mask.')\n",
    "#         else:\n",
    "#             print('\\twith soft mask.')\n",
    "\n",
    "#         self.n_ext = n_ext\n",
    "#         self.n_ext_m = n_ext_m\n",
    "\n",
    "#         self.n_cond = 0\n",
    "#         if n_cond is not None:\n",
    "#             self.n_cond = n_cond\n",
    "\n",
    "#         self.L0 = MaskedCondLayers(in_dim, out_dim, n_cond, bias=False, n_ext=n_ext, n_ext_m=n_ext_m,\n",
    "#                                    mask=mask, ext_mask=ext_mask)\n",
    "\n",
    "#         if last_layer == \"softmax\":\n",
    "#             self.mean_decoder = nn.Softmax(dim=-1)\n",
    "#         elif last_layer == \"softplus\":\n",
    "#             self.mean_decoder = nn.Softplus()\n",
    "#         elif last_layer == \"exp\":\n",
    "#             self.mean_decoder = torch.exp\n",
    "#         elif last_layer == \"relu\":\n",
    "#             self.mean_decoder = nn.ReLU()\n",
    "#         elif last_layer == \"identity\":\n",
    "#             self.mean_decoder = lambda a: a\n",
    "#         else:\n",
    "#             raise ValueError(\"Unrecognized last layer.\")\n",
    "\n",
    "#         print(\"Last Decoder layer:\", last_layer)\n",
    "\n",
    "#     def forward(self, z, batch=None):\n",
    "#         # if batch is not None:\n",
    "#         #     batch = one_hot_encoder(batch, n_cls=self.n_cond)\n",
    "#         #     z_cat = torch.cat((z, batch), dim=-1)\n",
    "#         #     dec_latent = self.L0(z_cat)\n",
    "#         # else:\n",
    "#         #     dec_latent = self.L0(z)\n",
    "\n",
    "#         dec_latent = self.L0(z)\n",
    "#         recon_x = self.mean_decoder(dec_latent)\n",
    "\n",
    "\n",
    "#         return recon_x, dec_latent\n",
    "\n",
    "class DecoderVELOVI(nn.Module):\n",
    "    \"\"\"\n",
    "    Decodes data from latent space of ``n_input`` dimensions ``n_output``dimensions.\n",
    "\n",
    "    Uses a fully-connected neural network of ``n_hidden`` layers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input\n",
    "        The dimensionality of the input (latent space)\n",
    "    n_output\n",
    "        The dimensionality of the output (data space)\n",
    "    n_cat_list\n",
    "        A list containing the number of categories\n",
    "        for each category of interest. Each category will be\n",
    "        included using a one-hot encoding\n",
    "    n_layers\n",
    "        The number of fully-connected hidden layers\n",
    "    n_hidden\n",
    "        The number of nodes per hidden layer\n",
    "    dropout_rate\n",
    "        Dropout rate to apply to each of the hidden layers\n",
    "    inject_covariates\n",
    "        Whether to inject covariates in each layer, or just the first (default).\n",
    "    use_batch_norm\n",
    "        Whether to use batch norm in layers\n",
    "    use_layer_norm\n",
    "        Whether to use layer norm in layers\n",
    "    linear_decoder\n",
    "        Whether to use linear decoder for time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_output: int,\n",
    "        n_ext: int = 0,\n",
    "        n_ext_m: int = 0,\n",
    "        n_cond: int = 0,\n",
    "        last_layer: str =None,\n",
    "        ext_mask: torch.Tensor = None,\n",
    "        mask: torch.Tensor = None,\n",
    "        recon_loss: str = 'nb',\n",
    "        n_cat_list: Iterable[int] = None,\n",
    "        n_layers: int = 1,\n",
    "        n_hidden: int = 128,\n",
    "        inject_covariates: bool = True,\n",
    "        use_batch_norm: bool = True,\n",
    "        use_layer_norm: bool = False,\n",
    "        dropout_rate: float = 0.0,\n",
    "        linear_decoder: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_ouput = n_output\n",
    "        self.linear_decoder = linear_decoder\n",
    "\n",
    "        ### GP decoder ###\n",
    "\n",
    "        if recon_loss == \"mse\":\n",
    "            if last_layer == \"softmax\":\n",
    "                raise ValueError(\"Can't specify softmax last layer with mse loss.\")\n",
    "            last_layer = \"identity\" if last_layer is None else last_layer\n",
    "        elif recon_loss == \"nb\":\n",
    "            last_layer = \"softmax\" if last_layer is None else last_layer\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized loss.\")\n",
    "\n",
    "        #print(\"GP Decoder Architecture:\")\n",
    "        #print(\"\\tMasked linear layer in, ext_m, ext, cond, out: \", in_dim, n_ext_m, n_ext, n_cond, out_dim)\n",
    "        if mask is not None:\n",
    "            print('\\twith hard mask.')\n",
    "        else:\n",
    "            print('\\twith soft mask.')\n",
    "\n",
    "        self.n_ext = n_ext\n",
    "        self.n_ext_m = n_ext_m\n",
    "\n",
    "        self.n_cond = 0\n",
    "        if n_cond is not None:\n",
    "            self.n_cond = n_cond\n",
    "\n",
    "        self.L0 = MaskedCondLayers(n_input, n_output, n_cond, bias=False, n_ext=n_ext, n_ext_m=n_ext_m,\n",
    "                                   mask=mask, ext_mask=ext_mask)\n",
    "\n",
    "        if last_layer == \"softmax\":\n",
    "            self.mean_decoder = nn.Softmax(dim=-1)\n",
    "        elif last_layer == \"softplus\":\n",
    "            self.mean_decoder = nn.Softplus()\n",
    "        elif last_layer == \"exp\":\n",
    "            self.mean_decoder = torch.exp\n",
    "        elif last_layer == \"relu\":\n",
    "            self.mean_decoder = nn.ReLU()\n",
    "        elif last_layer == \"identity\":\n",
    "            self.mean_decoder = lambda a: a\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized last layer.\")\n",
    "\n",
    "        print(\"Last Decoder layer:\", last_layer)\n",
    "\n",
    "        self.rho_first_decoder = FCLayers(\n",
    "            n_in=n_input,\n",
    "            n_out=n_hidden if not linear_decoder else n_output,\n",
    "            n_cat_list=n_cat_list,\n",
    "            n_layers=n_layers if not linear_decoder else 1,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate=dropout_rate,\n",
    "            inject_covariates=inject_covariates,\n",
    "            use_batch_norm=use_batch_norm,\n",
    "            use_layer_norm=use_layer_norm if not linear_decoder else False,\n",
    "            use_activation=not linear_decoder,\n",
    "            bias=not linear_decoder,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.pi_first_decoder = FCLayers(\n",
    "            n_in=n_input,\n",
    "            n_out=n_hidden,\n",
    "            n_cat_list=n_cat_list,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate=dropout_rate,\n",
    "            inject_covariates=inject_covariates,\n",
    "            use_batch_norm=use_batch_norm,\n",
    "            use_layer_norm=use_layer_norm,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.px_pi_decoder = nn.Linear(n_hidden, 4 * n_output)\n",
    "        \n",
    "\n",
    "        # rho for induction\n",
    "        self.px_rho_decoder = nn.Sequential(nn.Linear(n_hidden, n_output), nn.Sigmoid())\n",
    "\n",
    "        # tau for repression\n",
    "        self.px_tau_decoder = nn.Sequential(nn.Linear(n_hidden, n_output), nn.Sigmoid())\n",
    "\n",
    "        self.linear_scaling_tau = nn.Parameter(torch.zeros(n_output))\n",
    "        self.linear_scaling_tau_intercept = nn.Parameter(torch.zeros(n_output))\n",
    "\n",
    "    def forward(self, z: torch.Tensor, latent_dim: int = None):\n",
    "        \"\"\"\n",
    "        The forward computation for a single sample.\n",
    "\n",
    "         #. Decodes the data from the latent space using the decoder network\n",
    "         #. Returns parameters for the ZINB distribution of expression\n",
    "         #. If ``dispersion != 'gene-cell'`` then value for that param will be ``None``\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z :\n",
    "            tensor with shape ``(n_input,)``\n",
    "        cat_list\n",
    "            list of category membership(s) for this sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        4-tuple of :py:class:`torch.Tensor`\n",
    "            parameters for the ZINB distribution of expression\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        z_in = z\n",
    "        if latent_dim is not None:\n",
    "            mask = torch.zeros_like(z)\n",
    "            mask[..., latent_dim] = 1\n",
    "            z_in = z * mask\n",
    "        # The decoder returns values for the parameters of the ZINB distribution\n",
    "        rho_first = self.rho_first_decoder(z_in)\n",
    "\n",
    "        dec_latent = self.L0(z)\n",
    "        recon_x = self.mean_decoder(dec_latent)\n",
    "\n",
    "        if not self.linear_decoder:\n",
    "            px_rho = self.px_rho_decoder(rho_first)\n",
    "            px_tau = self.px_tau_decoder(rho_first)\n",
    "        else:\n",
    "            px_rho = nn.Sigmoid()(rho_first)\n",
    "            px_tau = 1 - nn.Sigmoid()(\n",
    "                rho_first * self.linear_scaling_tau.exp()\n",
    "                + self.linear_scaling_tau_intercept\n",
    "            )\n",
    "\n",
    "        # cells by genes by 4\n",
    "        pi_first = self.pi_first_decoder(z)\n",
    "        px_pi = nn.Softplus()(\n",
    "            torch.reshape(self.px_pi_decoder(pi_first), (z.shape[0], self.n_ouput, 4))\n",
    "        )\n",
    "\n",
    "        return px_pi, px_rho, px_tau, recon_x, dec_latent\n",
    "\n",
    "\n",
    "# VAE model\n",
    "class VELOVAE(BaseModuleClass):\n",
    "    \"\"\"\n",
    "    Variational auto-encoder model.\n",
    "\n",
    "    This is an implementation of the scVI model descibed in [Lopez18]_\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input\n",
    "        Number of input genes\n",
    "    n_hidden\n",
    "        Number of nodes per hidden layer\n",
    "    n_latent\n",
    "        Dimensionality of the latent space\n",
    "    n_layers\n",
    "        Number of hidden layers used for encoder and decoder NNs\n",
    "    dropout_rate\n",
    "        Dropout rate for neural networks\n",
    "    log_variational\n",
    "        Log(data+1) prior to encoding for numerical stability. Not normalization.\n",
    "    latent_distribution\n",
    "        One of\n",
    "\n",
    "        * ``'normal'`` - Isotropic normal\n",
    "        * ``'ln'`` - Logistic normal with normal params N(0, 1)\n",
    "    use_layer_norm\n",
    "        Whether to use layer norm in layers\n",
    "    use_observed_lib_size\n",
    "        Use observed library size for RNA as scaling factor in mean of conditional distribution\n",
    "    var_activation\n",
    "        Callable used to ensure positivity of the variational distributions' variance.\n",
    "        When `None`, defaults to `torch.exp`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        true_time_switch: Optional[np.ndarray] = None,\n",
    "        n_hidden: int = 128,\n",
    "        n_latent: int = 10,\n",
    "        n_layers: int = 1,\n",
    "        dropout_rate: float = 0.1,\n",
    "        log_variational: bool = False,\n",
    "        latent_distribution: str = \"normal\",\n",
    "        use_batch_norm: Literal[\"encoder\", \"decoder\", \"none\", \"both\"] = \"both\",\n",
    "        use_layer_norm: Literal[\"encoder\", \"decoder\", \"none\", \"both\"] = \"both\",\n",
    "        use_observed_lib_size: bool = True,\n",
    "        var_activation: Optional[Callable] = torch.nn.Softplus(),\n",
    "        model_steady_states: bool = True,\n",
    "        gamma_unconstr_init: Optional[np.ndarray] = None,\n",
    "        alpha_unconstr_init: Optional[np.ndarray] = None,\n",
    "        alpha_1_unconstr_init: Optional[np.ndarray] = None,\n",
    "        lambda_alpha_unconstr_init: Optional[np.ndarray] = None,\n",
    "        switch_spliced: Optional[np.ndarray] = None,\n",
    "        switch_unspliced: Optional[np.ndarray] = None,\n",
    "        t_max: float = 20,\n",
    "        penalty_scale: float = 0.2,\n",
    "        dirichlet_concentration: float = 0.25,\n",
    "        linear_decoder: bool = False,\n",
    "        time_dep_transcription_rate: bool = False,\n",
    "        #Parameters for masked linear decoder\n",
    "        mask: torch.Tensor = None,\n",
    "        recon_loss: str = 'nb',\n",
    "        conditions: list = [],\n",
    "        use_l_encoder: bool = False,\n",
    "        dr_rate: float = 0.05,\n",
    "        use_bn: bool = False,\n",
    "        use_ln: bool = True,\n",
    "        decoder_last_layer: Optional[str] = None,\n",
    "        soft_mask: bool = False,\n",
    "        n_ext: int = 0,\n",
    "        n_ext_m: int = 0,\n",
    "        use_hsic: bool = False,\n",
    "        hsic_one_vs_all: bool = False,\n",
    "        ext_mask: Optional[torch.Tensor] = None,\n",
    "        soft_ext_mask: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_latent = n_latent\n",
    "        self.log_variational = log_variational\n",
    "        self.latent_distribution = latent_distribution\n",
    "        self.use_observed_lib_size = use_observed_lib_size\n",
    "        self.n_input = n_input\n",
    "        self.model_steady_states = model_steady_states\n",
    "        self.t_max = t_max\n",
    "        self.penalty_scale = penalty_scale\n",
    "        self.dirichlet_concentration = dirichlet_concentration\n",
    "        self.time_dep_transcription_rate = time_dep_transcription_rate\n",
    "\n",
    "        if switch_spliced is not None:\n",
    "            self.register_buffer(\"switch_spliced\", torch.from_numpy(switch_spliced))\n",
    "        else:\n",
    "            self.switch_spliced = None\n",
    "        if switch_unspliced is not None:\n",
    "            self.register_buffer(\"switch_unspliced\", torch.from_numpy(switch_unspliced))\n",
    "        else:\n",
    "            self.switch_unspliced = None\n",
    "\n",
    "        n_genes = n_input * 2\n",
    "\n",
    "        # switching time\n",
    "        self.switch_time_unconstr = torch.nn.Parameter(7 + 0.5 * torch.randn(n_input))\n",
    "        if true_time_switch is not None:\n",
    "            self.register_buffer(\"true_time_switch\", torch.from_numpy(true_time_switch))\n",
    "        else:\n",
    "            self.true_time_switch = None\n",
    "\n",
    "        # degradation\n",
    "        if gamma_unconstr_init is None:\n",
    "            self.gamma_mean_unconstr = torch.nn.Parameter(-1 * torch.ones(n_input))\n",
    "        else:\n",
    "            self.gamma_mean_unconstr = torch.nn.Parameter(\n",
    "                torch.from_numpy(gamma_unconstr_init)\n",
    "            )\n",
    "\n",
    "        # splicing\n",
    "        # first samples around 1\n",
    "        self.beta_mean_unconstr = torch.nn.Parameter(0.5 * torch.ones(n_input))\n",
    "\n",
    "        # transcription\n",
    "        if alpha_unconstr_init is None:\n",
    "            self.alpha_unconstr = torch.nn.Parameter(0 * torch.ones(n_input))\n",
    "        else:\n",
    "            self.alpha_unconstr = torch.nn.Parameter(\n",
    "                torch.from_numpy(alpha_unconstr_init)\n",
    "            )\n",
    "\n",
    "        # TODO: Add `require_grad`\n",
    "        if alpha_1_unconstr_init is None:\n",
    "            self.alpha_1_unconstr = torch.nn.Parameter(0 * torch.ones(n_input))\n",
    "        else:\n",
    "            self.alpha_1_unconstr = torch.nn.Parameter(\n",
    "                torch.from_numpy(alpha_1_unconstr_init)\n",
    "            )\n",
    "        self.alpha_1_unconstr.requires_grad = time_dep_transcription_rate\n",
    "\n",
    "        if lambda_alpha_unconstr_init is None:\n",
    "            self.lambda_alpha_unconstr = torch.nn.Parameter(0 * torch.ones(n_input))\n",
    "        else:\n",
    "            self.lambda_alpha_unconstr = torch.nn.Parameter(\n",
    "                torch.from_numpy(lambda_alpha_unconstr_init)\n",
    "            )\n",
    "        self.lambda_alpha_unconstr.requires_grad = time_dep_transcription_rate\n",
    "\n",
    "        # likelihood dispersion\n",
    "        # for now, with normal dist, this is just the variance\n",
    "        self.scale_unconstr = torch.nn.Parameter(-1 * torch.ones(n_genes, 4))\n",
    "\n",
    "        use_batch_norm_encoder = use_batch_norm == \"encoder\" or use_batch_norm == \"both\"\n",
    "        use_batch_norm_decoder = use_batch_norm == \"decoder\" or use_batch_norm == \"both\"\n",
    "        use_layer_norm_encoder = use_layer_norm == \"encoder\" or use_layer_norm == \"both\"\n",
    "        use_layer_norm_decoder = use_layer_norm == \"decoder\" or use_layer_norm == \"both\"\n",
    "        self.use_batch_norm_decoder = use_batch_norm_decoder\n",
    "\n",
    "        # z encoder goes from the n_input-dimensional data to an n_latent-d\n",
    "        # latent space representation\n",
    "        n_input_encoder = n_genes\n",
    "        self.z_encoder = Encoder(\n",
    "            n_input_encoder,\n",
    "            n_latent,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate=dropout_rate,\n",
    "            distribution=latent_distribution,\n",
    "            use_batch_norm=use_batch_norm_encoder,\n",
    "            use_layer_norm=use_layer_norm_encoder,\n",
    "            var_activation=var_activation,\n",
    "            activation_fn=torch.nn.ReLU,\n",
    "        )\n",
    "\n",
    "         ### Attributes for masked linear decoder\n",
    "        self.n_conditions = len(conditions)\n",
    "        self.conditions = conditions\n",
    "        self.n_conditions=0\n",
    "        self.recon_loss = recon_loss\n",
    "        self.freeze = False\n",
    "        self.use_bn = use_bn\n",
    "        self.use_ln = use_ln\n",
    "\n",
    "        self.use_mmd = False\n",
    "\n",
    "        self.n_ext_encoder = n_ext + n_ext_m\n",
    "        self.n_ext_decoder = n_ext\n",
    "        self.n_ext_m_decoder = n_ext_m\n",
    "\n",
    "        self.use_hsic = use_hsic and self.n_ext_decoder > 0\n",
    "        self.hsic_one_vs_all = hsic_one_vs_all\n",
    "\n",
    "        self.soft_mask = soft_mask and mask is not None\n",
    "        self.soft_ext_mask = soft_ext_mask and ext_mask is not None\n",
    "\n",
    "        if decoder_last_layer is None:\n",
    "            if recon_loss == 'nb':\n",
    "                self.decoder_last_layer = 'softmax'\n",
    "            else:\n",
    "                self.decoder_last_layer = 'identity'\n",
    "        else:\n",
    "            self.decoder_last_layer = decoder_last_layer\n",
    "\n",
    "        self.use_l_encoder = use_l_encoder\n",
    "\n",
    "        self.dr_rate = dr_rate\n",
    "        if self.dr_rate > 0:\n",
    "            self.use_dr = True\n",
    "        else:\n",
    "            self.use_dr = False\n",
    "\n",
    "        if recon_loss == \"nb\":\n",
    "            if self.n_conditions != 0:\n",
    "                self.theta = torch.nn.Parameter(torch.randn(self.n_input, self.n_conditions))\n",
    "            else:\n",
    "                self.theta = torch.nn.Parameter(torch.randn(1, self.n_input))\n",
    "        else:\n",
    "            self.theta = None\n",
    "\n",
    "        if self.soft_mask:\n",
    "            self.n_inact_genes = (1-mask).sum().item()\n",
    "            soft_shape = mask.shape\n",
    "            if soft_shape[0] != n_latent or soft_shape[1] != n_input:\n",
    "                raise ValueError('Incorrect shape of the soft mask.')\n",
    "            self.mask = mask.t()\n",
    "            mask = None\n",
    "        else:\n",
    "            self.mask = None\n",
    "\n",
    "        if self.soft_ext_mask:\n",
    "            self.n_inact_ext_genes = (1-ext_mask).sum().item()\n",
    "            ext_shape = ext_mask.shape\n",
    "            if ext_shape[0] != self.n_ext_m_decoder:\n",
    "                raise ValueError('Dim 0 of ext_mask should be the same as n_ext_m_decoder.')\n",
    "            if ext_shape[1] != self.n_input:\n",
    "                raise ValueError('Dim 1 of ext_mask should be the same as n_input.')\n",
    "            self.ext_mask = ext_mask.t()\n",
    "            ext_mask = None\n",
    "        else:\n",
    "            self.ext_mask = None\n",
    "            \n",
    "        # decoder goes from n_latent-dimensional space to n_input-d data\n",
    "        n_input_decoder = n_latent\n",
    "        self.decoder = DecoderVELOVI(\n",
    "            n_input_decoder,\n",
    "            n_input,\n",
    "            n_ext = 0,\n",
    "            n_ext_m= 0,\n",
    "            n_cond= 0,\n",
    "            last_layer=None,\n",
    "            ext_mask = None,\n",
    "            mask = None,\n",
    "            recon_loss = 'nb',\n",
    "            n_cat_list= None,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            use_batch_norm=use_batch_norm_decoder,\n",
    "            use_layer_norm=use_layer_norm_decoder,\n",
    "            activation_fn=torch.nn.ReLU,\n",
    "            linear_decoder=linear_decoder,\n",
    "            )\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "    def _get_inference_input(self, tensors):\n",
    "        spliced = tensors[REGISTRY_KEYS.X_KEY]\n",
    "        unspliced = tensors[REGISTRY_KEYS.U_KEY]\n",
    "\n",
    "        input_dict = dict(\n",
    "            spliced=spliced,\n",
    "            unspliced=unspliced,\n",
    "        )\n",
    "        return input_dict\n",
    "\n",
    "    def _get_generative_input(self, tensors, inference_outputs):\n",
    "        z = inference_outputs[\"z\"]\n",
    "        gamma = inference_outputs[\"gamma\"]\n",
    "        beta = inference_outputs[\"beta\"]\n",
    "        alpha = inference_outputs[\"alpha\"]\n",
    "        alpha_1 = inference_outputs[\"alpha_1\"]\n",
    "        lambda_alpha = inference_outputs[\"lambda_alpha\"]\n",
    "\n",
    "        input_dict = {\n",
    "            \"z\": z,\n",
    "            \"gamma\": gamma,\n",
    "            \"beta\": beta,\n",
    "            \"alpha\": alpha,\n",
    "            \"alpha_1\": alpha_1,\n",
    "            \"lambda_alpha\": lambda_alpha,\n",
    "        }\n",
    "        return input_dict\n",
    "\n",
    "    @auto_move_data\n",
    "    def inference(\n",
    "        self,\n",
    "        spliced,\n",
    "        unspliced,\n",
    "        n_samples=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        High level inference method.\n",
    "\n",
    "        Runs the inference (encoder) model.\n",
    "        \"\"\"\n",
    "        spliced_ = spliced\n",
    "        unspliced_ = unspliced\n",
    "        if self.log_variational:\n",
    "            spliced_ = torch.log(0.01 + spliced)\n",
    "            unspliced_ = torch.log(0.01 + unspliced)\n",
    "\n",
    "        encoder_input = torch.cat((spliced_, unspliced_), dim=-1)\n",
    "\n",
    "        qz_m, qz_v, z = self.z_encoder(encoder_input)\n",
    "\n",
    "        if n_samples > 1:\n",
    "            qz_m = qz_m.unsqueeze(0).expand((n_samples, qz_m.size(0), qz_m.size(1)))\n",
    "            qz_v = qz_v.unsqueeze(0).expand((n_samples, qz_v.size(0), qz_v.size(1)))\n",
    "            # when z is normal, untran_z == z\n",
    "            untran_z = Normal(qz_m, qz_v.sqrt()).sample()\n",
    "            z = self.z_encoder.z_transformation(untran_z)\n",
    "\n",
    "        gamma, beta, alpha, alpha_1, lambda_alpha = self._get_rates()\n",
    "\n",
    "        outputs = dict(\n",
    "            z=z,\n",
    "            qz_m=qz_m,\n",
    "            qz_v=qz_v,\n",
    "            gamma=gamma,\n",
    "            beta=beta,\n",
    "            alpha=alpha,\n",
    "            alpha_1=alpha_1,\n",
    "            lambda_alpha=lambda_alpha,\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    def _get_rates(self):\n",
    "        # globals\n",
    "        # degradation\n",
    "        gamma = torch.clamp(F.softplus(self.gamma_mean_unconstr), 0, 50)\n",
    "        # splicing\n",
    "        beta = torch.clamp(F.softplus(self.beta_mean_unconstr), 0, 50)\n",
    "        # transcription\n",
    "        alpha = torch.clamp(F.softplus(self.alpha_unconstr), 0, 50)\n",
    "        if self.time_dep_transcription_rate:\n",
    "            alpha_1 = torch.clamp(F.softplus(self.alpha_1_unconstr), 0, 50)\n",
    "            lambda_alpha = torch.clamp(F.softplus(self.lambda_alpha_unconstr), 0, 50)\n",
    "        else:\n",
    "            alpha_1 = self.alpha_1_unconstr\n",
    "            lambda_alpha = self.lambda_alpha_unconstr\n",
    "\n",
    "        return gamma, beta, alpha, alpha_1, lambda_alpha\n",
    "\n",
    "    @auto_move_data\n",
    "    def generative(self, z, gamma, beta, alpha, alpha_1, lambda_alpha, latent_dim=None):\n",
    "        \"\"\"Runs the generative model.\"\"\"\n",
    "        decoder_input = z\n",
    "        px_pi_alpha, px_rho, px_tau, dec_mean, dec_latent = self.decoder(decoder_input, latent_dim=latent_dim)\n",
    "\n",
    "        px_pi = Dirichlet(px_pi_alpha).rsample()\n",
    "\n",
    "        #dec_mean, dec_latent = self.GP_linear_decoder(decoder_input, batch=None)\n",
    "\n",
    "        scale_unconstr = self.scale_unconstr\n",
    "        scale = F.softplus(scale_unconstr)\n",
    "\n",
    "        mixture_dist_s, mixture_dist_u, end_penalty = self.get_px(\n",
    "            px_pi,\n",
    "            px_rho,\n",
    "            px_tau,\n",
    "            scale,\n",
    "            gamma,\n",
    "            beta,\n",
    "            alpha,\n",
    "            alpha_1,\n",
    "            lambda_alpha,\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            px_pi=px_pi,\n",
    "            px_rho=px_rho,\n",
    "            px_tau=px_tau,\n",
    "            scale=scale,\n",
    "            px_pi_alpha=px_pi_alpha,\n",
    "            mixture_dist_u=mixture_dist_u,\n",
    "            mixture_dist_s=mixture_dist_s,\n",
    "            end_penalty=end_penalty,\n",
    "            gene_recon = dec_mean,\n",
    "            dec_latent = dec_latent\n",
    "        )\n",
    "\n",
    "    def loss(\n",
    "        self,\n",
    "        tensors,\n",
    "        inference_outputs,\n",
    "        generative_outputs,\n",
    "        cond_batch=None,\n",
    "        kl_weight: float = 1.0,\n",
    "        n_obs: float = 1.0,\n",
    "    ):\n",
    "        spliced = tensors[REGISTRY_KEYS.X_KEY]\n",
    "        unspliced = tensors[REGISTRY_KEYS.U_KEY]\n",
    "\n",
    "        #gene reconstruction loss\n",
    "        ground_truth_counts = spliced + unspliced\n",
    "        \n",
    "\n",
    "        if cond_batch is not None:\n",
    "            dispersion = F.linear(one_hot_encoder(cond_batch, self.n_conditions), self.theta) #batch is the\n",
    "        else:\n",
    "            dispersion = self.theta   \n",
    "        dispersion = torch.exp(dispersion)\n",
    "        print(f\"dispersion: {dispersion}\")\n",
    "        dec_mean = generative_outputs[\"gene_recon\"]\n",
    "        print(f\"dec_mean: {dec_mean}\")\n",
    "        negbin = NegativeBinomial(mu=dec_mean, theta=dispersion)\n",
    "        \n",
    "        gene_recon_loss = -negbin.log_prob(ground_truth_counts).sum(dim=-1)\n",
    "        \n",
    "\n",
    "        qz_m = inference_outputs[\"qz_m\"]\n",
    "        qz_v = inference_outputs[\"qz_v\"]\n",
    "\n",
    "        px_pi = generative_outputs[\"px_pi\"]\n",
    "        px_pi_alpha = generative_outputs[\"px_pi_alpha\"]\n",
    "\n",
    "        end_penalty = generative_outputs[\"end_penalty\"]\n",
    "        mixture_dist_s = generative_outputs[\"mixture_dist_s\"]\n",
    "        mixture_dist_u = generative_outputs[\"mixture_dist_u\"]\n",
    "\n",
    "        kl_divergence_z = kl(Normal(qz_m, torch.sqrt(qz_v)), Normal(0, 1)).sum(dim=1)\n",
    "\n",
    "        reconst_loss_s = -mixture_dist_s.log_prob(spliced)\n",
    "        reconst_loss_u = -mixture_dist_u.log_prob(unspliced)\n",
    "        reconst_loss = reconst_loss_u.sum(dim=-1) + reconst_loss_s.sum(dim=-1) \n",
    "\n",
    "        kl_pi = kl(\n",
    "            Dirichlet(px_pi_alpha),\n",
    "            Dirichlet(self.dirichlet_concentration * torch.ones_like(px_pi)),\n",
    "        ).sum(dim=-1)\n",
    "\n",
    "        # local loss\n",
    "        kl_local = kl_divergence_z + kl_pi\n",
    "        weighted_kl_local = kl_weight * (kl_divergence_z) + kl_pi\n",
    "\n",
    "        local_loss = torch.mean(reconst_loss + gene_recon_loss + weighted_kl_local)\n",
    "\n",
    "        # combine local and global\n",
    "        global_loss = 0\n",
    "        loss = (\n",
    "            local_loss\n",
    "            + self.penalty_scale * (1 - kl_weight) * end_penalty\n",
    "            + (1 / n_obs) * kl_weight * (global_loss)\n",
    "        )\n",
    "\n",
    "        loss_recorder = LossRecorder(\n",
    "            loss, reconst_loss, kl_local, torch.tensor(global_loss)\n",
    "        )\n",
    "\n",
    "        return loss_recorder\n",
    "\n",
    "\n",
    "    @auto_move_data\n",
    "    def get_px(\n",
    "        self,\n",
    "        px_pi,\n",
    "        px_rho,\n",
    "        px_tau,\n",
    "        scale,\n",
    "        gamma,\n",
    "        beta,\n",
    "        alpha,\n",
    "        alpha_1,\n",
    "        lambda_alpha,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        t_s = torch.clamp(F.softplus(self.switch_time_unconstr), 0, self.t_max)\n",
    "\n",
    "        n_cells = px_pi.shape[0]\n",
    "\n",
    "        # component dist\n",
    "        comp_dist = Categorical(probs=px_pi)\n",
    "\n",
    "        # induction\n",
    "        mean_u_ind, mean_s_ind = self._get_induction_unspliced_spliced(\n",
    "            alpha, alpha_1, lambda_alpha, beta, gamma, t_s * px_rho\n",
    "        )\n",
    "\n",
    "        if self.time_dep_transcription_rate:\n",
    "            mean_u_ind_steady = (alpha_1 / beta).expand(n_cells, self.n_input)\n",
    "            mean_s_ind_steady = (alpha_1 / gamma).expand(n_cells, self.n_input)\n",
    "        else:\n",
    "            mean_u_ind_steady = (alpha / beta).expand(n_cells, self.n_input)\n",
    "            mean_s_ind_steady = (alpha / gamma).expand(n_cells, self.n_input)\n",
    "        scale_u = scale[: self.n_input, :].expand(n_cells, self.n_input, 4).sqrt()\n",
    "\n",
    "        # repression\n",
    "        u_0, s_0 = self._get_induction_unspliced_spliced(\n",
    "            alpha, alpha_1, lambda_alpha, beta, gamma, t_s\n",
    "        )\n",
    "\n",
    "        tau = px_tau\n",
    "        mean_u_rep, mean_s_rep = self._get_repression_unspliced_spliced(\n",
    "            u_0,\n",
    "            s_0,\n",
    "            beta,\n",
    "            gamma,\n",
    "            (self.t_max - t_s) * tau,\n",
    "        )\n",
    "        mean_u_rep_steady = torch.zeros_like(mean_u_ind)\n",
    "        mean_s_rep_steady = torch.zeros_like(mean_u_ind)\n",
    "        scale_s = scale[self.n_input :, :].expand(n_cells, self.n_input, 4).sqrt()\n",
    "\n",
    "        end_penalty = ((u_0 - self.switch_unspliced).pow(2)).sum() + (\n",
    "            (s_0 - self.switch_spliced).pow(2)\n",
    "        ).sum()\n",
    "\n",
    "        # unspliced\n",
    "        mean_u = torch.stack(\n",
    "            (\n",
    "                mean_u_ind,\n",
    "                mean_u_ind_steady,\n",
    "                mean_u_rep,\n",
    "                mean_u_rep_steady,\n",
    "            ),\n",
    "            dim=2,\n",
    "        )\n",
    "        scale_u = torch.stack(\n",
    "            (\n",
    "                scale_u[..., 0],\n",
    "                scale_u[..., 0],\n",
    "                scale_u[..., 0],\n",
    "                0.1 * scale_u[..., 0],\n",
    "            ),\n",
    "            dim=2,\n",
    "        )\n",
    "        dist_u = Normal(mean_u, scale_u)\n",
    "        mixture_dist_u = MixtureSameFamily(comp_dist, dist_u)\n",
    "\n",
    "        # spliced\n",
    "        mean_s = torch.stack(\n",
    "            (mean_s_ind, mean_s_ind_steady, mean_s_rep, mean_s_rep_steady),\n",
    "            dim=2,\n",
    "        )\n",
    "        scale_s = torch.stack(\n",
    "            (\n",
    "                scale_s[..., 0],\n",
    "                scale_s[..., 0],\n",
    "                scale_s[..., 0],\n",
    "                0.1 * scale_s[..., 0],\n",
    "            ),\n",
    "            dim=2,\n",
    "        )\n",
    "        dist_s = Normal(mean_s, scale_s)\n",
    "        mixture_dist_s = MixtureSameFamily(comp_dist, dist_s)\n",
    "\n",
    "        return mixture_dist_s, mixture_dist_u, end_penalty\n",
    "\n",
    "    def _get_induction_unspliced_spliced(\n",
    "        self, alpha, alpha_1, lambda_alpha, beta, gamma, t, eps=1e-6\n",
    "    ):\n",
    "        if self.time_dep_transcription_rate:\n",
    "            unspliced = alpha_1 / beta * (1 - torch.exp(-beta * t)) - (\n",
    "                alpha_1 - alpha\n",
    "            ) / (beta - lambda_alpha) * (\n",
    "                torch.exp(-lambda_alpha * t) - torch.exp(-beta * t)\n",
    "            )\n",
    "\n",
    "            spliced = (\n",
    "                alpha_1 / gamma * (1 - torch.exp(-gamma * t))\n",
    "                + alpha_1\n",
    "                / (gamma - beta + eps)\n",
    "                * (torch.exp(-gamma * t) - torch.exp(-beta * t))\n",
    "                - beta\n",
    "                * (alpha_1 - alpha)\n",
    "                / (beta - lambda_alpha + eps)\n",
    "                / (gamma - lambda_alpha + eps)\n",
    "                * (torch.exp(-lambda_alpha * t) - torch.exp(-gamma * t))\n",
    "                + beta\n",
    "                * (alpha_1 - alpha)\n",
    "                / (beta - lambda_alpha + eps)\n",
    "                / (gamma - beta + eps)\n",
    "                * (torch.exp(-beta * t) - torch.exp(-gamma * t))\n",
    "            )\n",
    "        else:\n",
    "            unspliced = (alpha / beta) * (1 - torch.exp(-beta * t))\n",
    "            spliced = (alpha / gamma) * (1 - torch.exp(-gamma * t)) + (\n",
    "                alpha / ((gamma - beta) + eps)\n",
    "            ) * (torch.exp(-gamma * t) - torch.exp(-beta * t))\n",
    "\n",
    "        return unspliced, spliced\n",
    "\n",
    "    def _get_repression_unspliced_spliced(self, u_0, s_0, beta, gamma, t, eps=1e-6):\n",
    "        unspliced = torch.exp(-beta * t) * u_0\n",
    "        spliced = s_0 * torch.exp(-gamma * t) - (\n",
    "            beta * u_0 / ((gamma - beta) + eps)\n",
    "        ) * (torch.exp(-gamma * t) - torch.exp(-beta * t))\n",
    "        return unspliced, spliced\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Not implemented.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_loadings(self) -> np.ndarray:\n",
    "        \"\"\"Extract per-gene weights (for each Z, shape is genes by dim(Z)) in the linear decoder.\"\"\"\n",
    "        # This is BW, where B is diag(b) batch norm, W is weight matrix\n",
    "        if self.decoder.linear_decoder is False:\n",
    "            raise ValueError(\"Model not trained with linear decoder\")\n",
    "        w = self.decoder.rho_first_decoder.fc_layers[0][0].weight\n",
    "        if self.use_batch_norm_decoder:\n",
    "            bn = self.decoder.rho_first_decoder.fc_layers[0][1]\n",
    "            sigma = torch.sqrt(bn.running_var + bn.eps)\n",
    "            gamma = bn.weight\n",
    "            b = gamma / sigma\n",
    "            b_identity = torch.diag(b)\n",
    "            loadings = torch.matmul(b_identity, w)\n",
    "        else:\n",
    "            loadings = w\n",
    "        loadings = loadings.detach().cpu().numpy()\n",
    "\n",
    "        return loadings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _softplus_inverse(x: np.ndarray) -> np.ndarray:\n",
    "    x = torch.from_numpy(x)\n",
    "    x_inv = torch.where(x > 20, x, x.expm1().log()).numpy()\n",
    "    return x_inv\n",
    "\n",
    "\n",
    "class VELOVI(VAEMixin, UnsupervisedTrainingMixin, BaseModelClass):\n",
    "    \"\"\"\n",
    "    Velocity Variational Inference\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData object that has been registered via :func:`~velovi.VELOVI.setup_anndata`.\n",
    "    n_hidden\n",
    "        Number of nodes per hidden layer.\n",
    "    n_latent\n",
    "        Dimensionality of the latent space.\n",
    "    n_layers\n",
    "        Number of hidden layers used for encoder and decoder NNs.\n",
    "    dropout_rate\n",
    "        Dropout rate for neural networks.\n",
    "    gamma_init_data\n",
    "        Initialize gamma using the data-driven technique.\n",
    "    linear_decoder\n",
    "        Use a linear decoder from latent space to time.\n",
    "    **model_kwargs\n",
    "        Keyword args for :class:`~velovi.VELOVAE`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata: AnnData,\n",
    "        n_hidden: int = 256,\n",
    "        n_latent: int = 10,\n",
    "        n_layers: int = 1,\n",
    "        dropout_rate: float = 0.1,\n",
    "        gamma_init_data: bool = False,\n",
    "        linear_decoder: bool = False,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "        super().__init__(adata)\n",
    "        self.n_latent = n_latent\n",
    "\n",
    "        spliced = self.adata_manager.get_from_registry(REGISTRY_KEYS.X_KEY)\n",
    "        unspliced = self.adata_manager.get_from_registry(REGISTRY_KEYS.U_KEY)\n",
    "\n",
    "        sorted_unspliced = np.argsort(unspliced, axis=0)\n",
    "        ind = int(adata.n_obs * 0.99)\n",
    "        us_upper_ind = sorted_unspliced[ind:, :]\n",
    "\n",
    "        us_upper = []\n",
    "        ms_upper = []\n",
    "        for i in range(len(us_upper_ind)):\n",
    "            row = us_upper_ind[i]\n",
    "            us_upper += [unspliced[row, np.arange(adata.n_vars)][np.newaxis, :]]\n",
    "            ms_upper += [spliced[row, np.arange(adata.n_vars)][np.newaxis, :]]\n",
    "        us_upper = np.median(np.concatenate(us_upper, axis=0), axis=0)\n",
    "        ms_upper = np.median(np.concatenate(ms_upper, axis=0), axis=0)\n",
    "\n",
    "        alpha_unconstr = _softplus_inverse(us_upper)\n",
    "        alpha_unconstr = np.asarray(alpha_unconstr).ravel()\n",
    "\n",
    "        alpha_1_unconstr = np.zeros(us_upper.shape).ravel()\n",
    "        lambda_alpha_unconstr = np.zeros(us_upper.shape).ravel()\n",
    "\n",
    "        if gamma_init_data:\n",
    "            gamma_unconstr = np.clip(_softplus_inverse(us_upper / ms_upper), None, 10)\n",
    "        else:\n",
    "            gamma_unconstr = None\n",
    "\n",
    "        self.module = VELOVAE(\n",
    "            n_input=self.summary_stats[\"n_vars\"],\n",
    "            n_hidden=n_hidden,\n",
    "            n_latent=n_latent,\n",
    "            n_layers=n_layers,\n",
    "            dropout_rate=dropout_rate,\n",
    "            gamma_unconstr_init=gamma_unconstr,\n",
    "            alpha_unconstr_init=alpha_unconstr,\n",
    "            alpha_1_unconstr_init=alpha_1_unconstr,\n",
    "            lambda_alpha_unconstr_init=lambda_alpha_unconstr,\n",
    "            switch_spliced=ms_upper,\n",
    "            switch_unspliced=us_upper,\n",
    "            linear_decoder=linear_decoder,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        self._model_summary_string = (\n",
    "            \"VELOVI Model with the following params: \\nn_hidden: {}, n_latent: {}, n_layers: {}, dropout_rate: \"\n",
    "            \"{}\"\n",
    "        ).format(\n",
    "            n_hidden,\n",
    "            n_latent,\n",
    "            n_layers,\n",
    "            dropout_rate,\n",
    "        )\n",
    "        self.init_params_ = self._get_init_params(locals())\n",
    "\n",
    "    # def train(\n",
    "    #     self,\n",
    "    #     max_epochs: Optional[int] = 500,\n",
    "    #     lr: float = 1e-2,\n",
    "    #     weight_decay: float = 1e-2,\n",
    "    #     use_gpu: Optional[Union[str, int, bool]] = None,\n",
    "    #     train_size: float = 0.9,\n",
    "    #     validation_size: Optional[float] = None,\n",
    "    #     batch_size: int = 256,\n",
    "    #     early_stopping: bool = True,\n",
    "    #     gradient_clip_val: float = 10,\n",
    "    #     plan_kwargs: Optional[dict] = None,\n",
    "    #     **trainer_kwargs,\n",
    "    # ):\n",
    "    #     \"\"\"\n",
    "    #     Train the model.\n",
    "\n",
    "    #     Parameters\n",
    "    #     ----------\n",
    "    #     max_epochs\n",
    "    #         Number of passes through the dataset. If `None`, defaults to\n",
    "    #         `np.min([round((20000 / n_cells) * 400), 400])`\n",
    "    #     lr\n",
    "    #         Learning rate for optimization\n",
    "    #     weight_decay\n",
    "    #         Weight decay for optimization\n",
    "    #     use_gpu\n",
    "    #         Use default GPU if available (if None or True), or index of GPU to use (if int),\n",
    "    #         or name of GPU (if str, e.g., `'cuda:0'`), or use CPU (if False).\n",
    "    #     train_size\n",
    "    #         Size of training set in the range [0.0, 1.0].\n",
    "    #     validation_size\n",
    "    #         Size of the test set. If `None`, defaults to 1 - `train_size`. If\n",
    "    #         `train_size + validation_size < 1`, the remaining cells belong to a test set.\n",
    "    #     batch_size\n",
    "    #         Minibatch size to use during training.\n",
    "    #     early_stopping\n",
    "    #         Perform early stopping. Additional arguments can be passed in `**kwargs`.\n",
    "    #         See :class:`~scvi.train.Trainer` for further options.\n",
    "    #     gradient_clip_val\n",
    "    #         Val for gradient clipping\n",
    "    #     plan_kwargs\n",
    "    #         Keyword args for :class:`~scvi.train.TrainingPlan`. Keyword arguments passed to\n",
    "    #         `train()` will overwrite values present in `plan_kwargs`, when appropriate.\n",
    "    #     **trainer_kwargs\n",
    "    #         Other keyword args for :class:`~scvi.train.Trainer`.\n",
    "    #     \"\"\"\n",
    "    #     user_plan_kwargs = (\n",
    "    #         plan_kwargs.copy() if isinstance(plan_kwargs, dict) else dict()\n",
    "    #     )\n",
    "    #     plan_kwargs = dict(lr=lr, weight_decay=weight_decay, optimizer=\"AdamW\")\n",
    "    #     plan_kwargs.update(user_plan_kwargs)\n",
    "\n",
    "    #     user_train_kwargs = trainer_kwargs.copy()\n",
    "    #     trainer_kwargs = dict(gradient_clip_val=gradient_clip_val)\n",
    "    #     trainer_kwargs.update(user_train_kwargs)\n",
    "\n",
    "    #     data_splitter = DataSplitter(\n",
    "    #         self.adata_manager,\n",
    "    #         train_size=train_size,\n",
    "    #         validation_size=validation_size,\n",
    "    #         batch_size=batch_size,\n",
    "    #         use_gpu=use_gpu,\n",
    "    #     )\n",
    "    #     training_plan = TrainingPlan(self.module, **plan_kwargs)\n",
    "\n",
    "    #     es = \"early_stopping\"\n",
    "    #     trainer_kwargs[es] = (\n",
    "    #         early_stopping if es not in trainer_kwargs.keys() else trainer_kwargs[es]\n",
    "    #     )\n",
    "    #     runner = TrainRunner(\n",
    "    #         self,\n",
    "    #         training_plan=training_plan,\n",
    "    #         data_splitter=data_splitter,\n",
    "    #         max_epochs=max_epochs,\n",
    "    #         use_gpu=use_gpu,\n",
    "    #         **trainer_kwargs,\n",
    "    #     )\n",
    "    #     return runner()\n",
    "\n",
    "    \n",
    "    #optim = torch.optim.Adam(linear_reg_model.parameters(), lr=0.05)\n",
    "\n",
    "    def get_loss(self, adata):\n",
    "        # run the model forward on the data\n",
    "\n",
    "        adata = self._validate_anndata(adata)\n",
    "        scdl = self._make_data_loader(\n",
    "            adata=adata, indices=None, batch_size=256\n",
    "        )\n",
    "\n",
    "        for tensors in scdl:    \n",
    "            inference_outputs, generative_outputs, loss = self.module.forward(\n",
    "                tensors=tensors,\n",
    "                compute_loss=True,\n",
    "                )\n",
    "        \n",
    "        return inference_outputs, generative_outputs, loss\n",
    "        # # calculate the mse loss\n",
    "        \n",
    "        # # initialize gradients to zero\n",
    "        # optim.zero_grad()\n",
    "        # # backpropagate\n",
    "        # loss.backward()\n",
    "        # # take a gradient step\n",
    "        # optim.step()\n",
    "        # return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_state_assignment(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        indices: Optional[Sequence[int]] = None,\n",
    "        gene_list: Optional[Sequence[str]] = None,\n",
    "        hard_assignment: bool = False,\n",
    "        n_samples: int = 20,\n",
    "        batch_size: Optional[int] = None,\n",
    "        return_mean: bool = True,\n",
    "        return_numpy: Optional[bool] = None,\n",
    "    ) -> Tuple[Union[np.ndarray, pd.DataFrame], List[str]]:\n",
    "        \"\"\"\n",
    "        Returns cells by genes by states probabilities.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        adata\n",
    "            AnnData object with equivalent structure to initial AnnData. If `None`, defaults to the\n",
    "            AnnData object used to initialize the model.\n",
    "        indices\n",
    "            Indices of cells in adata to use. If `None`, all cells are used.\n",
    "        gene_list\n",
    "            Return frequencies of expression for a subset of genes.\n",
    "            This can save memory when working with large datasets and few genes are\n",
    "            of interest.\n",
    "        hard_assignment\n",
    "            Return a hard state assignment\n",
    "        n_samples\n",
    "            Number of posterior samples to use for estimation.\n",
    "        batch_size\n",
    "            Minibatch size for data loading into model. Defaults to `scvi.settings.batch_size`.\n",
    "        return_mean\n",
    "            Whether to return the mean of the samples.\n",
    "        return_numpy\n",
    "            Return a :class:`~numpy.ndarray` instead of a :class:`~pandas.DataFrame`. DataFrame includes\n",
    "            gene names as columns. If either `n_samples=1` or `return_mean=True`, defaults to `False`.\n",
    "            Otherwise, it defaults to `True`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        If `n_samples` > 1 and `return_mean` is False, then the shape is `(samples, cells, genes)`.\n",
    "        Otherwise, shape is `(cells, genes)`. In this case, return type is :class:`~pandas.DataFrame` unless `return_numpy` is True.\n",
    "        \"\"\"\n",
    "        adata = self._validate_anndata(adata)\n",
    "        scdl = self._make_data_loader(\n",
    "            adata=adata, indices=indices, batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        if gene_list is None:\n",
    "            gene_mask = slice(None)\n",
    "        else:\n",
    "            all_genes = adata.var_names\n",
    "            gene_mask = [True if gene in gene_list else False for gene in all_genes]\n",
    "\n",
    "        if n_samples > 1 and return_mean is False:\n",
    "            if return_numpy is False:\n",
    "                warnings.warn(\n",
    "                    \"return_numpy must be True if n_samples > 1 and return_mean is False, returning np.ndarray\"\n",
    "                )\n",
    "            return_numpy = True\n",
    "        if indices is None:\n",
    "            indices = np.arange(adata.n_obs)\n",
    "\n",
    "        states = []\n",
    "        for tensors in scdl:\n",
    "            minibatch_samples = []\n",
    "            for _ in range(n_samples):\n",
    "                _, generative_outputs = self.module.forward(\n",
    "                    tensors=tensors,\n",
    "                    compute_loss=True,\n",
    "                )\n",
    "                output = generative_outputs[\"px_pi\"]\n",
    "                output = output[..., gene_mask, :]\n",
    "                output = output.cpu().numpy()\n",
    "                minibatch_samples.append(output)\n",
    "            # samples by cells by genes by four\n",
    "            states.append(np.stack(minibatch_samples, axis=0))\n",
    "            if return_mean:\n",
    "                states[-1] = np.mean(states[-1], axis=0)\n",
    "\n",
    "        states = np.concatenate(states, axis=0)\n",
    "        state_cats = [\n",
    "            \"induction\",\n",
    "            \"induction_steady\",\n",
    "            \"repression\",\n",
    "            \"repression_steady\",\n",
    "        ]\n",
    "        if hard_assignment and return_mean:\n",
    "            hard_assign = states.argmax(-1)\n",
    "\n",
    "            hard_assign = pd.DataFrame(\n",
    "                data=hard_assign, index=adata.obs_names, columns=adata.var_names\n",
    "            )\n",
    "            for i, s in enumerate(state_cats):\n",
    "                hard_assign = hard_assign.replace(i, s)\n",
    "\n",
    "            states = hard_assign\n",
    "\n",
    "        return states, state_cats\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_latent_time(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        indices: Optional[Sequence[int]] = None,\n",
    "        gene_list: Optional[Sequence[str]] = None,\n",
    "        time_statistic: Literal[\"mean\", \"max\"] = \"mean\",\n",
    "        n_samples: int = 1,\n",
    "        n_samples_overall: Optional[int] = None,\n",
    "        batch_size: Optional[int] = None,\n",
    "        return_mean: bool = True,\n",
    "        return_numpy: Optional[bool] = None,\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Returns the cells by genes latent time.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        adata\n",
    "            AnnData object with equivalent structure to initial AnnData. If `None`, defaults to the\n",
    "            AnnData object used to initialize the model.\n",
    "        indices\n",
    "            Indices of cells in adata to use. If `None`, all cells are used.\n",
    "        gene_list\n",
    "            Return frequencies of expression for a subset of genes.\n",
    "            This can save memory when working with large datasets and few genes are\n",
    "            of interest.\n",
    "        time_statistic\n",
    "            Whether to compute expected time over states, or maximum a posteriori time over maximal\n",
    "            probability state.\n",
    "        n_samples\n",
    "            Number of posterior samples to use for estimation.\n",
    "        n_samples_overall\n",
    "            Number of overall samples to return. Setting this forces n_samples=1.\n",
    "        batch_size\n",
    "            Minibatch size for data loading into model. Defaults to `scvi.settings.batch_size`.\n",
    "        return_mean\n",
    "            Whether to return the mean of the samples.\n",
    "        return_numpy\n",
    "            Return a :class:`~numpy.ndarray` instead of a :class:`~pandas.DataFrame`. DataFrame includes\n",
    "            gene names as columns. If either `n_samples=1` or `return_mean=True`, defaults to `False`.\n",
    "            Otherwise, it defaults to `True`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        If `n_samples` > 1 and `return_mean` is False, then the shape is `(samples, cells, genes)`.\n",
    "        Otherwise, shape is `(cells, genes)`. In this case, return type is :class:`~pandas.DataFrame` unless `return_numpy` is True.\n",
    "        \"\"\"\n",
    "        adata = self._validate_anndata(adata)\n",
    "        if indices is None:\n",
    "            indices = np.arange(adata.n_obs)\n",
    "        if n_samples_overall is not None:\n",
    "            indices = np.random.choice(indices, n_samples_overall)\n",
    "        scdl = self._make_data_loader(\n",
    "            adata=adata, indices=indices, batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        if gene_list is None:\n",
    "            gene_mask = slice(None)\n",
    "        else:\n",
    "            all_genes = adata.var_names\n",
    "            gene_mask = [True if gene in gene_list else False for gene in all_genes]\n",
    "\n",
    "        if n_samples > 1 and return_mean is False:\n",
    "            if return_numpy is False:\n",
    "                warnings.warn(\n",
    "                    \"return_numpy must be True if n_samples > 1 and return_mean is False, returning np.ndarray\"\n",
    "                )\n",
    "            return_numpy = True\n",
    "        if indices is None:\n",
    "            indices = np.arange(adata.n_obs)\n",
    "\n",
    "        times = []\n",
    "        for tensors in scdl:\n",
    "            minibatch_samples = []\n",
    "            for _ in range(n_samples):\n",
    "                _, generative_outputs = self.module.forward(\n",
    "                    tensors=tensors,\n",
    "                    compute_loss=False,\n",
    "                )\n",
    "                pi = generative_outputs[\"px_pi\"]\n",
    "                ind_prob = pi[..., 0]\n",
    "                steady_prob = pi[..., 1]\n",
    "                rep_prob = pi[..., 2]\n",
    "                # rep_steady_prob = pi[..., 3]\n",
    "                switch_time = F.softplus(self.module.switch_time_unconstr)\n",
    "\n",
    "                ind_time = generative_outputs[\"px_rho\"] * switch_time\n",
    "                rep_time = switch_time + (\n",
    "                    generative_outputs[\"px_tau\"] * (self.module.t_max - switch_time)\n",
    "                )\n",
    "\n",
    "                if time_statistic == \"mean\":\n",
    "                    output = (\n",
    "                        ind_prob * ind_time\n",
    "                        + rep_prob * rep_time\n",
    "                        + steady_prob * switch_time\n",
    "                        # + rep_steady_prob * self.module.t_max\n",
    "                    )\n",
    "                else:\n",
    "                    t = torch.stack(\n",
    "                        [\n",
    "                            ind_time,\n",
    "                            switch_time.expand(ind_time.shape),\n",
    "                            rep_time,\n",
    "                            torch.zeros_like(ind_time),\n",
    "                        ],\n",
    "                        dim=2,\n",
    "                    )\n",
    "                    max_prob = torch.amax(pi, dim=-1)\n",
    "                    max_prob = torch.stack([max_prob] * 4, dim=2)\n",
    "                    max_prob_mask = pi.ge(max_prob)\n",
    "                    output = (t * max_prob_mask).sum(dim=-1)\n",
    "\n",
    "                output = output[..., gene_mask]\n",
    "                output = output.cpu().numpy()\n",
    "                minibatch_samples.append(output)\n",
    "            # samples by cells by genes by four\n",
    "            times.append(np.stack(minibatch_samples, axis=0))\n",
    "            if return_mean:\n",
    "                times[-1] = np.mean(times[-1], axis=0)\n",
    "\n",
    "        if n_samples > 1:\n",
    "            # The -2 axis correspond to cells.\n",
    "            times = np.concatenate(times, axis=-2)\n",
    "        else:\n",
    "            times = np.concatenate(times, axis=0)\n",
    "\n",
    "        if return_numpy is None or return_numpy is False:\n",
    "            return pd.DataFrame(\n",
    "                times,\n",
    "                columns=adata.var_names[gene_mask],\n",
    "                index=adata.obs_names[indices],\n",
    "            )\n",
    "        else:\n",
    "            return times\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_velocity(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        indices: Optional[Sequence[int]] = None,\n",
    "        gene_list: Optional[Sequence[str]] = None,\n",
    "        n_samples: int = 1,\n",
    "        n_samples_overall: Optional[int] = None,\n",
    "        batch_size: Optional[int] = None,\n",
    "        return_mean: bool = True,\n",
    "        return_numpy: Optional[bool] = None,\n",
    "        velo_statistic: str = \"mean\",\n",
    "        velo_mode: Literal[\"spliced\", \"unspliced\"] = \"spliced\",\n",
    "        clip: bool = True,\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Returns cells by genes velocity estimates.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        adata\n",
    "            AnnData object with equivalent structure to initial AnnData. If `None`, defaults to the\n",
    "            AnnData object used to initialize the model.\n",
    "        indices\n",
    "            Indices of cells in adata to use. If `None`, all cells are used.\n",
    "        gene_list\n",
    "            Return velocities for a subset of genes.\n",
    "            This can save memory when working with large datasets and few genes are\n",
    "            of interest.\n",
    "        n_samples\n",
    "            Number of posterior samples to use for estimation for each cell.\n",
    "        n_samples_overall\n",
    "            Number of overall samples to return. Setting this forces n_samples=1.\n",
    "        batch_size\n",
    "            Minibatch size for data loading into model. Defaults to `scvi.settings.batch_size`.\n",
    "        return_mean\n",
    "            Whether to return the mean of the samples.\n",
    "        return_numpy\n",
    "            Return a :class:`~numpy.ndarray` instead of a :class:`~pandas.DataFrame`. DataFrame includes\n",
    "            gene names as columns. If either `n_samples=1` or `return_mean=True`, defaults to `False`.\n",
    "            Otherwise, it defaults to `True`.\n",
    "        velo_statistic\n",
    "            Whether to compute expected velocity over states, or maximum a posteriori velocity over maximal\n",
    "            probability state.\n",
    "        velo_mode\n",
    "            Compute ds/dt or du/dt.\n",
    "        clip\n",
    "            Clip to minus spliced value\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        If `n_samples` > 1 and `return_mean` is False, then the shape is `(samples, cells, genes)`.\n",
    "        Otherwise, shape is `(cells, genes)`. In this case, return type is :class:`~pandas.DataFrame` unless `return_numpy` is True.\n",
    "        \"\"\"\n",
    "        adata = self._validate_anndata(adata)\n",
    "        if indices is None:\n",
    "            indices = np.arange(adata.n_obs)\n",
    "        if n_samples_overall is not None:\n",
    "            indices = np.random.choice(indices, n_samples_overall)\n",
    "            n_samples = 1\n",
    "        scdl = self._make_data_loader(\n",
    "            adata=adata, indices=indices, batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        if gene_list is None:\n",
    "            gene_mask = slice(None)\n",
    "        else:\n",
    "            all_genes = adata.var_names\n",
    "            gene_mask = [True if gene in gene_list else False for gene in all_genes]\n",
    "\n",
    "        if n_samples > 1 and return_mean is False:\n",
    "            if return_numpy is False:\n",
    "                warnings.warn(\n",
    "                    \"return_numpy must be True if n_samples > 1 and return_mean is False, returning np.ndarray\"\n",
    "                )\n",
    "            return_numpy = True\n",
    "        if indices is None:\n",
    "            indices = np.arange(adata.n_obs)\n",
    "\n",
    "        velos = []\n",
    "        for tensors in scdl:\n",
    "            minibatch_samples = []\n",
    "            for _ in range(n_samples):\n",
    "                inference_outputs, generative_outputs = self.module.forward(\n",
    "                    tensors=tensors,\n",
    "                    compute_loss=False,\n",
    "                )\n",
    "                pi = generative_outputs[\"px_pi\"]\n",
    "                alpha = inference_outputs[\"alpha\"]\n",
    "                alpha_1 = inference_outputs[\"alpha_1\"]\n",
    "                lambda_alpha = inference_outputs[\"lambda_alpha\"]\n",
    "                beta = inference_outputs[\"beta\"]\n",
    "                gamma = inference_outputs[\"gamma\"]\n",
    "                tau = generative_outputs[\"px_tau\"]\n",
    "                rho = generative_outputs[\"px_rho\"]\n",
    "\n",
    "                ind_prob = pi[..., 0]\n",
    "                steady_prob = pi[..., 1]\n",
    "                rep_prob = pi[..., 2]\n",
    "                switch_time = F.softplus(self.module.switch_time_unconstr)\n",
    "\n",
    "                ind_time = switch_time * rho\n",
    "                u_0, s_0 = self.module._get_induction_unspliced_spliced(\n",
    "                    alpha, alpha_1, lambda_alpha, beta, gamma, switch_time\n",
    "                )\n",
    "                rep_time = (self.module.t_max - switch_time) * tau\n",
    "                mean_u_rep, mean_s_rep = self.module._get_repression_unspliced_spliced(\n",
    "                    u_0,\n",
    "                    s_0,\n",
    "                    beta,\n",
    "                    gamma,\n",
    "                    rep_time,\n",
    "                )\n",
    "                if velo_mode == \"spliced\":\n",
    "                    velo_rep = beta * mean_u_rep - gamma * mean_s_rep\n",
    "                else:\n",
    "                    velo_rep = -beta * mean_u_rep\n",
    "                mean_u_ind, mean_s_ind = self.module._get_induction_unspliced_spliced(\n",
    "                    alpha, alpha_1, lambda_alpha, beta, gamma, ind_time\n",
    "                )\n",
    "                if velo_mode == \"spliced\":\n",
    "                    velo_ind = beta * mean_u_ind - gamma * mean_s_ind\n",
    "                else:\n",
    "                    transcription_rate = alpha_1 - (alpha_1 - alpha) * torch.exp(\n",
    "                        -lambda_alpha * ind_time\n",
    "                    )\n",
    "                    velo_ind = transcription_rate - beta * mean_u_ind\n",
    "\n",
    "                if velo_mode == \"spliced\":\n",
    "                    # velo_steady = beta * u_0 - gamma * s_0\n",
    "                    velo_steady = torch.zeros_like(velo_ind)\n",
    "                else:\n",
    "                    # velo_steady = alpha - beta * u_0\n",
    "                    velo_steady = torch.zeros_like(velo_ind)\n",
    "\n",
    "                # expectation\n",
    "                if velo_statistic == \"mean\":\n",
    "                    output = (\n",
    "                        ind_prob * velo_ind\n",
    "                        + rep_prob * velo_rep\n",
    "                        + steady_prob * velo_steady\n",
    "                    )\n",
    "                # maximum\n",
    "                else:\n",
    "                    v = torch.stack(\n",
    "                        [\n",
    "                            velo_ind,\n",
    "                            velo_steady.expand(velo_ind.shape),\n",
    "                            velo_rep,\n",
    "                            torch.zeros_like(velo_rep),\n",
    "                        ],\n",
    "                        dim=2,\n",
    "                    )\n",
    "                    max_prob = torch.amax(pi, dim=-1)\n",
    "                    max_prob = torch.stack([max_prob] * 4, dim=2)\n",
    "                    max_prob_mask = pi.ge(max_prob)\n",
    "                    output = (v * max_prob_mask).sum(dim=-1)\n",
    "\n",
    "                output = output[..., gene_mask]\n",
    "                output = output.cpu().numpy()\n",
    "                minibatch_samples.append(output)\n",
    "            # samples by cells by genes\n",
    "            velos.append(np.stack(minibatch_samples, axis=0))\n",
    "            if return_mean:\n",
    "                # mean over samples axis\n",
    "                velos[-1] = np.mean(velos[-1], axis=0)\n",
    "\n",
    "        if n_samples > 1:\n",
    "            # The -2 axis correspond to cells.\n",
    "            velos = np.concatenate(velos, axis=-2)\n",
    "        else:\n",
    "            velos = np.concatenate(velos, axis=0)\n",
    "\n",
    "        spliced = self.adata_manager.get_from_registry(REGISTRY_KEYS.X_KEY)\n",
    "\n",
    "        if clip:\n",
    "            velos = np.clip(velos, -spliced[indices], None)\n",
    "\n",
    "        if return_numpy is None or return_numpy is False:\n",
    "            return pd.DataFrame(\n",
    "                velos,\n",
    "                columns=adata.var_names[gene_mask],\n",
    "                index=adata.obs_names[indices],\n",
    "            )\n",
    "        else:\n",
    "            return velos\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_velocity_from_latent(\n",
    "        self,\n",
    "        latent_representation: np.ndarray,\n",
    "        return_numpy: Optional[bool] = None,\n",
    "        velo_statistic: str = \"mean\",\n",
    "        velo_mode: Literal[\"spliced\", \"unspliced\"] = \"spliced\",\n",
    "        clip: bool = True,\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        r\"\"\"\n",
    "        Returns the normalized (decoded) gene expression.\n",
    "\n",
    "        This is denoted as :math:`\\rho_n` in the scVI paper.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        adata\n",
    "            AnnData object with equivalent structure to initial AnnData. If `None`, defaults to the\n",
    "            AnnData object used to initialize the model.\n",
    "        return_numpy\n",
    "            Return a :class:`~numpy.ndarray` instead of a :class:`~pandas.DataFrame`. DataFrame includes\n",
    "            gene names as columns. If either `n_samples=1` or `return_mean=True`, defaults to `False`.\n",
    "            Otherwise, it defaults to `True`.\n",
    "        clip\n",
    "            Clip to minus spliced value\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        If `n_samples` > 1 and `return_mean` is False, then the shape is `(samples, cells, genes)`.\n",
    "        Otherwise, shape is `(cells, genes)`. In this case, return type is :class:`~pandas.DataFrame` unless `return_numpy` is True.\n",
    "        \"\"\"\n",
    "        adata = AnnData(latent_representation)\n",
    "        data_key = \"Z\"\n",
    "        manager = AnnDataManager(\n",
    "            [LayerField(data_key, layer=None, is_count_data=False)]\n",
    "        )\n",
    "        manager.register_fields(adata)\n",
    "        scdl = AnnDataLoader(manager)\n",
    "\n",
    "        gamma, beta, alpha, alpha_1, lambda_alpha = self.module._get_rates()\n",
    "\n",
    "        velos = []\n",
    "        for tensors in scdl:\n",
    "            z = tensors[data_key]\n",
    "            generative_outputs = self.module.generative(\n",
    "                z=z,\n",
    "                gamma=gamma,\n",
    "                beta=beta,\n",
    "                alpha=alpha,\n",
    "                alpha_1=alpha_1,\n",
    "                lambda_alpha=lambda_alpha,\n",
    "            )\n",
    "            pi = generative_outputs[\"px_pi\"]\n",
    "            tau = generative_outputs[\"px_tau\"]\n",
    "            rho = generative_outputs[\"px_rho\"]\n",
    "\n",
    "            ind_prob = pi[..., 0]\n",
    "            steady_prob = pi[..., 1]\n",
    "            rep_prob = pi[..., 2]\n",
    "            switch_time = F.softplus(self.module.switch_time_unconstr)\n",
    "\n",
    "            ind_time = switch_time * rho\n",
    "            u_0, s_0 = self.module._get_induction_unspliced_spliced(\n",
    "                alpha, alpha_1, lambda_alpha, beta, gamma, switch_time\n",
    "            )\n",
    "            rep_time = (self.module.t_max - switch_time) * tau\n",
    "            mean_u_rep, mean_s_rep = self.module._get_repression_unspliced_spliced(\n",
    "                u_0,\n",
    "                s_0,\n",
    "                beta,\n",
    "                gamma,\n",
    "                rep_time,\n",
    "            )\n",
    "            if velo_mode == \"spliced\":\n",
    "                velo_rep = beta * mean_u_rep - gamma * mean_s_rep\n",
    "            else:\n",
    "                velo_rep = -beta * mean_u_rep\n",
    "            mean_u_ind, mean_s_ind = self.module._get_induction_unspliced_spliced(\n",
    "                alpha, alpha_1, lambda_alpha, beta, gamma, ind_time\n",
    "            )\n",
    "            if velo_mode == \"spliced\":\n",
    "                velo_ind = beta * mean_u_ind - gamma * mean_s_ind\n",
    "            else:\n",
    "                transcription_rate = alpha_1 - (alpha_1 - alpha) * torch.exp(\n",
    "                    -lambda_alpha * ind_time\n",
    "                )\n",
    "                velo_ind = transcription_rate - beta * mean_u_ind\n",
    "\n",
    "            if velo_mode == \"spliced\":\n",
    "                # velo_steady = beta * u_0 - gamma * s_0\n",
    "                velo_steady = torch.zeros_like(velo_ind)\n",
    "            else:\n",
    "                # velo_steady = alpha - beta * u_0\n",
    "                velo_steady = torch.zeros_like(velo_ind)\n",
    "\n",
    "            # expectation\n",
    "            if velo_statistic == \"mean\":\n",
    "                output = (\n",
    "                    ind_prob * velo_ind\n",
    "                    + rep_prob * velo_rep\n",
    "                    + steady_prob * velo_steady\n",
    "                )\n",
    "            # maximum\n",
    "            else:\n",
    "                v = torch.stack(\n",
    "                    [\n",
    "                        velo_ind,\n",
    "                        velo_steady.expand(velo_ind.shape),\n",
    "                        velo_rep,\n",
    "                        torch.zeros_like(velo_rep),\n",
    "                    ],\n",
    "                    dim=2,\n",
    "                )\n",
    "                max_prob = torch.amax(pi, dim=-1)\n",
    "                max_prob = torch.stack([max_prob] * 4, dim=2)\n",
    "                max_prob_mask = pi.ge(max_prob)\n",
    "                output = (v * max_prob_mask).sum(dim=-1)\n",
    "\n",
    "            # samples by cells by genes\n",
    "            velos.append(output.cpu().numpy())\n",
    "\n",
    "        velos = np.concatenate(velos, axis=0)\n",
    "\n",
    "        if return_numpy is None or return_numpy is False:\n",
    "            return pd.DataFrame(\n",
    "                velos,\n",
    "                columns=self.adata.var_names,\n",
    "            )\n",
    "        else:\n",
    "            return velos\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_expression_fit(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        indices: Optional[Sequence[int]] = None,\n",
    "        gene_list: Optional[Sequence[str]] = None,\n",
    "        n_samples: int = 1,\n",
    "        batch_size: Optional[int] = None,\n",
    "        return_mean: bool = True,\n",
    "        return_numpy: Optional[bool] = None,\n",
    "        restrict_to_latent_dim: Optional[int] = None,\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        r\"\"\"\n",
    "        Returns the fitted spliced and unspliced abundance (s(t) and u(t)).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        adata\n",
    "            AnnData object with equivalent structure to initial AnnData. If `None`, defaults to the\n",
    "            AnnData object used to initialize the model.\n",
    "        indices\n",
    "            Indices of cells in adata to use. If `None`, all cells are used.\n",
    "        gene_list\n",
    "            Return frequencies of expression for a subset of genes.\n",
    "            This can save memory when working with large datasets and few genes are\n",
    "            of interest.\n",
    "        n_samples\n",
    "            Number of posterior samples to use for estimation.\n",
    "        batch_size\n",
    "            Minibatch size for data loading into model. Defaults to `scvi.settings.batch_size`.\n",
    "        return_mean\n",
    "            Whether to return the mean of the samples.\n",
    "        return_numpy\n",
    "            Return a :class:`~numpy.ndarray` instead of a :class:`~pandas.DataFrame`. DataFrame includes\n",
    "            gene names as columns. If either `n_samples=1` or `return_mean=True`, defaults to `False`.\n",
    "            Otherwise, it defaults to `True`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        If `n_samples` > 1 and `return_mean` is False, then the shape is `(samples, cells, genes)`.\n",
    "        Otherwise, shape is `(cells, genes)`. In this case, return type is :class:`~pandas.DataFrame` unless `return_numpy` is True.\n",
    "        \"\"\"\n",
    "        adata = self._validate_anndata(adata)\n",
    "\n",
    "        scdl = self._make_data_loader(\n",
    "            adata=adata, indices=indices, batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        if gene_list is None:\n",
    "            gene_mask = slice(None)\n",
    "        else:\n",
    "            all_genes = adata.var_names\n",
    "            gene_mask = [True if gene in gene_list else False for gene in all_genes]\n",
    "\n",
    "        if n_samples > 1 and return_mean is False:\n",
    "            if return_numpy is False:\n",
    "                warnings.warn(\n",
    "                    \"return_numpy must be True if n_samples > 1 and return_mean is False, returning np.ndarray\"\n",
    "                )\n",
    "            return_numpy = True\n",
    "        if indices is None:\n",
    "            indices = np.arange(adata.n_obs)\n",
    "\n",
    "        fits_s = []\n",
    "        fits_u = []\n",
    "        for tensors in scdl:\n",
    "            minibatch_samples_s = []\n",
    "            minibatch_samples_u = []\n",
    "            for _ in range(n_samples):\n",
    "                inference_outputs, generative_outputs = self.module.forward(\n",
    "                    tensors=tensors,\n",
    "                    compute_loss=False,\n",
    "                    generative_kwargs=dict(latent_dim=restrict_to_latent_dim),\n",
    "                )\n",
    "\n",
    "                gamma = inference_outputs[\"gamma\"]\n",
    "                beta = inference_outputs[\"beta\"]\n",
    "                alpha = inference_outputs[\"alpha\"]\n",
    "                alpha_1 = inference_outputs[\"alpha_1\"]\n",
    "                lambda_alpha = inference_outputs[\"lambda_alpha\"]\n",
    "                px_pi = generative_outputs[\"px_pi\"]\n",
    "                scale = generative_outputs[\"scale\"]\n",
    "                px_rho = generative_outputs[\"px_rho\"]\n",
    "                px_tau = generative_outputs[\"px_tau\"]\n",
    "\n",
    "                (mixture_dist_s, mixture_dist_u, _,) = self.module.get_px(\n",
    "                    px_pi,\n",
    "                    px_rho,\n",
    "                    px_tau,\n",
    "                    scale,\n",
    "                    gamma,\n",
    "                    beta,\n",
    "                    alpha,\n",
    "                    alpha_1,\n",
    "                    lambda_alpha,\n",
    "                )\n",
    "                fit_s = mixture_dist_s.mean\n",
    "                fit_u = mixture_dist_u.mean\n",
    "\n",
    "                fit_s = fit_s[..., gene_mask]\n",
    "                fit_s = fit_s.cpu().numpy()\n",
    "                fit_u = fit_u[..., gene_mask]\n",
    "                fit_u = fit_u.cpu().numpy()\n",
    "\n",
    "                minibatch_samples_s.append(fit_s)\n",
    "                minibatch_samples_u.append(fit_u)\n",
    "\n",
    "            # samples by cells by genes\n",
    "            fits_s.append(np.stack(minibatch_samples_s, axis=0))\n",
    "            if return_mean:\n",
    "                # mean over samples axis\n",
    "                fits_s[-1] = np.mean(fits_s[-1], axis=0)\n",
    "            # samples by cells by genes\n",
    "            fits_u.append(np.stack(minibatch_samples_u, axis=0))\n",
    "            if return_mean:\n",
    "                # mean over samples axis\n",
    "                fits_u[-1] = np.mean(fits_u[-1], axis=0)\n",
    "\n",
    "        if n_samples > 1:\n",
    "            # The -2 axis correspond to cells.\n",
    "            fits_s = np.concatenate(fits_s, axis=-2)\n",
    "            fits_u = np.concatenate(fits_u, axis=-2)\n",
    "        else:\n",
    "            fits_s = np.concatenate(fits_s, axis=0)\n",
    "            fits_u = np.concatenate(fits_u, axis=0)\n",
    "\n",
    "        if return_numpy is None or return_numpy is False:\n",
    "            df_s = pd.DataFrame(\n",
    "                fits_s,\n",
    "                columns=adata.var_names[gene_mask],\n",
    "                index=adata.obs_names[indices],\n",
    "            )\n",
    "            df_u = pd.DataFrame(\n",
    "                fits_u,\n",
    "                columns=adata.var_names[gene_mask],\n",
    "                index=adata.obs_names[indices],\n",
    "            )\n",
    "            return df_s, df_u\n",
    "        else:\n",
    "            return fits_s, fits_u\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_gene_likelihood(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        indices: Optional[Sequence[int]] = None,\n",
    "        gene_list: Optional[Sequence[str]] = None,\n",
    "        n_samples: int = 1,\n",
    "        batch_size: Optional[int] = None,\n",
    "        return_mean: bool = True,\n",
    "        return_numpy: Optional[bool] = None,\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        r\"\"\"\n",
    "        Returns the likelihood per gene. Higher is better.\n",
    "\n",
    "        This is denoted as :math:`\\rho_n` in the scVI paper.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        adata\n",
    "            AnnData object with equivalent structure to initial AnnData. If `None`, defaults to the\n",
    "            AnnData object used to initialize the model.\n",
    "        indices\n",
    "            Indices of cells in adata to use. If `None`, all cells are used.\n",
    "        transform_batch\n",
    "            Batch to condition on.\n",
    "            If transform_batch is:\n",
    "\n",
    "            - None, then real observed batch is used.\n",
    "            - int, then batch transform_batch is used.\n",
    "        gene_list\n",
    "            Return frequencies of expression for a subset of genes.\n",
    "            This can save memory when working with large datasets and few genes are\n",
    "            of interest.\n",
    "        library_size\n",
    "            Scale the expression frequencies to a common library size.\n",
    "            This allows gene expression levels to be interpreted on a common scale of relevant\n",
    "            magnitude. If set to `\"latent\"`, use the latent libary size.\n",
    "        n_samples\n",
    "            Number of posterior samples to use for estimation.\n",
    "        batch_size\n",
    "            Minibatch size for data loading into model. Defaults to `scvi.settings.batch_size`.\n",
    "        return_mean\n",
    "            Whether to return the mean of the samples.\n",
    "        return_numpy\n",
    "            Return a :class:`~numpy.ndarray` instead of a :class:`~pandas.DataFrame`. DataFrame includes\n",
    "            gene names as columns. If either `n_samples=1` or `return_mean=True`, defaults to `False`.\n",
    "            Otherwise, it defaults to `True`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        If `n_samples` > 1 and `return_mean` is False, then the shape is `(samples, cells, genes)`.\n",
    "        Otherwise, shape is `(cells, genes)`. In this case, return type is :class:`~pandas.DataFrame` unless `return_numpy` is True.\n",
    "        \"\"\"\n",
    "        adata = self._validate_anndata(adata)\n",
    "        scdl = self._make_data_loader(\n",
    "            adata=adata, indices=indices, batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        if gene_list is None:\n",
    "            gene_mask = slice(None)\n",
    "        else:\n",
    "            all_genes = adata.var_names\n",
    "            gene_mask = [True if gene in gene_list else False for gene in all_genes]\n",
    "\n",
    "        if n_samples > 1 and return_mean is False:\n",
    "            if return_numpy is False:\n",
    "                warnings.warn(\n",
    "                    \"return_numpy must be True if n_samples > 1 and return_mean is False, returning np.ndarray\"\n",
    "                )\n",
    "            return_numpy = True\n",
    "        if indices is None:\n",
    "            indices = np.arange(adata.n_obs)\n",
    "\n",
    "        rls = []\n",
    "        for tensors in scdl:\n",
    "            minibatch_samples = []\n",
    "            for _ in range(n_samples):\n",
    "                inference_outputs, generative_outputs = self.module.forward(\n",
    "                    tensors=tensors,\n",
    "                    compute_loss=False,\n",
    "                )\n",
    "                spliced = tensors[REGISTRY_KEYS.X_KEY]\n",
    "                unspliced = tensors[REGISTRY_KEYS.U_KEY]\n",
    "\n",
    "                gamma = inference_outputs[\"gamma\"]\n",
    "                beta = inference_outputs[\"beta\"]\n",
    "                alpha = inference_outputs[\"alpha\"]\n",
    "                alpha_1 = inference_outputs[\"alpha_1\"]\n",
    "                lambda_alpha = inference_outputs[\"lambda_alpha\"]\n",
    "                px_pi = generative_outputs[\"px_pi\"]\n",
    "                scale = generative_outputs[\"scale\"]\n",
    "                px_rho = generative_outputs[\"px_rho\"]\n",
    "                px_tau = generative_outputs[\"px_tau\"]\n",
    "                dec_mean = generative_outputs[\"gene_recon\"]\n",
    "                dec_lat = generative_outputs[\"dec_latent\"]\n",
    "\n",
    "                (mixture_dist_s, mixture_dist_u, _,) = self.module.get_px(\n",
    "                    px_pi,\n",
    "                    px_rho,\n",
    "                    px_tau,\n",
    "                    scale,\n",
    "                    gamma,\n",
    "                    beta,\n",
    "                    alpha,\n",
    "                    alpha_1,\n",
    "                    lambda_alpha,\n",
    "                )\n",
    "                reconst_loss_s = -mixture_dist_s.log_prob(spliced)\n",
    "                reconst_loss_u = -mixture_dist_u.log_prob(unspliced)\n",
    "                output = -(reconst_loss_s + reconst_loss_u)\n",
    "                output = output[..., gene_mask]\n",
    "                output = output.cpu().numpy()\n",
    "                minibatch_samples.append(output)\n",
    "            # samples by cells by genes by four\n",
    "            rls.append(np.stack(minibatch_samples, axis=0))\n",
    "            if return_mean:\n",
    "                rls[-1] = np.mean(rls[-1], axis=0)\n",
    "\n",
    "        rls = np.concatenate(rls, axis=0)\n",
    "        return rls.shape, dec_mean.shape, dec_lat.shape\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_rates(self, mean: bool = True):\n",
    "\n",
    "        gamma, beta, alpha, alpha_1, lambda_alpha = self.module._get_rates()\n",
    "\n",
    "        return {\n",
    "            \"beta\": beta.cpu().numpy(),\n",
    "            \"gamma\": gamma.cpu().numpy(),\n",
    "            \"alpha\": alpha.cpu().numpy(),\n",
    "            \"alpha_1\": alpha_1.cpu().numpy(),\n",
    "            \"lambda_alpha\": lambda_alpha.cpu().numpy(),\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    @setup_anndata_dsp.dedent\n",
    "    def setup_anndata(\n",
    "        cls,\n",
    "        adata: AnnData,\n",
    "        spliced_layer: str,\n",
    "        unspliced_layer: str,\n",
    "        **kwargs,\n",
    "    ) -> Optional[AnnData]:\n",
    "        \"\"\"\n",
    "        %(summary)s.\n",
    "        Parameters\n",
    "        ----------\n",
    "        %(param_adata)s\n",
    "        spliced_layer\n",
    "            Layer in adata with spliced normalized expression\n",
    "        unspliced_layer\n",
    "            Layer in adata with unspliced normalized expression\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        %(returns)s\n",
    "        \"\"\"\n",
    "        setup_method_args = cls._get_setup_method_args(**locals())\n",
    "        anndata_fields = [\n",
    "            LayerField(REGISTRY_KEYS.X_KEY, spliced_layer, is_count_data=False),\n",
    "            LayerField(REGISTRY_KEYS.U_KEY, unspliced_layer, is_count_data=False),\n",
    "        ]\n",
    "        adata_manager = AnnDataManager(\n",
    "            fields=anndata_fields, setup_method_args=setup_method_args\n",
    "        )\n",
    "        adata_manager.register_fields(adata, **kwargs)\n",
    "        cls.register_manager(adata_manager)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @_doc_params(\n",
    "        doc_differential_expression=doc_differential_expression,\n",
    "    )\n",
    "    def differential_velocity(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        groupby: Optional[str] = None,\n",
    "        group1: Optional[Iterable[str]] = None,\n",
    "        group2: Optional[str] = None,\n",
    "        idx1: Optional[Union[Sequence[int], Sequence[bool], str]] = None,\n",
    "        idx2: Optional[Union[Sequence[int], Sequence[bool], str]] = None,\n",
    "        mode: Literal[\"vanilla\", \"change\"] = \"vanilla\",\n",
    "        delta: float = 0.25,\n",
    "        batch_size: Optional[int] = None,\n",
    "        all_stats: bool = True,\n",
    "        batch_correction: bool = False,\n",
    "        batchid1: Optional[Iterable[str]] = None,\n",
    "        batchid2: Optional[Iterable[str]] = None,\n",
    "        fdr_target: float = 0.05,\n",
    "        silent: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> pd.DataFrame:\n",
    "        r\"\"\"\n",
    "        A unified method for differential velocity analysis.\n",
    "\n",
    "        Implements `\"vanilla\"` DE [Lopez18]_ and `\"change\"` mode DE [Boyeau19]_.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        {doc_differential_expression}\n",
    "        **kwargs\n",
    "            Keyword args for :meth:`scvi.model.base.DifferentialComputation.get_bayes_factors`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Differential expression DataFrame.\n",
    "        \"\"\"\n",
    "        adata = self._validate_anndata(adata)\n",
    "\n",
    "        def model_fn(adata, **kwargs):\n",
    "            if \"transform_batch\" in kwargs.keys():\n",
    "                kwargs.pop(\"transform_batch\")\n",
    "            return partial(\n",
    "                self.get_velocity,\n",
    "                batch_size=batch_size,\n",
    "                n_samples=1,\n",
    "                return_numpy=True,\n",
    "                clip=False,\n",
    "            )(adata, **kwargs)\n",
    "\n",
    "        col_names = adata.var_names\n",
    "\n",
    "        result = _de_core(\n",
    "            self.get_anndata_manager(adata, required=True),\n",
    "            model_fn,\n",
    "            groupby,\n",
    "            group1,\n",
    "            group2,\n",
    "            idx1,\n",
    "            idx2,\n",
    "            all_stats,\n",
    "            scrna_raw_counts_properties,\n",
    "            col_names,\n",
    "            mode,\n",
    "            batchid1,\n",
    "            batchid2,\n",
    "            delta,\n",
    "            batch_correction,\n",
    "            fdr_target,\n",
    "            silent,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def differential_transition(\n",
    "        self,\n",
    "        groupby: str,\n",
    "        group1: str,\n",
    "        group2: str,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        batch_size: Optional[int] = None,\n",
    "        n_samples: Optional[int] = 5000,\n",
    "    ) -> pd.DataFrame:\n",
    "        adata = self._validate_anndata(adata)\n",
    "        adata_manager = self.get_anndata_manager(adata, required=True)\n",
    "\n",
    "        if not isinstance(group1, str):\n",
    "            raise ValueError(\"Group 1 must be a string\")\n",
    "\n",
    "        cell_idx1 = (adata.obs[groupby] == group1).to_numpy().ravel()\n",
    "        if group2 is None:\n",
    "            cell_idx2 = ~cell_idx1\n",
    "        else:\n",
    "            cell_idx2 = (adata.obs[groupby] == group2).to_numpy().ravel()\n",
    "\n",
    "        indices1 = np.random.choice(\n",
    "            np.asarray(np.where(cell_idx1)[0].ravel()), n_samples\n",
    "        )\n",
    "        indices2 = np.random.choice(\n",
    "            np.asarray(np.where(cell_idx2)[0].ravel()), n_samples\n",
    "        )\n",
    "\n",
    "        velo1 = self.get_velocity(\n",
    "            adata,\n",
    "            return_numpy=True,\n",
    "            indices=indices1,\n",
    "            n_samples=1,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        velo1 = velo1 - velo1.mean(1)[:, np.newaxis]\n",
    "        velo2 = self.get_velocity(\n",
    "            adata,\n",
    "            return_numpy=True,\n",
    "            indices=indices2,\n",
    "            n_samples=1,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        velo2 = velo2 - velo2.mean(1)[:, np.newaxis]\n",
    "\n",
    "        spliced = adata_manager.get_from_registry(REGISTRY_KEYS.X_KEY)\n",
    "        delta12 = spliced[indices2] - spliced[indices1]\n",
    "        delta12 = delta12 - delta12.mean(1)[:, np.newaxis]\n",
    "\n",
    "        delta21 = spliced[indices1] - spliced[indices2]\n",
    "        delta21 = delta21 - delta21.mean(1)[:, np.newaxis]\n",
    "\n",
    "        # TODO: Make more efficient\n",
    "        correlation12 = np.diagonal(cosine_similarity(velo1, delta12))\n",
    "        correlation21 = np.diagonal(cosine_similarity(velo2, delta21))\n",
    "\n",
    "        return correlation12, correlation21\n",
    "\n",
    "    def get_loadings(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract per-gene weights in the linear decoder.\n",
    "\n",
    "        Shape is genes by `n_latent`.\n",
    "        \"\"\"\n",
    "        cols = [\"Z_{}\".format(i) for i in range(self.n_latent)]\n",
    "        var_names = self.adata.var_names\n",
    "        loadings = pd.DataFrame(\n",
    "            self.module.get_loadings(), index=var_names, columns=cols\n",
    "        )\n",
    "\n",
    "        return loadings\n",
    "\n",
    "    def get_variance_explained(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        labels_key: Optional[str] = None,\n",
    "        n_samples: int = 10,\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "        if self.module.decoder.linear_decoder is False:\n",
    "            raise ValueError(\"Model not trained with linear decoder\")\n",
    "        adata = self._validate_anndata(adata)\n",
    "        adata_manager = self.get_anndata_manager(adata)\n",
    "        n_latent = self.module.n_latent\n",
    "\n",
    "        if labels_key is not None:\n",
    "            groups = np.unique(adata.obs[labels_key])\n",
    "        else:\n",
    "            groups = [None]\n",
    "\n",
    "        spliced = adata_manager.get_from_registry(REGISTRY_KEYS.X_KEY)\n",
    "        unspliced = adata_manager.get_from_registry(REGISTRY_KEYS.U_KEY)\n",
    "\n",
    "        centered_s = spliced - spliced.mean(0)\n",
    "        centered_u = unspliced - unspliced.mean(0)\n",
    "\n",
    "        def r_squared(true_s, pred_s, true_u, pred_u, centered_s, centered_u):\n",
    "            rss_s = np.sum((true_s - pred_s) ** 2)\n",
    "            tss_s = np.sum(centered_s**2)\n",
    "            rss_u = np.sum((true_u - pred_u) ** 2)\n",
    "            tss_u = np.sum(centered_u**2)\n",
    "\n",
    "            return (1 - (rss_s + rss_u) / (tss_s + tss_u)) * 100\n",
    "\n",
    "        df_out = pd.DataFrame(\n",
    "            data=np.zeros((n_latent, len(groups))),\n",
    "            index=[f\"Z_{i}\" for i in range(n_latent)],\n",
    "            columns=groups if groups[0] is not None else [\"0\"],\n",
    "        )\n",
    "\n",
    "        for i in range(n_latent):\n",
    "            fitted_s, fitted_u = self.get_expression_fit(\n",
    "                adata, restrict_to_latent_dim=i, n_samples=n_samples, return_numpy=True\n",
    "            )\n",
    "            for j, g in enumerate(groups):\n",
    "                if g is None:\n",
    "                    subset = slice(None)\n",
    "                else:\n",
    "                    subset = adata.obs[labels_key] == g\n",
    "                r_2 = r_squared(\n",
    "                    spliced[subset],\n",
    "                    fitted_s[subset],\n",
    "                    unspliced[subset],\n",
    "                    fitted_u[subset],\n",
    "                    centered_s[subset],\n",
    "                    centered_u[subset],\n",
    "                )\n",
    "                df_out.iloc[i, j] = r_2\n",
    "\n",
    "        return df_out\n",
    "\n",
    "    def get_directional_uncertainty(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        n_samples: int = 50,\n",
    "        gene_list: Iterable[str] = None,\n",
    "        n_jobs: int = -1,\n",
    "    ):\n",
    "\n",
    "        adata = self._validate_anndata(adata)\n",
    "\n",
    "        logger.info(\"Sampling from model...\")\n",
    "        velocities_all = self.get_velocity(\n",
    "            n_samples=n_samples, return_mean=False, gene_list=gene_list\n",
    "        )  # (n_samples, n_cells, n_genes)\n",
    "\n",
    "        df, cosine_sims = _compute_directional_statistics_tensor(\n",
    "            tensor=velocities_all, n_jobs=n_jobs, n_cells=adata.n_obs\n",
    "        )\n",
    "        df.index = adata.obs_names\n",
    "\n",
    "        return df, cosine_sims\n",
    "\n",
    "    def get_permutation_scores(\n",
    "        self, labels_key: str, adata: Optional[AnnData] = None\n",
    "    ) -> Tuple[pd.DataFrame, AnnData]:\n",
    "        \"\"\"\n",
    "        Compute permutation scores.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        labels_key\n",
    "            Key in adata.obs encoding cell types\n",
    "        adata\n",
    "            AnnData object with equivalent structure to initial AnnData. If `None`, defaults to the\n",
    "            AnnData object used to initialize the model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple of DataFrame and AnnData. DataFrame is genes by cell types with score per cell type.\n",
    "        AnnData is the permutated version of the original AnnData.\n",
    "        \"\"\"\n",
    "        adata = self._validate_anndata(adata)\n",
    "        adata_manager = self.get_anndata_manager(adata)\n",
    "        if labels_key not in adata.obs:\n",
    "            raise ValueError(f\"{labels_key} not found in adata.obs\")\n",
    "\n",
    "        # shuffle spliced then unspliced\n",
    "        bdata = self._shuffle_layer_celltype(\n",
    "            adata_manager, labels_key, REGISTRY_KEYS.X_KEY\n",
    "        )\n",
    "        bdata_manager = self.get_anndata_manager(bdata)\n",
    "        bdata = self._shuffle_layer_celltype(\n",
    "            bdata_manager, labels_key, REGISTRY_KEYS.U_KEY\n",
    "        )\n",
    "        bdata_manager = self.get_anndata_manager(bdata)\n",
    "\n",
    "        ms_ = adata_manager.get_from_registry(REGISTRY_KEYS.X_KEY)\n",
    "        mu_ = adata_manager.get_from_registry(REGISTRY_KEYS.U_KEY)\n",
    "\n",
    "        ms_p = bdata_manager.get_from_registry(REGISTRY_KEYS.X_KEY)\n",
    "        mu_p = bdata_manager.get_from_registry(REGISTRY_KEYS.U_KEY)\n",
    "\n",
    "        spliced_, unspliced_ = self.get_expression_fit(adata, n_samples=10)\n",
    "        root_squared_error = np.abs(spliced_ - ms_)\n",
    "        root_squared_error += np.abs(unspliced_ - mu_)\n",
    "\n",
    "        spliced_p, unspliced_p = self.get_expression_fit(bdata, n_samples=10)\n",
    "        root_squared_error_p = np.abs(spliced_p - ms_p)\n",
    "        root_squared_error_p += np.abs(unspliced_p - mu_p)\n",
    "\n",
    "        celltypes = np.unique(adata.obs[labels_key])\n",
    "\n",
    "        dynamical_df = pd.DataFrame(\n",
    "            index=adata.var_names,\n",
    "            columns=celltypes,\n",
    "            data=np.zeros((adata.shape[1], len(celltypes))),\n",
    "        )\n",
    "        N = 200\n",
    "        for ct in celltypes:\n",
    "            for g in adata.var_names.tolist():\n",
    "                x = root_squared_error_p[g][adata.obs[labels_key] == ct]\n",
    "                y = root_squared_error[g][adata.obs[labels_key] == ct]\n",
    "                ratio = ttest_ind(x[:N], y[:N])[0]\n",
    "                dynamical_df.loc[g, ct] = ratio\n",
    "\n",
    "        return dynamical_df, bdata\n",
    "\n",
    "    def _shuffle_layer_celltype(\n",
    "        self, adata_manager: AnnDataManager, labels_key: str, registry_key: str\n",
    "    ) -> AnnData:\n",
    "        \"\"\"Shuffle cells within cell types for each gene.\"\"\"\n",
    "        from scvi.data._constants import _SCVI_UUID_KEY\n",
    "\n",
    "        bdata = adata_manager.adata.copy()\n",
    "        labels = bdata.obs[labels_key]\n",
    "        del bdata.uns[_SCVI_UUID_KEY]\n",
    "        self._validate_anndata(bdata)\n",
    "        bdata_manager = self.get_anndata_manager(bdata)\n",
    "\n",
    "        # get registry info to later set data back in bdata\n",
    "        # in a way that doesn't require actual knowledge of location\n",
    "        unspliced = bdata_manager.get_from_registry(registry_key)\n",
    "        u_registry = bdata_manager.data_registry[registry_key]\n",
    "        attr_name = u_registry.attr_name\n",
    "        attr_key = u_registry.attr_key\n",
    "\n",
    "        for lab in np.unique(labels):\n",
    "            mask = np.asarray(labels == lab)\n",
    "            unspliced_ct = unspliced[mask].copy()\n",
    "            unspliced_ct = np.apply_along_axis(\n",
    "                np.random.permutation, axis=0, arr=unspliced_ct\n",
    "            )\n",
    "            unspliced[mask] = unspliced_ct\n",
    "        # e.g., if using adata.X\n",
    "        if attr_key is None:\n",
    "            setattr(bdata, attr_name, unspliced)\n",
    "        # e.g., if using a layer\n",
    "        elif attr_key is not None:\n",
    "            attribute = getattr(bdata, attr_name)\n",
    "            attribute[attr_key] = unspliced\n",
    "            setattr(bdata, attr_name, attribute)\n",
    "\n",
    "        return bdata\n",
    "\n",
    "\n",
    "def _compute_directional_statistics_tensor(\n",
    "    tensor: np.ndarray, n_jobs: int, n_cells: int\n",
    ") -> pd.DataFrame:\n",
    "    df = pd.DataFrame(index=np.arange(n_cells))\n",
    "    df[\"directional_variance\"] = np.nan\n",
    "    df[\"directional_difference\"] = np.nan\n",
    "    df[\"directional_cosine_sim_variance\"] = np.nan\n",
    "    df[\"directional_cosine_sim_difference\"] = np.nan\n",
    "    df[\"directional_cosine_sim_mean\"] = np.nan\n",
    "    logger.info(\"Computing the uncertainties...\")\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=3)(\n",
    "        delayed(_directional_statistics_per_cell)(tensor[:, cell_index, :])\n",
    "        for cell_index in range(n_cells)\n",
    "    )\n",
    "    # cells by samples\n",
    "    cosine_sims = np.stack([results[i][0] for i in range(n_cells)])\n",
    "    df.loc[:, \"directional_cosine_sim_variance\"] = [\n",
    "        results[i][1] for i in range(n_cells)\n",
    "    ]\n",
    "    df.loc[:, \"directional_cosine_sim_difference\"] = [\n",
    "        results[i][2] for i in range(n_cells)\n",
    "    ]\n",
    "    df.loc[:, \"directional_variance\"] = [results[i][3] for i in range(n_cells)]\n",
    "    df.loc[:, \"directional_difference\"] = [results[i][4] for i in range(n_cells)]\n",
    "    df.loc[:, \"directional_cosine_sim_mean\"] = [results[i][5] for i in range(n_cells)]\n",
    "\n",
    "    return df, cosine_sims\n",
    "\n",
    "\n",
    "def _directional_statistics_per_cell(\n",
    "    tensor: np.ndarray,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Internal function for parallelization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor\n",
    "        Shape of samples by genes for a given cell.\n",
    "    \"\"\"\n",
    "    n_samples = tensor.shape[0]\n",
    "    # over samples axis\n",
    "    mean_velocity_of_cell = tensor.mean(0)\n",
    "    cosine_sims = [\n",
    "        _cosine_sim(tensor[i, :], mean_velocity_of_cell) for i in range(n_samples)\n",
    "    ]\n",
    "    angle_samples = [np.arccos(el) for el in cosine_sims]\n",
    "    return (\n",
    "        cosine_sims,\n",
    "        np.var(cosine_sims),\n",
    "        np.percentile(cosine_sims, 95) - np.percentile(cosine_sims, 5),\n",
    "        np.var(angle_samples),\n",
    "        np.percentile(angle_samples, 95) - np.percentile(angle_samples, 5),\n",
    "        np.mean(cosine_sims),\n",
    "    )\n",
    "\n",
    "\n",
    "def _centered_unit_vector(vector: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns the centered unit vector of the vector.\"\"\"\n",
    "    vector = vector - np.mean(vector)\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "\n",
    "def _cosine_sim(v1: np.ndarray, v2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns cosine similarity of the vectors.\"\"\"\n",
    "    v1_u = _centered_unit_vector(v1)\n",
    "    v2_u = _centered_unit_vector(v2)\n",
    "    return np.clip(np.dot(v1_u, v2_u), -1.0, 1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = scv.datasets.pancreas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 21611 genes that are detected 30 counts (shared).\n",
      "Normalized count data: X, spliced, unspliced.\n",
      "Extracted 2000 highly variable genes.\n",
      "Logarithmized X.\n",
      "computing neighbors\n",
      "    finished (0:00:19) --> added \n",
      "    'distances' and 'connectivities', weighted adjacency matrices (adata.obsp)\n",
      "computing moments based on connectivities\n",
      "    finished (0:00:01) --> added \n",
      "    'Ms' and 'Mu', moments of un/spliced abundances (adata.layers)\n"
     ]
    }
   ],
   "source": [
    "scv.pp.filter_and_normalize(adata, min_shared_counts=30, n_top_genes=2000)\n",
    "scv.pp.moments(adata, n_pcs=30, n_neighbors=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing velocities\n",
      "    finished (0:00:01) --> added \n",
      "    'velocity', velocity vectors for each individual cell (adata.layers)\n"
     ]
    }
   ],
   "source": [
    "adata = preprocess_data(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\twith soft mask.\n",
      "Last Decoder layer: softmax\n"
     ]
    }
   ],
   "source": [
    "VELOVI.setup_anndata(adata, spliced_layer=\"Ms\", unspliced_layer=\"Mu\")\n",
    "vae = VELOVI(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispersion: tensor([[0.2895, 1.0160, 0.9753,  ..., 0.2503, 0.3756, 1.7516]],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "dec_mean: tensor([[0.0016, 0.0011, 0.0006,  ..., 0.0008, 0.0008, 0.0010],\n",
      "        [0.0004, 0.0004, 0.0009,  ..., 0.0008, 0.0005, 0.0002],\n",
      "        [0.0006, 0.0002, 0.0004,  ..., 0.0006, 0.0015, 0.0008],\n",
      "        ...,\n",
      "        [0.0009, 0.0013, 0.0013,  ..., 0.0008, 0.0005, 0.0006],\n",
      "        [0.0016, 0.0013, 0.0006,  ..., 0.0005, 0.0008, 0.0011],\n",
      "        [0.0018, 0.0003, 0.0004,  ..., 0.0009, 0.0016, 0.0006]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "dispersion: tensor([[0.2895, 1.0160, 0.9753,  ..., 0.2503, 0.3756, 1.7516]],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "dec_mean: tensor([[0.0008, 0.0011, 0.0008,  ..., 0.0014, 0.0016, 0.0052],\n",
      "        [0.0008, 0.0008, 0.0022,  ..., 0.0029, 0.0009, 0.0005],\n",
      "        [0.0015, 0.0010, 0.0005,  ..., 0.0005, 0.0008, 0.0005],\n",
      "        ...,\n",
      "        [0.0014, 0.0009, 0.0007,  ..., 0.0004, 0.0005, 0.0006],\n",
      "        [0.0019, 0.0007, 0.0007,  ..., 0.0004, 0.0008, 0.0005],\n",
      "        [0.0007, 0.0009, 0.0005,  ..., 0.0006, 0.0008, 0.0003]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "dispersion: tensor([[0.2895, 1.0160, 0.9753,  ..., 0.2503, 0.3756, 1.7516]],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "dec_mean: tensor([[0.0004, 0.0005, 0.0004,  ..., 0.0003, 0.0006, 0.0004],\n",
      "        [0.0007, 0.0009, 0.0010,  ..., 0.0009, 0.0007, 0.0017],\n",
      "        [0.0018, 0.0011, 0.0015,  ..., 0.0008, 0.0008, 0.0007],\n",
      "        ...,\n",
      "        [0.0005, 0.0004, 0.0008,  ..., 0.0009, 0.0012, 0.0003],\n",
      "        [0.0009, 0.0003, 0.0002,  ..., 0.0002, 0.0005, 0.0002],\n",
      "        [0.0012, 0.0002, 0.0004,  ..., 0.0005, 0.0010, 0.0011]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "dispersion: tensor([[0.2895, 1.0160, 0.9753,  ..., 0.2503, 0.3756, 1.7516]],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "dec_mean: tensor([[0.0008, 0.0006, 0.0009,  ..., 0.0004, 0.0004, 0.0014],\n",
      "        [0.0010, 0.0021, 0.0004,  ..., 0.0003, 0.0002, 0.0010],\n",
      "        [0.0012, 0.0012, 0.0005,  ..., 0.0004, 0.0005, 0.0002],\n",
      "        ...,\n",
      "        [0.0008, 0.0005, 0.0006,  ..., 0.0008, 0.0018, 0.0007],\n",
      "        [0.0013, 0.0034, 0.0011,  ..., 0.0008, 0.0006, 0.0004],\n",
      "        [0.0012, 0.0007, 0.0010,  ..., 0.0010, 0.0009, 0.0021]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "dispersion: tensor([[0.2895, 1.0160, 0.9753,  ..., 0.2503, 0.3756, 1.7516]],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "dec_mean: tensor([[0.0004, 0.0003, 0.0004,  ..., 0.0002, 0.0004, 0.0021],\n",
      "        [0.0006, 0.0008, 0.0012,  ..., 0.0008, 0.0006, 0.0009],\n",
      "        [0.0007, 0.0006, 0.0003,  ..., 0.0002, 0.0004, 0.0008],\n",
      "        ...,\n",
      "        [0.0004, 0.0005, 0.0003,  ..., 0.0003, 0.0006, 0.0029],\n",
      "        [0.0010, 0.0008, 0.0016,  ..., 0.0032, 0.0014, 0.0006],\n",
      "        [0.0014, 0.0003, 0.0004,  ..., 0.0003, 0.0006, 0.0016]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "dispersion: tensor([[0.2895, 1.0160, 0.9753,  ..., 0.2503, 0.3756, 1.7516]],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "dec_mean: tensor([[0.0004, 0.0005, 0.0008,  ..., 0.0007, 0.0007, 0.0004],\n",
      "        [0.0022, 0.0005, 0.0003,  ..., 0.0004, 0.0011, 0.0013],\n",
      "        [0.0007, 0.0004, 0.0004,  ..., 0.0005, 0.0008, 0.0008],\n",
      "        ...,\n",
      "        [0.0014, 0.0013, 0.0011,  ..., 0.0004, 0.0003, 0.0007],\n",
      "        [0.0006, 0.0017, 0.0012,  ..., 0.0006, 0.0006, 0.0006],\n",
      "        [0.0011, 0.0003, 0.0005,  ..., 0.0004, 0.0009, 0.0017]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "dispersion: tensor([[0.2895, 1.0160, 0.9753,  ..., 0.2503, 0.3756, 1.7516]],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "dec_mean: tensor([[0.0004, 0.0008, 0.0012,  ..., 0.0009, 0.0007, 0.0018],\n",
      "        [0.0010, 0.0015, 0.0009,  ..., 0.0009, 0.0008, 0.0007],\n",
      "        [0.0011, 0.0025, 0.0010,  ..., 0.0004, 0.0004, 0.0003],\n",
      "        ...,\n",
      "        [0.0015, 0.0012, 0.0012,  ..., 0.0006, 0.0006, 0.0016],\n",
      "        [0.0011, 0.0016, 0.0014,  ..., 0.0010, 0.0005, 0.0021],\n",
      "        [0.0010, 0.0006, 0.0007,  ..., 0.0008, 0.0010, 0.0012]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "dispersion: tensor([[0.2895, 1.0160, 0.9753,  ..., 0.2503, 0.3756, 1.7516]],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "dec_mean: tensor([[0.0013, 0.0011, 0.0007,  ..., 0.0017, 0.0019, 0.0006],\n",
      "        [0.0005, 0.0004, 0.0004,  ..., 0.0001, 0.0003, 0.0006],\n",
      "        [0.0006, 0.0007, 0.0004,  ..., 0.0002, 0.0005, 0.0009],\n",
      "        ...,\n",
      "        [0.0003, 0.0015, 0.0008,  ..., 0.0004, 0.0003, 0.0004],\n",
      "        [0.0007, 0.0023, 0.0008,  ..., 0.0010, 0.0008, 0.0005],\n",
      "        [0.0005, 0.0007, 0.0009,  ..., 0.0010, 0.0005, 0.0004]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "dispersion: tensor([[0.2895, 1.0160, 0.9753,  ..., 0.2503, 0.3756, 1.7516]],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "dec_mean: tensor([[0.0005, 0.0008, 0.0005,  ..., 0.0005, 0.0008, 0.0009],\n",
      "        [0.0007, 0.0002, 0.0002,  ..., 0.0005, 0.0010, 0.0012],\n",
      "        [0.0010, 0.0004, 0.0002,  ..., 0.0003, 0.0008, 0.0006],\n",
      "        ...,\n",
      "        [0.0005, 0.0017, 0.0006,  ..., 0.0008, 0.0008, 0.0002],\n",
      "        [0.0012, 0.0014, 0.0011,  ..., 0.0013, 0.0013, 0.0025],\n",
      "        [0.0009, 0.0010, 0.0008,  ..., 0.0011, 0.0009, 0.0016]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "dispersion: tensor([[0.2895, 1.0160, 0.9753,  ..., 0.2503, 0.3756, 1.7516]],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "dec_mean: tensor([[0.0007, 0.0004, 0.0004,  ..., 0.0003, 0.0008, 0.0007],\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0001, 0.0003, 0.0006],\n",
      "        [0.0016, 0.0004, 0.0005,  ..., 0.0008, 0.0014, 0.0014],\n",
      "        ...,\n",
      "        [0.0006, 0.0011, 0.0005,  ..., 0.0002, 0.0003, 0.0007],\n",
      "        [0.0017, 0.0012, 0.0008,  ..., 0.0008, 0.0010, 0.0005],\n",
      "        [0.0012, 0.0019, 0.0021,  ..., 0.0008, 0.0005, 0.0002]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "dispersion: tensor([[0.2895, 1.0160, 0.9753,  ..., 0.2503, 0.3756, 1.7516]],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "dec_mean: tensor([[0.0010, 0.0010, 0.0005,  ..., 0.0002, 0.0004, 0.0004],\n",
      "        [0.0007, 0.0013, 0.0007,  ..., 0.0008, 0.0008, 0.0012],\n",
      "        [0.0007, 0.0012, 0.0014,  ..., 0.0025, 0.0010, 0.0014],\n",
      "        ...,\n",
      "        [0.0014, 0.0005, 0.0006,  ..., 0.0005, 0.0006, 0.0016],\n",
      "        [0.0005, 0.0019, 0.0011,  ..., 0.0009, 0.0005, 0.0005],\n",
      "        [0.0010, 0.0002, 0.0006,  ..., 0.0007, 0.0012, 0.0031]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "dispersion: tensor([[0.2895, 1.0160, 0.9753,  ..., 0.2503, 0.3756, 1.7516]],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "dec_mean: tensor([[0.0013, 0.0004, 0.0007,  ..., 0.0009, 0.0007, 0.0020],\n",
      "        [0.0011, 0.0006, 0.0002,  ..., 0.0003, 0.0006, 0.0012],\n",
      "        [0.0011, 0.0018, 0.0012,  ..., 0.0004, 0.0003, 0.0006],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0003,  ..., 0.0001, 0.0004, 0.0011],\n",
      "        [0.0009, 0.0019, 0.0021,  ..., 0.0011, 0.0004, 0.0011],\n",
      "        [0.0003, 0.0004, 0.0004,  ..., 0.0009, 0.0008, 0.0018]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "dispersion: tensor([[0.2895, 1.0160, 0.9753,  ..., 0.2503, 0.3756, 1.7516]],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "dec_mean: tensor([[0.0007, 0.0006, 0.0003,  ..., 0.0007, 0.0008, 0.0030],\n",
      "        [0.0008, 0.0016, 0.0007,  ..., 0.0007, 0.0009, 0.0004],\n",
      "        [0.0009, 0.0004, 0.0004,  ..., 0.0004, 0.0007, 0.0010],\n",
      "        ...,\n",
      "        [0.0010, 0.0028, 0.0006,  ..., 0.0005, 0.0005, 0.0002],\n",
      "        [0.0009, 0.0006, 0.0006,  ..., 0.0008, 0.0010, 0.0011],\n",
      "        [0.0015, 0.0004, 0.0008,  ..., 0.0010, 0.0014, 0.0010]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "dispersion: tensor([[0.2895, 1.0160, 0.9753,  ..., 0.2503, 0.3756, 1.7516]],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "dec_mean: tensor([[0.0011, 0.0008, 0.0005,  ..., 0.0014, 0.0015, 0.0005],\n",
      "        [0.0005, 0.0012, 0.0007,  ..., 0.0006, 0.0006, 0.0010],\n",
      "        [0.0011, 0.0014, 0.0008,  ..., 0.0006, 0.0007, 0.0007],\n",
      "        ...,\n",
      "        [0.0008, 0.0012, 0.0007,  ..., 0.0008, 0.0009, 0.0008],\n",
      "        [0.0015, 0.0011, 0.0010,  ..., 0.0006, 0.0007, 0.0002],\n",
      "        [0.0005, 0.0005, 0.0012,  ..., 0.0011, 0.0009, 0.0005]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "dispersion: tensor([[0.2895, 1.0160, 0.9753,  ..., 0.2503, 0.3756, 1.7516]],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "dec_mean: tensor([[0.0010, 0.0005, 0.0007,  ..., 0.0005, 0.0008, 0.0006],\n",
      "        [0.0012, 0.0010, 0.0009,  ..., 0.0014, 0.0016, 0.0009],\n",
      "        [0.0006, 0.0017, 0.0025,  ..., 0.0009, 0.0004, 0.0005],\n",
      "        ...,\n",
      "        [0.0011, 0.0007, 0.0006,  ..., 0.0025, 0.0017, 0.0008],\n",
      "        [0.0016, 0.0003, 0.0005,  ..., 0.0003, 0.0004, 0.0016],\n",
      "        [0.0005, 0.0009, 0.0008,  ..., 0.0004, 0.0006, 0.0004]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'z': tensor([[ 1.7182,  1.2581, -0.8243,  ...,  0.4431, -0.6734, -0.7041],\n",
       "          [-0.3307,  2.4791, -1.2982,  ...,  0.1882, -1.0349,  0.2625],\n",
       "          [ 1.2156, -0.5420,  0.0697,  ...,  0.2572,  0.4849,  0.4625],\n",
       "          ...,\n",
       "          [-0.7401,  2.0252, -0.1735,  ..., -0.4803, -1.9329,  0.6052],\n",
       "          [ 0.8853, -0.8612,  0.0749,  ..., -0.0288, -0.5423, -3.1330],\n",
       "          [ 1.1498,  0.3751, -0.5417,  ..., -0.3570,  0.4697,  0.2778]],\n",
       "         grad_fn=<AddBackward0>),\n",
       "  'qz_m': tensor([[ 0.4593, -0.0094, -0.6761,  ...,  0.0724, -0.1528, -0.8626],\n",
       "          [-0.2271,  0.3055, -0.7122,  ..., -0.1018, -0.1714, -0.3415],\n",
       "          [ 0.5889, -0.0584, -0.8595,  ..., -0.1074, -0.3344, -0.5143],\n",
       "          ...,\n",
       "          [-0.0637,  0.4425,  0.1695,  ...,  0.0026, -0.1221, -0.1140],\n",
       "          [ 0.3923,  0.1439, -0.9276,  ..., -0.0126,  0.1596, -0.5593],\n",
       "          [ 0.2684, -0.2689, -0.7343,  ..., -0.1870,  0.1149, -0.4349]],\n",
       "         grad_fn=<AddmmBackward0>),\n",
       "  'qz_v': tensor([[0.4964, 0.5646, 0.6672,  ..., 0.8881, 0.5138, 0.8378],\n",
       "          [0.3537, 0.6336, 0.6219,  ..., 1.0071, 0.8330, 1.0045],\n",
       "          [0.5214, 0.5987, 0.8960,  ..., 0.4689, 0.7996, 0.8432],\n",
       "          ...,\n",
       "          [1.1538, 0.6120, 0.8443,  ..., 0.5872, 0.8252, 0.4743],\n",
       "          [0.3821, 0.5279, 0.6169,  ..., 0.9406, 0.7809, 1.1388],\n",
       "          [0.7883, 0.6386, 1.1857,  ..., 0.4574, 0.7412, 0.9660]],\n",
       "         grad_fn=<AddBackward0>),\n",
       "  'gamma': tensor([0.3133, 0.3133, 0.3133,  ..., 0.3133, 0.3133, 0.3133],\n",
       "         grad_fn=<ClampBackward1>),\n",
       "  'beta': tensor([0.9741, 0.9741, 0.9741,  ..., 0.9741, 0.9741, 0.9741],\n",
       "         grad_fn=<ClampBackward1>),\n",
       "  'alpha': tensor([0.8781, 0.7718, 0.6977,  ..., 0.8797, 0.6982, 0.8357],\n",
       "         grad_fn=<ClampBackward1>),\n",
       "  'alpha_1': Parameter containing:\n",
       "  tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64),\n",
       "  'lambda_alpha': Parameter containing:\n",
       "  tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64)},\n",
       " {'px_pi': tensor([[[2.9955e-01, 4.4577e-01, 2.2344e-01, 3.1237e-02],\n",
       "           [5.6571e-01, 2.0181e-02, 3.8810e-01, 2.6011e-02],\n",
       "           [3.1443e-01, 3.8029e-01, 7.0979e-02, 2.3430e-01],\n",
       "           ...,\n",
       "           [1.6072e-01, 4.4089e-01, 1.3488e-01, 2.6351e-01],\n",
       "           [4.3571e-05, 9.4019e-01, 5.7051e-02, 2.7161e-03],\n",
       "           [2.9065e-02, 6.5815e-02, 4.7675e-01, 4.2837e-01]],\n",
       "  \n",
       "          [[3.0626e-01, 2.0605e-01, 3.5455e-03, 4.8414e-01],\n",
       "           [2.1677e-01, 3.6653e-01, 1.0633e-01, 3.1037e-01],\n",
       "           [1.3419e-01, 2.4857e-01, 3.4759e-01, 2.6965e-01],\n",
       "           ...,\n",
       "           [1.0226e-01, 1.9765e-02, 2.7266e-03, 8.7525e-01],\n",
       "           [3.0348e-01, 5.0358e-01, 6.0681e-02, 1.3225e-01],\n",
       "           [1.4635e-01, 4.7956e-01, 1.8987e-01, 1.8422e-01]],\n",
       "  \n",
       "          [[2.3333e-01, 4.0121e-01, 4.4623e-02, 3.2083e-01],\n",
       "           [3.5149e-01, 2.6050e-02, 1.4844e-01, 4.7401e-01],\n",
       "           [2.1460e-01, 6.2325e-01, 1.5220e-01, 9.9498e-03],\n",
       "           ...,\n",
       "           [7.0062e-03, 1.0785e-01, 6.2459e-01, 2.6055e-01],\n",
       "           [1.7002e-01, 1.4076e-01, 6.4550e-01, 4.3726e-02],\n",
       "           [1.9471e-01, 1.7146e-01, 4.3388e-01, 1.9995e-01]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[7.5377e-01, 1.1925e-01, 3.9441e-02, 8.7533e-02],\n",
       "           [2.7689e-01, 1.0630e-01, 2.5407e-01, 3.6273e-01],\n",
       "           [4.0720e-01, 3.6758e-01, 2.0297e-02, 2.0493e-01],\n",
       "           ...,\n",
       "           [2.5138e-01, 1.6414e-02, 6.2435e-01, 1.0786e-01],\n",
       "           [1.0809e-01, 1.3236e-01, 3.7795e-01, 3.8159e-01],\n",
       "           [2.7494e-01, 3.3808e-02, 1.4976e-02, 6.7627e-01]],\n",
       "  \n",
       "          [[4.4713e-01, 8.4101e-02, 4.3645e-06, 4.6876e-01],\n",
       "           [1.0087e-01, 5.1782e-01, 4.7682e-02, 3.3363e-01],\n",
       "           [3.1850e-01, 3.3426e-02, 5.0541e-01, 1.4266e-01],\n",
       "           ...,\n",
       "           [3.0639e-02, 9.3925e-01, 2.8995e-03, 2.7216e-02],\n",
       "           [9.3911e-02, 4.9014e-01, 1.9116e-01, 2.2479e-01],\n",
       "           [2.6363e-01, 2.4349e-01, 1.5908e-01, 3.3380e-01]],\n",
       "  \n",
       "          [[8.6063e-01, 2.6528e-02, 6.6481e-02, 4.6363e-02],\n",
       "           [5.1466e-01, 1.9293e-01, 6.4406e-02, 2.2801e-01],\n",
       "           [2.3009e-01, 5.9014e-01, 1.0503e-01, 7.4732e-02],\n",
       "           ...,\n",
       "           [9.9420e-02, 4.1904e-02, 8.4028e-01, 1.8397e-02],\n",
       "           [3.9839e-01, 5.7730e-01, 8.7730e-03, 1.5540e-02],\n",
       "           [1.5188e-01, 6.5531e-01, 1.7947e-01, 1.3339e-02]]],\n",
       "         grad_fn=<_DirichletBackward>),\n",
       "  'px_rho': tensor([[0.4021, 0.4363, 0.6925,  ..., 0.3686, 0.5387, 0.6529],\n",
       "          [0.3224, 0.5586, 0.4805,  ..., 0.4309, 0.5050, 0.6901],\n",
       "          [0.4333, 0.7134, 0.4305,  ..., 0.5341, 0.4147, 0.4632],\n",
       "          ...,\n",
       "          [0.3852, 0.5798, 0.3699,  ..., 0.4144, 0.5724, 0.5988],\n",
       "          [0.4264, 0.4871, 0.4738,  ..., 0.4630, 0.6186, 0.4559],\n",
       "          [0.2503, 0.6410, 0.7081,  ..., 0.4670, 0.5322, 0.5612]],\n",
       "         grad_fn=<SigmoidBackward0>),\n",
       "  'px_tau': tensor([[0.5547, 0.4056, 0.2693,  ..., 0.3212, 0.5681, 0.4541],\n",
       "          [0.5586, 0.5180, 0.4139,  ..., 0.4484, 0.5349, 0.3982],\n",
       "          [0.4260, 0.4576, 0.4602,  ..., 0.3625, 0.6243, 0.6656],\n",
       "          ...,\n",
       "          [0.5347, 0.6018, 0.4421,  ..., 0.5087, 0.4532, 0.4994],\n",
       "          [0.6210, 0.4075, 0.4939,  ..., 0.3617, 0.5844, 0.5420],\n",
       "          [0.4232, 0.4648, 0.4951,  ..., 0.4300, 0.6159, 0.5253]],\n",
       "         grad_fn=<SigmoidBackward0>),\n",
       "  'scale': tensor([[0.3133, 0.3133, 0.3133, 0.3133],\n",
       "          [0.3133, 0.3133, 0.3133, 0.3133],\n",
       "          [0.3133, 0.3133, 0.3133, 0.3133],\n",
       "          ...,\n",
       "          [0.3133, 0.3133, 0.3133, 0.3133],\n",
       "          [0.3133, 0.3133, 0.3133, 0.3133],\n",
       "          [0.3133, 0.3133, 0.3133, 0.3133]], grad_fn=<SoftplusBackward0>),\n",
       "  'px_pi_alpha': tensor([[[0.7749, 0.5352, 0.2755, 0.3585],\n",
       "           [0.4023, 0.6537, 0.6509, 0.7313],\n",
       "           [0.7884, 0.6551, 0.6735, 0.8905],\n",
       "           ...,\n",
       "           [0.5317, 0.6880, 0.4966, 0.5238],\n",
       "           [0.3489, 0.5898, 0.7974, 0.8009],\n",
       "           [0.5876, 0.7158, 0.6662, 0.8407]],\n",
       "  \n",
       "          [[0.8417, 0.9261, 0.2932, 0.3322],\n",
       "           [0.7161, 0.7057, 1.1113, 0.7739],\n",
       "           [0.4831, 0.9325, 0.5022, 0.7608],\n",
       "           ...,\n",
       "           [0.5829, 0.4702, 0.4969, 0.8874],\n",
       "           [0.5733, 0.7430, 0.7366, 0.6104],\n",
       "           [0.6221, 0.5805, 0.6606, 1.1273]],\n",
       "  \n",
       "          [[1.0751, 0.6283, 0.2699, 0.6366],\n",
       "           [1.0104, 1.0398, 0.4671, 0.9757],\n",
       "           [0.5991, 0.8247, 0.3968, 0.6504],\n",
       "           ...,\n",
       "           [0.3940, 0.6895, 0.7062, 0.3951],\n",
       "           [0.6847, 0.7739, 0.6828, 0.8235],\n",
       "           [0.7656, 0.5919, 0.9781, 0.8155]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0.9007, 0.7479, 0.5072, 0.4485],\n",
       "           [0.8299, 0.9130, 0.8540, 0.9927],\n",
       "           [0.3824, 0.6313, 0.6254, 0.4446],\n",
       "           ...,\n",
       "           [0.6141, 0.5919, 0.5971, 0.8955],\n",
       "           [0.5060, 0.6027, 1.0624, 0.7892],\n",
       "           [0.6892, 0.6151, 0.7009, 0.8631]],\n",
       "  \n",
       "          [[0.9103, 0.6858, 0.3456, 0.7875],\n",
       "           [0.8042, 0.7684, 0.5684, 0.6702],\n",
       "           [0.9397, 0.7556, 0.5087, 0.7454],\n",
       "           ...,\n",
       "           [0.5415, 1.1239, 0.8215, 0.5407],\n",
       "           [0.4787, 0.3364, 0.8572, 0.9813],\n",
       "           [0.5042, 0.8006, 1.1732, 0.8470]],\n",
       "  \n",
       "          [[1.2254, 0.5152, 0.3761, 0.6574],\n",
       "           [0.4969, 0.7314, 0.8706, 0.8739],\n",
       "           [0.5767, 1.0954, 0.6949, 1.0742],\n",
       "           ...,\n",
       "           [0.5380, 0.5470, 0.6073, 0.3873],\n",
       "           [0.8705, 0.4946, 0.3096, 0.5321],\n",
       "           [0.8752, 0.5336, 0.7586, 0.7139]]], grad_fn=<SoftplusBackward0>),\n",
       "  'mixture_dist_u': MixtureSameFamily(\n",
       "    Categorical(probs: torch.Size([112, 1043, 4]), logits: torch.Size([112, 1043, 4])),\n",
       "    Normal(loc: torch.Size([112, 1043, 4]), scale: torch.Size([112, 1043, 4]))),\n",
       "  'mixture_dist_s': MixtureSameFamily(\n",
       "    Categorical(probs: torch.Size([112, 1043, 4]), logits: torch.Size([112, 1043, 4])),\n",
       "    Normal(loc: torch.Size([112, 1043, 4]), scale: torch.Size([112, 1043, 4]))),\n",
       "  'end_penalty': tensor(2402.4727, grad_fn=<AddBackward0>),\n",
       "  'gene_recon': tensor([[0.0010, 0.0005, 0.0007,  ..., 0.0005, 0.0008, 0.0006],\n",
       "          [0.0012, 0.0010, 0.0009,  ..., 0.0014, 0.0016, 0.0009],\n",
       "          [0.0006, 0.0017, 0.0025,  ..., 0.0009, 0.0004, 0.0005],\n",
       "          ...,\n",
       "          [0.0011, 0.0007, 0.0006,  ..., 0.0025, 0.0017, 0.0008],\n",
       "          [0.0016, 0.0003, 0.0005,  ..., 0.0003, 0.0004, 0.0016],\n",
       "          [0.0005, 0.0009, 0.0008,  ..., 0.0004, 0.0006, 0.0004]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  'dec_latent': tensor([[ 1.6579e-01, -5.4687e-01, -1.0473e-01,  ..., -5.1291e-01,\n",
       "           -6.1779e-02, -2.6016e-01],\n",
       "          [ 4.1512e-01,  1.9130e-01,  5.5012e-02,  ...,  5.1754e-01,\n",
       "            6.4919e-01,  1.0732e-01],\n",
       "          [-2.8634e-01,  7.6959e-01,  1.1650e+00,  ...,  1.7508e-01,\n",
       "           -7.5862e-01, -4.4034e-01],\n",
       "          ...,\n",
       "          [ 4.2532e-01,  6.9191e-04, -2.7026e-01,  ...,  1.2233e+00,\n",
       "            8.4146e-01,  8.6864e-02],\n",
       "          [ 7.1275e-01, -8.9519e-01, -4.9982e-01,  ..., -1.0938e+00,\n",
       "           -5.5866e-01,  7.1963e-01],\n",
       "          [-4.6111e-01,  1.1761e-01, -9.0587e-02,  ..., -8.5508e-01,\n",
       "           -3.4379e-01, -8.4772e-01]], grad_fn=<MmBackward0>)},\n",
       " <scvi.module.base._base_module.LossRecorder at 0x7f7d66358eb0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.get_loss(adata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('thesis': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10d1bd7936c97bab4011aab90821338c12e0c1d0b82a3498f3b9832575200244"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
