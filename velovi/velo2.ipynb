{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet scvi-colab\n",
    "from scvi_colab import install\n",
    "install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scvelo as scv\n",
    "import torch\n",
    "from velovi import preprocess_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scvi.train import LoudEarlyStopping\n",
    "class MyEarlyStopping(LoudEarlyStopping):\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _evaluate_stopping_criteria(self, current):\n",
    "        should_stop, reason  = super()._evaluate_stopping_criteria(current)\n",
    "\n",
    "        if not should_stop:\n",
    "            new_lr = self.optimizer.param_groups[0]['lr']\n",
    "            if self.watch_lr is not None and self.watch_lr != new_lr:\n",
    "                self.watch_lr = new_lr\n",
    "                self.update_prox_ops()\n",
    "\n",
    "        return should_stop, reason\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Main module.\"\"\"\n",
    "from typing import Callable, Iterable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scvi._compat import Literal\n",
    "from scvi.module.base import BaseModuleClass, LossRecorder, auto_move_data\n",
    "from scvi.nn import Encoder, FCLayers\n",
    "from torch import nn as nn\n",
    "from torch.distributions import Categorical, Dirichlet, MixtureSameFamily, Normal\n",
    "from torch.distributions import kl_divergence as kl\n",
    "from scvi.distributions import NegativeBinomial\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "from functools import partial\n",
    "from typing import Iterable, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "from anndata import AnnData\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import ttest_ind\n",
    "from scvi._compat import Literal\n",
    "from scvi._utils import _doc_params\n",
    "from scvi.data import AnnDataManager\n",
    "from scvi.data.fields import LayerField\n",
    "from scvi.dataloaders import AnnDataLoader, DataSplitter\n",
    "from scvi.model._utils import scrna_raw_counts_properties\n",
    "from scvi.model.base import BaseModelClass, UnsupervisedTrainingMixin, VAEMixin\n",
    "from scvi.model.base._utils import _de_core\n",
    "from scvi.train import TrainRunner\n",
    "from scvi.utils._docstrings import doc_differential_expression, setup_anndata_dsp\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from velovi import REGISTRY_KEYS\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def one_hot_encoder(idx, n_cls):\n",
    "    assert torch.max(idx).item() < n_cls\n",
    "    if idx.dim() == 1:\n",
    "        idx = idx.unsqueeze(1)\n",
    "    onehot = torch.zeros(idx.size(0), n_cls)\n",
    "    onehot = onehot.to(idx.device)\n",
    "    onehot.scatter_(1, idx.long(), 1)\n",
    "    return onehot\n",
    "\n",
    "class MaskedLinear(nn.Linear):\n",
    "    def __init__(self, n_in,  n_out, mask, bias=True):\n",
    "        # mask should have the same dimensions as the transposed linear weight\n",
    "        # n_input x n_output_nodes\n",
    "        if n_in != mask.shape[0] or n_out != mask.shape[1]:\n",
    "            raise ValueError('Incorrect shape of the mask.')\n",
    "\n",
    "        super().__init__(n_in, n_out, bias)\n",
    "\n",
    "        self.register_buffer('mask', mask.t())\n",
    "\n",
    "        # zero out the weights for group lasso\n",
    "        # gradient descent won't change these zero weights\n",
    "        self.weight.data*=self.mask\n",
    "\n",
    "    def forward(self, input):\n",
    "        return nn.functional.linear(input, self.weight*self.mask, self.bias)\n",
    "\n",
    "class MaskedCondLayers(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in: int,\n",
    "        n_out: int,\n",
    "        n_cond: int,\n",
    "        bias: bool,\n",
    "        n_ext: int = 0,\n",
    "        n_ext_m: int = 0,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        ext_mask: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_cond = n_cond\n",
    "        self.n_ext = n_ext\n",
    "        self.n_ext_m = n_ext_m\n",
    "\n",
    "        self.expr_L = nn.Linear(n_in, n_out, bias=bias)\n",
    "\n",
    "        # if mask is None:\n",
    "        #     self.expr_L = nn.Linear(n_in, n_out, bias=bias)\n",
    "        # else:\n",
    "        #     self.expr_L = MaskedLinear(n_in, n_out, mask, bias=bias)\n",
    "\n",
    "        # if self.n_cond != 0:\n",
    "        #     self.cond_L = nn.Linear(self.n_cond, n_out, bias=False)\n",
    "\n",
    "        # if self.n_ext != 0:\n",
    "        #     self.ext_L = nn.Linear(self.n_ext, n_out, bias=False)\n",
    "\n",
    "        # if self.n_ext_m != 0:\n",
    "        #     if ext_mask is not None:\n",
    "        #         self.ext_L_m = MaskedLinear(self.n_ext_m, n_out, ext_mask, bias=False)\n",
    "        #     else:\n",
    "        #         self.ext_L_m = nn.Linear(self.n_ext_m, n_out, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # if self.n_cond == 0:\n",
    "        #     expr, cond = x, None\n",
    "        # else:\n",
    "        #     expr, cond = torch.split(x, [x.shape[1] - self.n_cond, self.n_cond], dim=1)\n",
    "\n",
    "        # if self.n_ext == 0:\n",
    "        #     ext = None\n",
    "        # else:\n",
    "        #     expr, ext = torch.split(expr, [expr.shape[1] - self.n_ext, self.n_ext], dim=1)\n",
    "\n",
    "        # if self.n_ext_m == 0:\n",
    "        #     ext_m = None\n",
    "        # else:\n",
    "        #     expr, ext_m = torch.split(expr, [expr.shape[1] - self.n_ext_m, self.n_ext_m], dim=1)\n",
    "\n",
    "        expr=x\n",
    "\n",
    "        out = self.expr_L(expr)\n",
    "        # if ext is not None:\n",
    "        #     out = out + self.ext_L(ext)\n",
    "        # if ext_m is not None:\n",
    "        #     out = out + self.ext_L_m(ext_m)\n",
    "        # if cond is not None:\n",
    "        #     out = out + self.cond_L(cond)\n",
    "        return out\n",
    "\n",
    "\n",
    "# class MaskedLinearDecoder(nn.Module):\n",
    "#     def __init__(self, in_dim, out_dim, n_cond, mask, ext_mask, recon_loss,\n",
    "#                  last_layer=None, n_ext=0, n_ext_m=0):\n",
    "#         super().__init__()\n",
    "\n",
    "#         if recon_loss == \"mse\":\n",
    "#             if last_layer == \"softmax\":\n",
    "#                 raise ValueError(\"Can't specify softmax last layer with mse loss.\")\n",
    "#             last_layer = \"identity\" if last_layer is None else last_layer\n",
    "#         elif recon_loss == \"nb\":\n",
    "#             last_layer = \"softmax\" if last_layer is None else last_layer\n",
    "#         else:\n",
    "#             raise ValueError(\"Unrecognized loss.\")\n",
    "\n",
    "#         print(\"GP Decoder Architecture:\")\n",
    "#         print(\"\\tMasked linear layer in, ext_m, ext, cond, out: \", in_dim, n_ext_m, n_ext, n_cond, out_dim)\n",
    "#         if mask is not None:\n",
    "#             print('\\twith hard mask.')\n",
    "#         else:\n",
    "#             print('\\twith soft mask.')\n",
    "\n",
    "#         self.n_ext = n_ext\n",
    "#         self.n_ext_m = n_ext_m\n",
    "\n",
    "#         self.n_cond = 0\n",
    "#         if n_cond is not None:\n",
    "#             self.n_cond = n_cond\n",
    "\n",
    "#         self.L0 = MaskedCondLayers(in_dim, out_dim, n_cond, bias=False, n_ext=n_ext, n_ext_m=n_ext_m,\n",
    "#                                    mask=mask, ext_mask=ext_mask)\n",
    "\n",
    "#         if last_layer == \"softmax\":\n",
    "#             self.mean_decoder = nn.Softmax(dim=-1)\n",
    "#         elif last_layer == \"softplus\":\n",
    "#             self.mean_decoder = nn.Softplus()\n",
    "#         elif last_layer == \"exp\":\n",
    "#             self.mean_decoder = torch.exp\n",
    "#         elif last_layer == \"relu\":\n",
    "#             self.mean_decoder = nn.ReLU()\n",
    "#         elif last_layer == \"identity\":\n",
    "#             self.mean_decoder = lambda a: a\n",
    "#         else:\n",
    "#             raise ValueError(\"Unrecognized last layer.\")\n",
    "\n",
    "#         print(\"Last Decoder layer:\", last_layer)\n",
    "\n",
    "#     def forward(self, z, batch=None):\n",
    "#         # if batch is not None:\n",
    "#         #     batch = one_hot_encoder(batch, n_cls=self.n_cond)\n",
    "#         #     z_cat = torch.cat((z, batch), dim=-1)\n",
    "#         #     dec_latent = self.L0(z_cat)\n",
    "#         # else:\n",
    "#         #     dec_latent = self.L0(z)\n",
    "\n",
    "#         dec_latent = self.L0(z)\n",
    "#         recon_x = self.mean_decoder(dec_latent)\n",
    "\n",
    "\n",
    "#         return recon_x, dec_latent\n",
    "\n",
    "class DecoderVELOVI(nn.Module):\n",
    "    \"\"\"\n",
    "    Decodes data from latent space of ``n_input`` dimensions ``n_output``dimensions.\n",
    "\n",
    "    Uses a fully-connected neural network of ``n_hidden`` layers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input\n",
    "        The dimensionality of the input (latent space)\n",
    "    n_output\n",
    "        The dimensionality of the output (data space)\n",
    "    n_cat_list\n",
    "        A list containing the number of categories\n",
    "        for each category of interest. Each category will be\n",
    "        included using a one-hot encoding\n",
    "    n_layers\n",
    "        The number of fully-connected hidden layers\n",
    "    n_hidden\n",
    "        The number of nodes per hidden layer\n",
    "    dropout_rate\n",
    "        Dropout rate to apply to each of the hidden layers\n",
    "    inject_covariates\n",
    "        Whether to inject covariates in each layer, or just the first (default).\n",
    "    use_batch_norm\n",
    "        Whether to use batch norm in layers\n",
    "    use_layer_norm\n",
    "        Whether to use layer norm in layers\n",
    "    linear_decoder\n",
    "        Whether to use linear decoder for time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_output: int,\n",
    "        n_ext: int = 0,\n",
    "        n_ext_m: int = 0,\n",
    "        n_cond: int = 0,\n",
    "        last_layer: str =None,\n",
    "        ext_mask: torch.Tensor = None,\n",
    "        mask: torch.Tensor = None,\n",
    "        recon_loss: str = 'nb',\n",
    "        n_cat_list: Iterable[int] = None,\n",
    "        n_layers: int = 1,\n",
    "        n_hidden: int = 128,\n",
    "        inject_covariates: bool = True,\n",
    "        use_batch_norm: bool = True,\n",
    "        use_layer_norm: bool = False,\n",
    "        dropout_rate: float = 0.0,\n",
    "        linear_decoder: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_ouput = n_output\n",
    "        self.linear_decoder = linear_decoder\n",
    "\n",
    "        ### GP decoder ###\n",
    "\n",
    "        if recon_loss == \"mse\":\n",
    "            if last_layer == \"softmax\":\n",
    "                raise ValueError(\"Can't specify softmax last layer with mse loss.\")\n",
    "            last_layer = \"identity\" if last_layer is None else last_layer\n",
    "        elif recon_loss == \"nb\":\n",
    "            last_layer = \"softmax\" if last_layer is None else last_layer\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized loss.\")\n",
    "\n",
    "        #print(\"GP Decoder Architecture:\")\n",
    "        #print(\"\\tMasked linear layer in, ext_m, ext, cond, out: \", in_dim, n_ext_m, n_ext, n_cond, out_dim)\n",
    "        if mask is not None:\n",
    "            print('\\twith hard mask.')\n",
    "        else:\n",
    "            print('\\twith soft mask.')\n",
    "\n",
    "        self.n_ext = n_ext\n",
    "        self.n_ext_m = n_ext_m\n",
    "\n",
    "        self.n_cond = 0\n",
    "        if n_cond is not None:\n",
    "            self.n_cond = n_cond\n",
    "\n",
    "        self.L0 = MaskedCondLayers(n_input, n_output, n_cond, bias=False, n_ext=n_ext, n_ext_m=n_ext_m,\n",
    "                                   mask=mask, ext_mask=ext_mask)\n",
    "\n",
    "        if last_layer == \"softmax\":\n",
    "            self.mean_decoder = nn.Softmax(dim=-1)\n",
    "        elif last_layer == \"softplus\":\n",
    "            self.mean_decoder = nn.Softplus()\n",
    "        elif last_layer == \"exp\":\n",
    "            self.mean_decoder = torch.exp\n",
    "        elif last_layer == \"relu\":\n",
    "            self.mean_decoder = nn.ReLU()\n",
    "        elif last_layer == \"identity\":\n",
    "            self.mean_decoder = lambda a: a\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized last layer.\")\n",
    "\n",
    "        print(\"Last Decoder layer:\", last_layer)\n",
    "\n",
    "        self.rho_first_decoder = FCLayers(\n",
    "            n_in=n_input,\n",
    "            n_out=n_hidden if not linear_decoder else n_output,\n",
    "            n_cat_list=n_cat_list,\n",
    "            n_layers=n_layers if not linear_decoder else 1,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate=dropout_rate,\n",
    "            inject_covariates=inject_covariates,\n",
    "            use_batch_norm=use_batch_norm,\n",
    "            use_layer_norm=use_layer_norm if not linear_decoder else False,\n",
    "            use_activation=not linear_decoder,\n",
    "            bias=not linear_decoder,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.pi_first_decoder = FCLayers(\n",
    "            n_in=n_input,\n",
    "            n_out=n_hidden,\n",
    "            n_cat_list=n_cat_list,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate=dropout_rate,\n",
    "            inject_covariates=inject_covariates,\n",
    "            use_batch_norm=use_batch_norm,\n",
    "            use_layer_norm=use_layer_norm,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.px_pi_decoder = nn.Linear(n_hidden, 4 * n_output)\n",
    "        \n",
    "\n",
    "        # rho for induction\n",
    "        self.px_rho_decoder = nn.Sequential(nn.Linear(n_hidden, n_output), nn.Sigmoid())\n",
    "\n",
    "        # tau for repression\n",
    "        self.px_tau_decoder = nn.Sequential(nn.Linear(n_hidden, n_output), nn.Sigmoid())\n",
    "\n",
    "        self.linear_scaling_tau = nn.Parameter(torch.zeros(n_output))\n",
    "        self.linear_scaling_tau_intercept = nn.Parameter(torch.zeros(n_output))\n",
    "\n",
    "    def forward(self, z: torch.Tensor, latent_dim: int = None):\n",
    "        \"\"\"\n",
    "        The forward computation for a single sample.\n",
    "\n",
    "         #. Decodes the data from the latent space using the decoder network\n",
    "         #. Returns parameters for the ZINB distribution of expression\n",
    "         #. If ``dispersion != 'gene-cell'`` then value for that param will be ``None``\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z :\n",
    "            tensor with shape ``(n_input,)``\n",
    "        cat_list\n",
    "            list of category membership(s) for this sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        4-tuple of :py:class:`torch.Tensor`\n",
    "            parameters for the ZINB distribution of expression\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        z_in = z\n",
    "        if latent_dim is not None:\n",
    "            mask = torch.zeros_like(z)\n",
    "            mask[..., latent_dim] = 1\n",
    "            z_in = z * mask\n",
    "        # The decoder returns values for the parameters of the ZINB distribution\n",
    "        rho_first = self.rho_first_decoder(z_in)\n",
    "\n",
    "        dec_latent = self.L0(z)\n",
    "        recon_x = self.mean_decoder(dec_latent)\n",
    "\n",
    "        if not self.linear_decoder:\n",
    "            px_rho = self.px_rho_decoder(rho_first)\n",
    "            px_tau = self.px_tau_decoder(rho_first)\n",
    "        else:\n",
    "            px_rho = nn.Sigmoid()(rho_first)\n",
    "            px_tau = 1 - nn.Sigmoid()(\n",
    "                rho_first * self.linear_scaling_tau.exp()\n",
    "                + self.linear_scaling_tau_intercept\n",
    "            )\n",
    "\n",
    "        # cells by genes by 4\n",
    "        pi_first = self.pi_first_decoder(z)\n",
    "        px_pi = nn.Softplus()(\n",
    "            torch.reshape(self.px_pi_decoder(pi_first), (z.shape[0], self.n_ouput, 4))\n",
    "        )\n",
    "\n",
    "        return px_pi, px_rho, px_tau, recon_x, dec_latent\n",
    "\n",
    "\n",
    "# VAE model\n",
    "class VELOVAE(BaseModuleClass):\n",
    "    \"\"\"\n",
    "    Variational auto-encoder model.\n",
    "\n",
    "    This is an implementation of the scVI model descibed in [Lopez18]_\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input\n",
    "        Number of input genes\n",
    "    n_hidden\n",
    "        Number of nodes per hidden layer\n",
    "    n_latent\n",
    "        Dimensionality of the latent space\n",
    "    n_layers\n",
    "        Number of hidden layers used for encoder and decoder NNs\n",
    "    dropout_rate\n",
    "        Dropout rate for neural networks\n",
    "    log_variational\n",
    "        Log(data+1) prior to encoding for numerical stability. Not normalization.\n",
    "    latent_distribution\n",
    "        One of\n",
    "\n",
    "        * ``'normal'`` - Isotropic normal\n",
    "        * ``'ln'`` - Logistic normal with normal params N(0, 1)\n",
    "    use_layer_norm\n",
    "        Whether to use layer norm in layers\n",
    "    use_observed_lib_size\n",
    "        Use observed library size for RNA as scaling factor in mean of conditional distribution\n",
    "    var_activation\n",
    "        Callable used to ensure positivity of the variational distributions' variance.\n",
    "        When `None`, defaults to `torch.exp`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        true_time_switch: Optional[np.ndarray] = None,\n",
    "        n_hidden: int = 128,\n",
    "        n_latent: int = 10,\n",
    "        n_layers: int = 1,\n",
    "        dropout_rate: float = 0.1,\n",
    "        log_variational: bool = False,\n",
    "        latent_distribution: str = \"normal\",\n",
    "        use_batch_norm: Literal[\"encoder\", \"decoder\", \"none\", \"both\"] = \"both\",\n",
    "        use_layer_norm: Literal[\"encoder\", \"decoder\", \"none\", \"both\"] = \"both\",\n",
    "        use_observed_lib_size: bool = True,\n",
    "        var_activation: Optional[Callable] = torch.nn.Softplus(),\n",
    "        model_steady_states: bool = True,\n",
    "        gamma_unconstr_init: Optional[np.ndarray] = None,\n",
    "        alpha_unconstr_init: Optional[np.ndarray] = None,\n",
    "        alpha_1_unconstr_init: Optional[np.ndarray] = None,\n",
    "        lambda_alpha_unconstr_init: Optional[np.ndarray] = None,\n",
    "        switch_spliced: Optional[np.ndarray] = None,\n",
    "        switch_unspliced: Optional[np.ndarray] = None,\n",
    "        t_max: float = 20,\n",
    "        penalty_scale: float = 0.2,\n",
    "        dirichlet_concentration: float = 0.25,\n",
    "        linear_decoder: bool = False,\n",
    "        time_dep_transcription_rate: bool = False,\n",
    "        #Parameters for masked linear decoder\n",
    "        mask: torch.Tensor = None,\n",
    "        recon_loss: str = 'nb',\n",
    "        conditions: list = [],\n",
    "        use_l_encoder: bool = False,\n",
    "        dr_rate: float = 0.05,\n",
    "        use_bn: bool = False,\n",
    "        use_ln: bool = True,\n",
    "        decoder_last_layer: Optional[str] = None,\n",
    "        soft_mask: bool = False,\n",
    "        n_ext: int = 0,\n",
    "        n_ext_m: int = 0,\n",
    "        use_hsic: bool = False,\n",
    "        hsic_one_vs_all: bool = False,\n",
    "        ext_mask: Optional[torch.Tensor] = None,\n",
    "        soft_ext_mask: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_latent = n_latent\n",
    "        self.log_variational = log_variational\n",
    "        self.latent_distribution = latent_distribution\n",
    "        self.use_observed_lib_size = use_observed_lib_size\n",
    "        self.n_input = n_input\n",
    "        self.model_steady_states = model_steady_states\n",
    "        self.t_max = t_max\n",
    "        self.penalty_scale = penalty_scale\n",
    "        self.dirichlet_concentration = dirichlet_concentration\n",
    "        self.time_dep_transcription_rate = time_dep_transcription_rate\n",
    "\n",
    "        if switch_spliced is not None:\n",
    "            self.register_buffer(\"switch_spliced\", torch.from_numpy(switch_spliced))\n",
    "        else:\n",
    "            self.switch_spliced = None\n",
    "        if switch_unspliced is not None:\n",
    "            self.register_buffer(\"switch_unspliced\", torch.from_numpy(switch_unspliced))\n",
    "        else:\n",
    "            self.switch_unspliced = None\n",
    "\n",
    "        n_genes = n_input * 2\n",
    "\n",
    "        # switching time\n",
    "        self.switch_time_unconstr = torch.nn.Parameter(7 + 0.5 * torch.randn(n_input))\n",
    "        if true_time_switch is not None:\n",
    "            self.register_buffer(\"true_time_switch\", torch.from_numpy(true_time_switch))\n",
    "        else:\n",
    "            self.true_time_switch = None\n",
    "\n",
    "        # degradation\n",
    "        if gamma_unconstr_init is None:\n",
    "            self.gamma_mean_unconstr = torch.nn.Parameter(-1 * torch.ones(n_input))\n",
    "        else:\n",
    "            self.gamma_mean_unconstr = torch.nn.Parameter(\n",
    "                torch.from_numpy(gamma_unconstr_init)\n",
    "            )\n",
    "\n",
    "        # splicing\n",
    "        # first samples around 1\n",
    "        self.beta_mean_unconstr = torch.nn.Parameter(0.5 * torch.ones(n_input))\n",
    "\n",
    "        # transcription\n",
    "        if alpha_unconstr_init is None:\n",
    "            self.alpha_unconstr = torch.nn.Parameter(0 * torch.ones(n_input))\n",
    "        else:\n",
    "            self.alpha_unconstr = torch.nn.Parameter(\n",
    "                torch.from_numpy(alpha_unconstr_init)\n",
    "            )\n",
    "\n",
    "        # TODO: Add `require_grad`\n",
    "        if alpha_1_unconstr_init is None:\n",
    "            self.alpha_1_unconstr = torch.nn.Parameter(0 * torch.ones(n_input))\n",
    "        else:\n",
    "            self.alpha_1_unconstr = torch.nn.Parameter(\n",
    "                torch.from_numpy(alpha_1_unconstr_init)\n",
    "            )\n",
    "        self.alpha_1_unconstr.requires_grad = time_dep_transcription_rate\n",
    "\n",
    "        if lambda_alpha_unconstr_init is None:\n",
    "            self.lambda_alpha_unconstr = torch.nn.Parameter(0 * torch.ones(n_input))\n",
    "        else:\n",
    "            self.lambda_alpha_unconstr = torch.nn.Parameter(\n",
    "                torch.from_numpy(lambda_alpha_unconstr_init)\n",
    "            )\n",
    "        self.lambda_alpha_unconstr.requires_grad = time_dep_transcription_rate\n",
    "\n",
    "        # likelihood dispersion\n",
    "        # for now, with normal dist, this is just the variance\n",
    "        self.scale_unconstr = torch.nn.Parameter(-1 * torch.ones(n_genes, 4))\n",
    "\n",
    "        use_batch_norm_encoder = use_batch_norm == \"encoder\" or use_batch_norm == \"both\"\n",
    "        use_batch_norm_decoder = use_batch_norm == \"decoder\" or use_batch_norm == \"both\"\n",
    "        use_layer_norm_encoder = use_layer_norm == \"encoder\" or use_layer_norm == \"both\"\n",
    "        use_layer_norm_decoder = use_layer_norm == \"decoder\" or use_layer_norm == \"both\"\n",
    "        self.use_batch_norm_decoder = use_batch_norm_decoder\n",
    "\n",
    "        # z encoder goes from the n_input-dimensional data to an n_latent-d\n",
    "        # latent space representation\n",
    "        n_input_encoder = n_genes\n",
    "        self.z_encoder = Encoder(\n",
    "            n_input_encoder,\n",
    "            n_latent,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate=dropout_rate,\n",
    "            distribution=latent_distribution,\n",
    "            use_batch_norm=use_batch_norm_encoder,\n",
    "            use_layer_norm=use_layer_norm_encoder,\n",
    "            var_activation=var_activation,\n",
    "            activation_fn=torch.nn.ReLU,\n",
    "        )\n",
    "\n",
    "         ### Attributes for masked linear decoder\n",
    "        self.n_conditions = len(conditions)\n",
    "        self.conditions = conditions\n",
    "        self.n_conditions=0\n",
    "        self.recon_loss = recon_loss\n",
    "        self.freeze = False\n",
    "        self.use_bn = use_bn\n",
    "        self.use_ln = use_ln\n",
    "\n",
    "        self.use_mmd = False\n",
    "\n",
    "        self.n_ext_encoder = n_ext + n_ext_m\n",
    "        self.n_ext_decoder = n_ext\n",
    "        self.n_ext_m_decoder = n_ext_m\n",
    "\n",
    "        self.use_hsic = use_hsic and self.n_ext_decoder > 0\n",
    "        self.hsic_one_vs_all = hsic_one_vs_all\n",
    "\n",
    "        self.soft_mask = soft_mask and mask is not None\n",
    "        self.soft_ext_mask = soft_ext_mask and ext_mask is not None\n",
    "\n",
    "        if decoder_last_layer is None:\n",
    "            if recon_loss == 'nb':\n",
    "                self.decoder_last_layer = 'softmax'\n",
    "            else:\n",
    "                self.decoder_last_layer = 'identity'\n",
    "        else:\n",
    "            self.decoder_last_layer = decoder_last_layer\n",
    "\n",
    "        self.use_l_encoder = use_l_encoder\n",
    "\n",
    "        self.dr_rate = dr_rate\n",
    "        if self.dr_rate > 0:\n",
    "            self.use_dr = True\n",
    "        else:\n",
    "            self.use_dr = False\n",
    "\n",
    "        if recon_loss == \"nb\":\n",
    "            if self.n_conditions != 0:\n",
    "                self.theta = torch.nn.Parameter(torch.randn(self.n_input, self.n_conditions))\n",
    "            else:\n",
    "                self.theta = torch.nn.Parameter(torch.randn(1, self.n_input))\n",
    "        else:\n",
    "            self.theta = None\n",
    "\n",
    "        if self.soft_mask:\n",
    "            self.n_inact_genes = (1-mask).sum().item()\n",
    "            soft_shape = mask.shape\n",
    "            if soft_shape[0] != n_latent or soft_shape[1] != n_input:\n",
    "                raise ValueError('Incorrect shape of the soft mask.')\n",
    "            self.mask = mask.t()\n",
    "            mask = None\n",
    "        else:\n",
    "            self.mask = None\n",
    "\n",
    "        if self.soft_ext_mask:\n",
    "            self.n_inact_ext_genes = (1-ext_mask).sum().item()\n",
    "            ext_shape = ext_mask.shape\n",
    "            if ext_shape[0] != self.n_ext_m_decoder:\n",
    "                raise ValueError('Dim 0 of ext_mask should be the same as n_ext_m_decoder.')\n",
    "            if ext_shape[1] != self.n_input:\n",
    "                raise ValueError('Dim 1 of ext_mask should be the same as n_input.')\n",
    "            self.ext_mask = ext_mask.t()\n",
    "            ext_mask = None\n",
    "        else:\n",
    "            self.ext_mask = None\n",
    "            \n",
    "        # decoder goes from n_latent-dimensional space to n_input-d data\n",
    "        n_input_decoder = n_latent\n",
    "        self.decoder = DecoderVELOVI(\n",
    "            n_input_decoder,\n",
    "            n_input,\n",
    "            n_ext = 0,\n",
    "            n_ext_m= 0,\n",
    "            n_cond= 0,\n",
    "            last_layer=None,\n",
    "            ext_mask = None,\n",
    "            mask = None,\n",
    "            recon_loss = 'nb',\n",
    "            n_cat_list= None,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            use_batch_norm=use_batch_norm_decoder,\n",
    "            use_layer_norm=use_layer_norm_decoder,\n",
    "            activation_fn=torch.nn.ReLU,\n",
    "            linear_decoder=linear_decoder,\n",
    "            )\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "    def _get_inference_input(self, tensors):\n",
    "        spliced = tensors[REGISTRY_KEYS.X_KEY]\n",
    "        unspliced = tensors[REGISTRY_KEYS.U_KEY]\n",
    "\n",
    "        input_dict = dict(\n",
    "            spliced=spliced,\n",
    "            unspliced=unspliced,\n",
    "        )\n",
    "        return input_dict\n",
    "\n",
    "    def _get_generative_input(self, tensors, inference_outputs):\n",
    "        z = inference_outputs[\"z\"]\n",
    "        gamma = inference_outputs[\"gamma\"]\n",
    "        beta = inference_outputs[\"beta\"]\n",
    "        alpha = inference_outputs[\"alpha\"]\n",
    "        alpha_1 = inference_outputs[\"alpha_1\"]\n",
    "        lambda_alpha = inference_outputs[\"lambda_alpha\"]\n",
    "\n",
    "        input_dict = {\n",
    "            \"z\": z,\n",
    "            \"gamma\": gamma,\n",
    "            \"beta\": beta,\n",
    "            \"alpha\": alpha,\n",
    "            \"alpha_1\": alpha_1,\n",
    "            \"lambda_alpha\": lambda_alpha,\n",
    "        }\n",
    "        return input_dict\n",
    "\n",
    "    @auto_move_data\n",
    "    def inference(\n",
    "        self,\n",
    "        spliced,\n",
    "        unspliced,\n",
    "        n_samples=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        High level inference method.\n",
    "\n",
    "        Runs the inference (encoder) model.\n",
    "        \"\"\"\n",
    "        spliced_ = spliced\n",
    "        unspliced_ = unspliced\n",
    "        if self.log_variational:\n",
    "            spliced_ = torch.log(0.01 + spliced)\n",
    "            unspliced_ = torch.log(0.01 + unspliced)\n",
    "\n",
    "        encoder_input = torch.cat((spliced_, unspliced_), dim=-1)\n",
    "\n",
    "        qz_m, qz_v, z = self.z_encoder(encoder_input)\n",
    "\n",
    "        if n_samples > 1:\n",
    "            qz_m = qz_m.unsqueeze(0).expand((n_samples, qz_m.size(0), qz_m.size(1)))\n",
    "            qz_v = qz_v.unsqueeze(0).expand((n_samples, qz_v.size(0), qz_v.size(1)))\n",
    "            # when z is normal, untran_z == z\n",
    "            untran_z = Normal(qz_m, qz_v.sqrt()).sample()\n",
    "            z = self.z_encoder.z_transformation(untran_z)\n",
    "\n",
    "        gamma, beta, alpha, alpha_1, lambda_alpha = self._get_rates()\n",
    "\n",
    "        outputs = dict(\n",
    "            z=z,\n",
    "            qz_m=qz_m,\n",
    "            qz_v=qz_v,\n",
    "            gamma=gamma,\n",
    "            beta=beta,\n",
    "            alpha=alpha,\n",
    "            alpha_1=alpha_1,\n",
    "            lambda_alpha=lambda_alpha,\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    def _get_rates(self):\n",
    "        # globals\n",
    "        # degradation\n",
    "        gamma = torch.clamp(F.softplus(self.gamma_mean_unconstr), 0, 50)\n",
    "        # splicing\n",
    "        beta = torch.clamp(F.softplus(self.beta_mean_unconstr), 0, 50)\n",
    "        # transcription\n",
    "        alpha = torch.clamp(F.softplus(self.alpha_unconstr), 0, 50)\n",
    "        if self.time_dep_transcription_rate:\n",
    "            alpha_1 = torch.clamp(F.softplus(self.alpha_1_unconstr), 0, 50)\n",
    "            lambda_alpha = torch.clamp(F.softplus(self.lambda_alpha_unconstr), 0, 50)\n",
    "        else:\n",
    "            alpha_1 = self.alpha_1_unconstr\n",
    "            lambda_alpha = self.lambda_alpha_unconstr\n",
    "\n",
    "        return gamma, beta, alpha, alpha_1, lambda_alpha\n",
    "\n",
    "    @auto_move_data\n",
    "    def generative(self, z, gamma, beta, alpha, alpha_1, lambda_alpha, latent_dim=None):\n",
    "        \"\"\"Runs the generative model.\"\"\"\n",
    "        decoder_input = z\n",
    "        px_pi_alpha, px_rho, px_tau, dec_mean, dec_latent = self.decoder(decoder_input, latent_dim=latent_dim)\n",
    "\n",
    "        px_pi = Dirichlet(px_pi_alpha).rsample()\n",
    "\n",
    "        #dec_mean, dec_latent = self.GP_linear_decoder(decoder_input, batch=None)\n",
    "\n",
    "        scale_unconstr = self.scale_unconstr\n",
    "        scale = F.softplus(scale_unconstr)\n",
    "\n",
    "        mixture_dist_s, mixture_dist_u, end_penalty = self.get_px(\n",
    "            px_pi,\n",
    "            px_rho,\n",
    "            px_tau,\n",
    "            scale,\n",
    "            gamma,\n",
    "            beta,\n",
    "            alpha,\n",
    "            alpha_1,\n",
    "            lambda_alpha,\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            px_pi=px_pi,\n",
    "            px_rho=px_rho,\n",
    "            px_tau=px_tau,\n",
    "            scale=scale,\n",
    "            px_pi_alpha=px_pi_alpha,\n",
    "            mixture_dist_u=mixture_dist_u,\n",
    "            mixture_dist_s=mixture_dist_s,\n",
    "            end_penalty=end_penalty,\n",
    "            gene_recon = dec_mean,\n",
    "            dec_latent = dec_latent\n",
    "        )\n",
    "\n",
    "    def loss(\n",
    "        self,\n",
    "        tensors,\n",
    "        inference_outputs,\n",
    "        generative_outputs,\n",
    "        cond_batch=None,\n",
    "        kl_weight: float = 1.0,\n",
    "        n_obs: float = 1.0,\n",
    "    ):\n",
    "        spliced = tensors[REGISTRY_KEYS.X_KEY]\n",
    "        unspliced = tensors[REGISTRY_KEYS.U_KEY]\n",
    "\n",
    "        #gene reconstruction loss\n",
    "        ground_truth_counts = spliced + unspliced\n",
    "        \n",
    "\n",
    "        if cond_batch is not None:\n",
    "            dispersion = F.linear(one_hot_encoder(cond_batch, self.n_conditions), self.theta) #batch is the\n",
    "        else:\n",
    "            dispersion = self.theta   \n",
    "        dispersion = torch.exp(dispersion)\n",
    "\n",
    "        dec_mean = generative_outputs[\"gene_recon\"]\n",
    "        negbin = NegativeBinomial(mu=dec_mean, theta=dispersion)\n",
    "        \n",
    "        gene_recon_loss = -negbin.log_prob(ground_truth_counts).sum(dim=-1)\n",
    "        \n",
    "\n",
    "        qz_m = inference_outputs[\"qz_m\"]\n",
    "        qz_v = inference_outputs[\"qz_v\"]\n",
    "\n",
    "        px_pi = generative_outputs[\"px_pi\"]\n",
    "        px_pi_alpha = generative_outputs[\"px_pi_alpha\"]\n",
    "\n",
    "        end_penalty = generative_outputs[\"end_penalty\"]\n",
    "        mixture_dist_s = generative_outputs[\"mixture_dist_s\"]\n",
    "        mixture_dist_u = generative_outputs[\"mixture_dist_u\"]\n",
    "\n",
    "        kl_divergence_z = kl(Normal(qz_m, torch.sqrt(qz_v)), Normal(0, 1)).sum(dim=1)\n",
    "\n",
    "        reconst_loss_s = -mixture_dist_s.log_prob(spliced)\n",
    "        reconst_loss_u = -mixture_dist_u.log_prob(unspliced)\n",
    "        reconst_loss = reconst_loss_u.sum(dim=-1) + reconst_loss_s.sum(dim=-1) \n",
    "\n",
    "        kl_pi = kl(\n",
    "            Dirichlet(px_pi_alpha),\n",
    "            Dirichlet(self.dirichlet_concentration * torch.ones_like(px_pi)),\n",
    "        ).sum(dim=-1)\n",
    "\n",
    "        # local loss\n",
    "        kl_local = kl_divergence_z + kl_pi\n",
    "        weighted_kl_local = kl_weight * (kl_divergence_z) + kl_pi\n",
    "\n",
    "        local_loss = torch.mean(reconst_loss + gene_recon_loss + weighted_kl_local)\n",
    "\n",
    "        # combine local and global\n",
    "        global_loss = 0\n",
    "        loss = (\n",
    "            local_loss\n",
    "            + self.penalty_scale * (1 - kl_weight) * end_penalty\n",
    "            + (1 / n_obs) * kl_weight * (global_loss)\n",
    "        )\n",
    "\n",
    "        loss_recorder = LossRecorder(\n",
    "            loss, reconst_loss, kl_local, torch.tensor(global_loss)\n",
    "        )\n",
    "\n",
    "        return loss_recorder\n",
    "\n",
    "\n",
    "    @auto_move_data\n",
    "    def get_px(\n",
    "        self,\n",
    "        px_pi,\n",
    "        px_rho,\n",
    "        px_tau,\n",
    "        scale,\n",
    "        gamma,\n",
    "        beta,\n",
    "        alpha,\n",
    "        alpha_1,\n",
    "        lambda_alpha,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        t_s = torch.clamp(F.softplus(self.switch_time_unconstr), 0, self.t_max)\n",
    "\n",
    "        n_cells = px_pi.shape[0]\n",
    "\n",
    "        # component dist\n",
    "        comp_dist = Categorical(probs=px_pi)\n",
    "\n",
    "        # induction\n",
    "        mean_u_ind, mean_s_ind = self._get_induction_unspliced_spliced(\n",
    "            alpha, alpha_1, lambda_alpha, beta, gamma, t_s * px_rho\n",
    "        )\n",
    "\n",
    "        if self.time_dep_transcription_rate:\n",
    "            mean_u_ind_steady = (alpha_1 / beta).expand(n_cells, self.n_input)\n",
    "            mean_s_ind_steady = (alpha_1 / gamma).expand(n_cells, self.n_input)\n",
    "        else:\n",
    "            mean_u_ind_steady = (alpha / beta).expand(n_cells, self.n_input)\n",
    "            mean_s_ind_steady = (alpha / gamma).expand(n_cells, self.n_input)\n",
    "        scale_u = scale[: self.n_input, :].expand(n_cells, self.n_input, 4).sqrt()\n",
    "\n",
    "        # repression\n",
    "        u_0, s_0 = self._get_induction_unspliced_spliced(\n",
    "            alpha, alpha_1, lambda_alpha, beta, gamma, t_s\n",
    "        )\n",
    "\n",
    "        tau = px_tau\n",
    "        mean_u_rep, mean_s_rep = self._get_repression_unspliced_spliced(\n",
    "            u_0,\n",
    "            s_0,\n",
    "            beta,\n",
    "            gamma,\n",
    "            (self.t_max - t_s) * tau,\n",
    "        )\n",
    "        mean_u_rep_steady = torch.zeros_like(mean_u_ind)\n",
    "        mean_s_rep_steady = torch.zeros_like(mean_u_ind)\n",
    "        scale_s = scale[self.n_input :, :].expand(n_cells, self.n_input, 4).sqrt()\n",
    "\n",
    "        end_penalty = ((u_0 - self.switch_unspliced).pow(2)).sum() + (\n",
    "            (s_0 - self.switch_spliced).pow(2)\n",
    "        ).sum()\n",
    "\n",
    "        # unspliced\n",
    "        mean_u = torch.stack(\n",
    "            (\n",
    "                mean_u_ind,\n",
    "                mean_u_ind_steady,\n",
    "                mean_u_rep,\n",
    "                mean_u_rep_steady,\n",
    "            ),\n",
    "            dim=2,\n",
    "        )\n",
    "        scale_u = torch.stack(\n",
    "            (\n",
    "                scale_u[..., 0],\n",
    "                scale_u[..., 0],\n",
    "                scale_u[..., 0],\n",
    "                0.1 * scale_u[..., 0],\n",
    "            ),\n",
    "            dim=2,\n",
    "        )\n",
    "        dist_u = Normal(mean_u, scale_u)\n",
    "        mixture_dist_u = MixtureSameFamily(comp_dist, dist_u)\n",
    "\n",
    "        # spliced\n",
    "        mean_s = torch.stack(\n",
    "            (mean_s_ind, mean_s_ind_steady, mean_s_rep, mean_s_rep_steady),\n",
    "            dim=2,\n",
    "        )\n",
    "        scale_s = torch.stack(\n",
    "            (\n",
    "                scale_s[..., 0],\n",
    "                scale_s[..., 0],\n",
    "                scale_s[..., 0],\n",
    "                0.1 * scale_s[..., 0],\n",
    "            ),\n",
    "            dim=2,\n",
    "        )\n",
    "        dist_s = Normal(mean_s, scale_s)\n",
    "        mixture_dist_s = MixtureSameFamily(comp_dist, dist_s)\n",
    "\n",
    "        return mixture_dist_s, mixture_dist_u, end_penalty\n",
    "\n",
    "    def _get_induction_unspliced_spliced(\n",
    "        self, alpha, alpha_1, lambda_alpha, beta, gamma, t, eps=1e-6\n",
    "    ):\n",
    "        if self.time_dep_transcription_rate:\n",
    "            unspliced = alpha_1 / beta * (1 - torch.exp(-beta * t)) - (\n",
    "                alpha_1 - alpha\n",
    "            ) / (beta - lambda_alpha) * (\n",
    "                torch.exp(-lambda_alpha * t) - torch.exp(-beta * t)\n",
    "            )\n",
    "\n",
    "            spliced = (\n",
    "                alpha_1 / gamma * (1 - torch.exp(-gamma * t))\n",
    "                + alpha_1\n",
    "                / (gamma - beta + eps)\n",
    "                * (torch.exp(-gamma * t) - torch.exp(-beta * t))\n",
    "                - beta\n",
    "                * (alpha_1 - alpha)\n",
    "                / (beta - lambda_alpha + eps)\n",
    "                / (gamma - lambda_alpha + eps)\n",
    "                * (torch.exp(-lambda_alpha * t) - torch.exp(-gamma * t))\n",
    "                + beta\n",
    "                * (alpha_1 - alpha)\n",
    "                / (beta - lambda_alpha + eps)\n",
    "                / (gamma - beta + eps)\n",
    "                * (torch.exp(-beta * t) - torch.exp(-gamma * t))\n",
    "            )\n",
    "        else:\n",
    "            unspliced = (alpha / beta) * (1 - torch.exp(-beta * t))\n",
    "            spliced = (alpha / gamma) * (1 - torch.exp(-gamma * t)) + (\n",
    "                alpha / ((gamma - beta) + eps)\n",
    "            ) * (torch.exp(-gamma * t) - torch.exp(-beta * t))\n",
    "\n",
    "        return unspliced, spliced\n",
    "\n",
    "    def _get_repression_unspliced_spliced(self, u_0, s_0, beta, gamma, t, eps=1e-6):\n",
    "        unspliced = torch.exp(-beta * t) * u_0\n",
    "        spliced = s_0 * torch.exp(-gamma * t) - (\n",
    "            beta * u_0 / ((gamma - beta) + eps)\n",
    "        ) * (torch.exp(-gamma * t) - torch.exp(-beta * t))\n",
    "        return unspliced, spliced\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Not implemented.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_loadings(self) -> np.ndarray:\n",
    "        \"\"\"Extract per-gene weights (for each Z, shape is genes by dim(Z)) in the linear decoder.\"\"\"\n",
    "        # This is BW, where B is diag(b) batch norm, W is weight matrix\n",
    "        if self.decoder.linear_decoder is False:\n",
    "            raise ValueError(\"Model not trained with linear decoder\")\n",
    "        w = self.decoder.rho_first_decoder.fc_layers[0][0].weight\n",
    "        if self.use_batch_norm_decoder:\n",
    "            bn = self.decoder.rho_first_decoder.fc_layers[0][1]\n",
    "            sigma = torch.sqrt(bn.running_var + bn.eps)\n",
    "            gamma = bn.weight\n",
    "            b = gamma / sigma\n",
    "            b_identity = torch.diag(b)\n",
    "            loadings = torch.matmul(b_identity, w)\n",
    "        else:\n",
    "            loadings = w\n",
    "        loadings = loadings.detach().cpu().numpy()\n",
    "\n",
    "        return loadings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _softplus_inverse(x: np.ndarray) -> np.ndarray:\n",
    "    x = torch.from_numpy(x)\n",
    "    x_inv = torch.where(x > 20, x, x.expm1().log()).numpy()\n",
    "    return x_inv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VELOVI(VAEMixin, UnsupervisedTrainingMixin, BaseModelClass):\n",
    "    \"\"\"\n",
    "    Velocity Variational Inference\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData object that has been registered via :func:`~velovi.VELOVI.setup_anndata`.\n",
    "    n_hidden\n",
    "        Number of nodes per hidden layer.\n",
    "    n_latent\n",
    "        Dimensionality of the latent space.\n",
    "    n_layers\n",
    "        Number of hidden layers used for encoder and decoder NNs.\n",
    "    dropout_rate\n",
    "        Dropout rate for neural networks.\n",
    "    gamma_init_data\n",
    "        Initialize gamma using the data-driven technique.\n",
    "    linear_decoder\n",
    "        Use a linear decoder from latent space to time.\n",
    "    **model_kwargs\n",
    "        Keyword args for :class:`~velovi.VELOVAE`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata: AnnData,\n",
    "        n_hidden: int = 256,\n",
    "        n_latent: int = 10,\n",
    "        n_layers: int = 1,\n",
    "        dropout_rate: float = 0.1,\n",
    "        gamma_init_data: bool = False,\n",
    "        linear_decoder: bool = False,\n",
    "        mask: Optional[Union[np.ndarray, list]] = None,\n",
    "        mask_key: str = 'I',\n",
    "        soft_mask: bool = False,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "        super().__init__(adata)\n",
    "        self.n_latent = n_latent\n",
    "\n",
    "        if mask is None and mask_key not in self.adata.varm:\n",
    "            raise ValueError('Please provide mask.')\n",
    "        \n",
    "        if mask is None:\n",
    "            mask = adata.varm[mask_key].T\n",
    "\n",
    "        self.mask_ = mask if isinstance(mask, list) else mask.tolist()\n",
    "        mask = torch.tensor(mask).float()\n",
    "\n",
    "        self.soft_mask_ = soft_mask\n",
    "\n",
    "        spliced = self.adata_manager.get_from_registry(REGISTRY_KEYS.X_KEY)\n",
    "        unspliced = self.adata_manager.get_from_registry(REGISTRY_KEYS.U_KEY)\n",
    "\n",
    "        sorted_unspliced = np.argsort(unspliced, axis=0)\n",
    "        ind = int(adata.n_obs * 0.99)\n",
    "        us_upper_ind = sorted_unspliced[ind:, :]\n",
    "\n",
    "        us_upper = []\n",
    "        ms_upper = []\n",
    "        for i in range(len(us_upper_ind)):\n",
    "            row = us_upper_ind[i]\n",
    "            us_upper += [unspliced[row, np.arange(adata.n_vars)][np.newaxis, :]]\n",
    "            ms_upper += [spliced[row, np.arange(adata.n_vars)][np.newaxis, :]]\n",
    "        us_upper = np.median(np.concatenate(us_upper, axis=0), axis=0)\n",
    "        ms_upper = np.median(np.concatenate(ms_upper, axis=0), axis=0)\n",
    "\n",
    "        alpha_unconstr = _softplus_inverse(us_upper)\n",
    "        alpha_unconstr = np.asarray(alpha_unconstr).ravel()\n",
    "\n",
    "        alpha_1_unconstr = np.zeros(us_upper.shape).ravel()\n",
    "        lambda_alpha_unconstr = np.zeros(us_upper.shape).ravel()\n",
    "\n",
    "        if gamma_init_data:\n",
    "            gamma_unconstr = np.clip(_softplus_inverse(us_upper / ms_upper), None, 10)\n",
    "        else:\n",
    "            gamma_unconstr = None\n",
    "\n",
    "        self.module = VELOVAE(\n",
    "            n_input=self.summary_stats[\"n_vars\"],\n",
    "            n_hidden=n_hidden,\n",
    "            n_latent=n_latent,\n",
    "            n_layers=n_layers,\n",
    "            dropout_rate=dropout_rate,\n",
    "            gamma_unconstr_init=gamma_unconstr,\n",
    "            alpha_unconstr_init=alpha_unconstr,\n",
    "            alpha_1_unconstr_init=alpha_1_unconstr,\n",
    "            lambda_alpha_unconstr_init=lambda_alpha_unconstr,\n",
    "            switch_spliced=ms_upper,\n",
    "            switch_unspliced=us_upper,\n",
    "            linear_decoder=linear_decoder,\n",
    "            mask=mask,\n",
    "            soft_mask=self.soft_mask_,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        self._model_summary_string = (\n",
    "            \"VELOVI Model with the following params: \\nn_hidden: {}, n_latent: {}, n_layers: {}, dropout_rate: \"\n",
    "            \"{}\"\n",
    "        ).format(\n",
    "            n_hidden,\n",
    "            n_latent,\n",
    "            n_layers,\n",
    "            dropout_rate,\n",
    "        )\n",
    "        self.init_params_ = self._get_init_params(locals())\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        max_epochs: Optional[int] = 500,\n",
    "        lr: float = 1e-2,\n",
    "        weight_decay: float = 1e-2,\n",
    "        use_gpu: Optional[Union[str, int, bool]] = None,\n",
    "        train_size: float = 0.9,\n",
    "        validation_size: Optional[float] = None,\n",
    "        batch_size: int = 256,\n",
    "        early_stopping: bool = True,\n",
    "        gradient_clip_val: float = 10,\n",
    "        alpha=0.7,\n",
    "        plan_kwargs: Optional[dict] = None,\n",
    "        **trainer_kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_epochs\n",
    "            Number of passes through the dataset. If `None`, defaults to\n",
    "            `np.min([round((20000 / n_cells) * 400), 400])`\n",
    "        lr\n",
    "            Learning rate for optimization\n",
    "        weight_decay\n",
    "            Weight decay for optimization\n",
    "        use_gpu\n",
    "            Use default GPU if available (if None or True), or index of GPU to use (if int),\n",
    "            or name of GPU (if str, e.g., `'cuda:0'`), or use CPU (if False).\n",
    "        train_size\n",
    "            Size of training set in the range [0.0, 1.0].\n",
    "        validation_size\n",
    "            Size of the test set. If `None`, defaults to 1 - `train_size`. If\n",
    "            `train_size + validation_size < 1`, the remaining cells belong to a test set.\n",
    "        batch_size\n",
    "            Minibatch size to use during training.\n",
    "        early_stopping\n",
    "            Perform early stopping. Additional arguments can be passed in `**kwargs`.\n",
    "            See :class:`~scvi.train.Trainer` for further options.\n",
    "        gradient_clip_val\n",
    "            Val for gradient clipping\n",
    "        plan_kwargs\n",
    "            Keyword args for :class:`~scvi.train.TrainingPlan`. Keyword arguments passed to\n",
    "            `train()` will overwrite values present in `plan_kwargs`, when appropriate.\n",
    "        **trainer_kwargs\n",
    "            Other keyword args for :class:`~scvi.train.Trainer`.\n",
    "        \"\"\"\n",
    "        user_plan_kwargs = (\n",
    "            plan_kwargs.copy() if isinstance(plan_kwargs, dict) else dict()\n",
    "        )\n",
    "        plan_kwargs = dict(lr=lr, weight_decay=weight_decay, optimizer=\"AdamW\")\n",
    "        plan_kwargs.update(user_plan_kwargs)\n",
    "\n",
    "        user_train_kwargs = trainer_kwargs.copy()\n",
    "        trainer_kwargs = dict(gradient_clip_val=gradient_clip_val)\n",
    "        trainer_kwargs.update(user_train_kwargs)\n",
    "\n",
    "        data_splitter = DataSplitter(\n",
    "            self.adata_manager,\n",
    "            train_size=train_size,\n",
    "            validation_size=validation_size,\n",
    "            batch_size=batch_size,\n",
    "            use_gpu=use_gpu,\n",
    "        )\n",
    "        training_plan = CustomTrainingPlan(self.module, alpha=alpha, **plan_kwargs)\n",
    "\n",
    "        es = \"early_stopping\"\n",
    "        trainer_kwargs[es] = (\n",
    "            early_stopping if es not in trainer_kwargs.keys() else trainer_kwargs[es]\n",
    "        )\n",
    "\n",
    "        #trainer_kwargs[\"callbacks\"] = MyEarlyStopping()\n",
    "\n",
    "        runner = TrainRunner(\n",
    "            self,\n",
    "            training_plan=training_plan,\n",
    "            data_splitter=data_splitter,\n",
    "            max_epochs=max_epochs,\n",
    "            use_gpu=use_gpu,\n",
    "            **trainer_kwargs,\n",
    "        )\n",
    "        return runner()\n",
    "\n",
    "    \n",
    "    #optim = torch.optim.Adam(linear_reg_model.parameters(), lr=0.05)\n",
    "\n",
    "    def get_loss(self, adata):\n",
    "        # run the model forward on the data\n",
    "\n",
    "        adata = self._validate_anndata(adata)\n",
    "        scdl = self._make_data_loader(\n",
    "            adata=adata, indices=None, batch_size=256\n",
    "        )\n",
    "\n",
    "        for tensors in scdl:    \n",
    "            inference_outputs, generative_outputs, loss = self.module.forward(\n",
    "                tensors=tensors,\n",
    "                compute_loss=True,\n",
    "                )\n",
    "        \n",
    "        return inference_outputs, generative_outputs, loss\n",
    "        # # calculate the mse loss\n",
    "        \n",
    "        # # initialize gradients to zero\n",
    "        # optim.zero_grad()\n",
    "        # # backpropagate\n",
    "        # loss.backward()\n",
    "        # # take a gradient step\n",
    "        # optim.step()\n",
    "        # return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_state_assignment(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        indices: Optional[Sequence[int]] = None,\n",
    "        gene_list: Optional[Sequence[str]] = None,\n",
    "        hard_assignment: bool = False,\n",
    "        n_samples: int = 20,\n",
    "        batch_size: Optional[int] = None,\n",
    "        return_mean: bool = True,\n",
    "        return_numpy: Optional[bool] = None,\n",
    "    ) -> Tuple[Union[np.ndarray, pd.DataFrame], List[str]]:\n",
    "        \"\"\"\n",
    "        Returns cells by genes by states probabilities.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        adata\n",
    "            AnnData object with equivalent structure to initial AnnData. If `None`, defaults to the\n",
    "            AnnData object used to initialize the model.\n",
    "        indices\n",
    "            Indices of cells in adata to use. If `None`, all cells are used.\n",
    "        gene_list\n",
    "            Return frequencies of expression for a subset of genes.\n",
    "            This can save memory when working with large datasets and few genes are\n",
    "            of interest.\n",
    "        hard_assignment\n",
    "            Return a hard state assignment\n",
    "        n_samples\n",
    "            Number of posterior samples to use for estimation.\n",
    "        batch_size\n",
    "            Minibatch size for data loading into model. Defaults to `scvi.settings.batch_size`.\n",
    "        return_mean\n",
    "            Whether to return the mean of the samples.\n",
    "        return_numpy\n",
    "            Return a :class:`~numpy.ndarray` instead of a :class:`~pandas.DataFrame`. DataFrame includes\n",
    "            gene names as columns. If either `n_samples=1` or `return_mean=True`, defaults to `False`.\n",
    "            Otherwise, it defaults to `True`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        If `n_samples` > 1 and `return_mean` is False, then the shape is `(samples, cells, genes)`.\n",
    "        Otherwise, shape is `(cells, genes)`. In this case, return type is :class:`~pandas.DataFrame` unless `return_numpy` is True.\n",
    "        \"\"\"\n",
    "        adata = self._validate_anndata(adata)\n",
    "        scdl = self._make_data_loader(\n",
    "            adata=adata, indices=indices, batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        if gene_list is None:\n",
    "            gene_mask = slice(None)\n",
    "        else:\n",
    "            all_genes = adata.var_names\n",
    "            gene_mask = [True if gene in gene_list else False for gene in all_genes]\n",
    "\n",
    "        if n_samples > 1 and return_mean is False:\n",
    "            if return_numpy is False:\n",
    "                warnings.warn(\n",
    "                    \"return_numpy must be True if n_samples > 1 and return_mean is False, returning np.ndarray\"\n",
    "                )\n",
    "            return_numpy = True\n",
    "        if indices is None:\n",
    "            indices = np.arange(adata.n_obs)\n",
    "\n",
    "        states = []\n",
    "        for tensors in scdl:\n",
    "            minibatch_samples = []\n",
    "            for _ in range(n_samples):\n",
    "                _, generative_outputs = self.module.forward(\n",
    "                    tensors=tensors,\n",
    "                    compute_loss=True,\n",
    "                )\n",
    "                output = generative_outputs[\"px_pi\"]\n",
    "                output = output[..., gene_mask, :]\n",
    "                output = output.cpu().numpy()\n",
    "                minibatch_samples.append(output)\n",
    "            # samples by cells by genes by four\n",
    "            states.append(np.stack(minibatch_samples, axis=0))\n",
    "            if return_mean:\n",
    "                states[-1] = np.mean(states[-1], axis=0)\n",
    "\n",
    "        states = np.concatenate(states, axis=0)\n",
    "        state_cats = [\n",
    "            \"induction\",\n",
    "            \"induction_steady\",\n",
    "            \"repression\",\n",
    "            \"repression_steady\",\n",
    "        ]\n",
    "        if hard_assignment and return_mean:\n",
    "            hard_assign = states.argmax(-1)\n",
    "\n",
    "            hard_assign = pd.DataFrame(\n",
    "                data=hard_assign, index=adata.obs_names, columns=adata.var_names\n",
    "            )\n",
    "            for i, s in enumerate(state_cats):\n",
    "                hard_assign = hard_assign.replace(i, s)\n",
    "\n",
    "            states = hard_assign\n",
    "\n",
    "        return states, state_cats\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_latent_time(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        indices: Optional[Sequence[int]] = None,\n",
    "        gene_list: Optional[Sequence[str]] = None,\n",
    "        time_statistic: Literal[\"mean\", \"max\"] = \"mean\",\n",
    "        n_samples: int = 1,\n",
    "        n_samples_overall: Optional[int] = None,\n",
    "        batch_size: Optional[int] = None,\n",
    "        return_mean: bool = True,\n",
    "        return_numpy: Optional[bool] = None,\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Returns the cells by genes latent time.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        adata\n",
    "            AnnData object with equivalent structure to initial AnnData. If `None`, defaults to the\n",
    "            AnnData object used to initialize the model.\n",
    "        indices\n",
    "            Indices of cells in adata to use. If `None`, all cells are used.\n",
    "        gene_list\n",
    "            Return frequencies of expression for a subset of genes.\n",
    "            This can save memory when working with large datasets and few genes are\n",
    "            of interest.\n",
    "        time_statistic\n",
    "            Whether to compute expected time over states, or maximum a posteriori time over maximal\n",
    "            probability state.\n",
    "        n_samples\n",
    "            Number of posterior samples to use for estimation.\n",
    "        n_samples_overall\n",
    "            Number of overall samples to return. Setting this forces n_samples=1.\n",
    "        batch_size\n",
    "            Minibatch size for data loading into model. Defaults to `scvi.settings.batch_size`.\n",
    "        return_mean\n",
    "            Whether to return the mean of the samples.\n",
    "        return_numpy\n",
    "            Return a :class:`~numpy.ndarray` instead of a :class:`~pandas.DataFrame`. DataFrame includes\n",
    "            gene names as columns. If either `n_samples=1` or `return_mean=True`, defaults to `False`.\n",
    "            Otherwise, it defaults to `True`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        If `n_samples` > 1 and `return_mean` is False, then the shape is `(samples, cells, genes)`.\n",
    "        Otherwise, shape is `(cells, genes)`. In this case, return type is :class:`~pandas.DataFrame` unless `return_numpy` is True.\n",
    "        \"\"\"\n",
    "        adata = self._validate_anndata(adata)\n",
    "        if indices is None:\n",
    "            indices = np.arange(adata.n_obs)\n",
    "        if n_samples_overall is not None:\n",
    "            indices = np.random.choice(indices, n_samples_overall)\n",
    "        scdl = self._make_data_loader(\n",
    "            adata=adata, indices=indices, batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        if gene_list is None:\n",
    "            gene_mask = slice(None)\n",
    "        else:\n",
    "            all_genes = adata.var_names\n",
    "            gene_mask = [True if gene in gene_list else False for gene in all_genes]\n",
    "\n",
    "        if n_samples > 1 and return_mean is False:\n",
    "            if return_numpy is False:\n",
    "                warnings.warn(\n",
    "                    \"return_numpy must be True if n_samples > 1 and return_mean is False, returning np.ndarray\"\n",
    "                )\n",
    "            return_numpy = True\n",
    "        if indices is None:\n",
    "            indices = np.arange(adata.n_obs)\n",
    "\n",
    "        times = []\n",
    "        for tensors in scdl:\n",
    "            minibatch_samples = []\n",
    "            for _ in range(n_samples):\n",
    "                _, generative_outputs = self.module.forward(\n",
    "                    tensors=tensors,\n",
    "                    compute_loss=False,\n",
    "                )\n",
    "                pi = generative_outputs[\"px_pi\"]\n",
    "                ind_prob = pi[..., 0]\n",
    "                steady_prob = pi[..., 1]\n",
    "                rep_prob = pi[..., 2]\n",
    "                # rep_steady_prob = pi[..., 3]\n",
    "                switch_time = F.softplus(self.module.switch_time_unconstr)\n",
    "\n",
    "                ind_time = generative_outputs[\"px_rho\"] * switch_time\n",
    "                rep_time = switch_time + (\n",
    "                    generative_outputs[\"px_tau\"] * (self.module.t_max - switch_time)\n",
    "                )\n",
    "\n",
    "                if time_statistic == \"mean\":\n",
    "                    output = (\n",
    "                        ind_prob * ind_time\n",
    "                        + rep_prob * rep_time\n",
    "                        + steady_prob * switch_time\n",
    "                        # + rep_steady_prob * self.module.t_max\n",
    "                    )\n",
    "                else:\n",
    "                    t = torch.stack(\n",
    "                        [\n",
    "                            ind_time,\n",
    "                            switch_time.expand(ind_time.shape),\n",
    "                            rep_time,\n",
    "                            torch.zeros_like(ind_time),\n",
    "                        ],\n",
    "                        dim=2,\n",
    "                    )\n",
    "                    max_prob = torch.amax(pi, dim=-1)\n",
    "                    max_prob = torch.stack([max_prob] * 4, dim=2)\n",
    "                    max_prob_mask = pi.ge(max_prob)\n",
    "                    output = (t * max_prob_mask).sum(dim=-1)\n",
    "\n",
    "                output = output[..., gene_mask]\n",
    "                output = output.cpu().numpy()\n",
    "                minibatch_samples.append(output)\n",
    "            # samples by cells by genes by four\n",
    "            times.append(np.stack(minibatch_samples, axis=0))\n",
    "            if return_mean:\n",
    "                times[-1] = np.mean(times[-1], axis=0)\n",
    "\n",
    "        if n_samples > 1:\n",
    "            # The -2 axis correspond to cells.\n",
    "            times = np.concatenate(times, axis=-2)\n",
    "        else:\n",
    "            times = np.concatenate(times, axis=0)\n",
    "\n",
    "        if return_numpy is None or return_numpy is False:\n",
    "            return pd.DataFrame(\n",
    "                times,\n",
    "                columns=adata.var_names[gene_mask],\n",
    "                index=adata.obs_names[indices],\n",
    "            )\n",
    "        else:\n",
    "            return times\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_velocity(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        indices: Optional[Sequence[int]] = None,\n",
    "        gene_list: Optional[Sequence[str]] = None,\n",
    "        n_samples: int = 1,\n",
    "        n_samples_overall: Optional[int] = None,\n",
    "        batch_size: Optional[int] = None,\n",
    "        return_mean: bool = True,\n",
    "        return_numpy: Optional[bool] = None,\n",
    "        velo_statistic: str = \"mean\",\n",
    "        velo_mode: Literal[\"spliced\", \"unspliced\"] = \"spliced\",\n",
    "        clip: bool = True,\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Returns cells by genes velocity estimates.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        adata\n",
    "            AnnData object with equivalent structure to initial AnnData. If `None`, defaults to the\n",
    "            AnnData object used to initialize the model.\n",
    "        indices\n",
    "            Indices of cells in adata to use. If `None`, all cells are used.\n",
    "        gene_list\n",
    "            Return velocities for a subset of genes.\n",
    "            This can save memory when working with large datasets and few genes are\n",
    "            of interest.\n",
    "        n_samples\n",
    "            Number of posterior samples to use for estimation for each cell.\n",
    "        n_samples_overall\n",
    "            Number of overall samples to return. Setting this forces n_samples=1.\n",
    "        batch_size\n",
    "            Minibatch size for data loading into model. Defaults to `scvi.settings.batch_size`.\n",
    "        return_mean\n",
    "            Whether to return the mean of the samples.\n",
    "        return_numpy\n",
    "            Return a :class:`~numpy.ndarray` instead of a :class:`~pandas.DataFrame`. DataFrame includes\n",
    "            gene names as columns. If either `n_samples=1` or `return_mean=True`, defaults to `False`.\n",
    "            Otherwise, it defaults to `True`.\n",
    "        velo_statistic\n",
    "            Whether to compute expected velocity over states, or maximum a posteriori velocity over maximal\n",
    "            probability state.\n",
    "        velo_mode\n",
    "            Compute ds/dt or du/dt.\n",
    "        clip\n",
    "            Clip to minus spliced value\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        If `n_samples` > 1 and `return_mean` is False, then the shape is `(samples, cells, genes)`.\n",
    "        Otherwise, shape is `(cells, genes)`. In this case, return type is :class:`~pandas.DataFrame` unless `return_numpy` is True.\n",
    "        \"\"\"\n",
    "        adata = self._validate_anndata(adata)\n",
    "        if indices is None:\n",
    "            indices = np.arange(adata.n_obs)\n",
    "        if n_samples_overall is not None:\n",
    "            indices = np.random.choice(indices, n_samples_overall)\n",
    "            n_samples = 1\n",
    "        scdl = self._make_data_loader(\n",
    "            adata=adata, indices=indices, batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        if gene_list is None:\n",
    "            gene_mask = slice(None)\n",
    "        else:\n",
    "            all_genes = adata.var_names\n",
    "            gene_mask = [True if gene in gene_list else False for gene in all_genes]\n",
    "\n",
    "        if n_samples > 1 and return_mean is False:\n",
    "            if return_numpy is False:\n",
    "                warnings.warn(\n",
    "                    \"return_numpy must be True if n_samples > 1 and return_mean is False, returning np.ndarray\"\n",
    "                )\n",
    "            return_numpy = True\n",
    "        if indices is None:\n",
    "            indices = np.arange(adata.n_obs)\n",
    "\n",
    "        velos = []\n",
    "        for tensors in scdl:\n",
    "            minibatch_samples = []\n",
    "            for _ in range(n_samples):\n",
    "                inference_outputs, generative_outputs = self.module.forward(\n",
    "                    tensors=tensors,\n",
    "                    compute_loss=False,\n",
    "                )\n",
    "                pi = generative_outputs[\"px_pi\"]\n",
    "                alpha = inference_outputs[\"alpha\"]\n",
    "                alpha_1 = inference_outputs[\"alpha_1\"]\n",
    "                lambda_alpha = inference_outputs[\"lambda_alpha\"]\n",
    "                beta = inference_outputs[\"beta\"]\n",
    "                gamma = inference_outputs[\"gamma\"]\n",
    "                tau = generative_outputs[\"px_tau\"]\n",
    "                rho = generative_outputs[\"px_rho\"]\n",
    "\n",
    "                ind_prob = pi[..., 0]\n",
    "                steady_prob = pi[..., 1]\n",
    "                rep_prob = pi[..., 2]\n",
    "                switch_time = F.softplus(self.module.switch_time_unconstr)\n",
    "\n",
    "                ind_time = switch_time * rho\n",
    "                u_0, s_0 = self.module._get_induction_unspliced_spliced(\n",
    "                    alpha, alpha_1, lambda_alpha, beta, gamma, switch_time\n",
    "                )\n",
    "                rep_time = (self.module.t_max - switch_time) * tau\n",
    "                mean_u_rep, mean_s_rep = self.module._get_repression_unspliced_spliced(\n",
    "                    u_0,\n",
    "                    s_0,\n",
    "                    beta,\n",
    "                    gamma,\n",
    "                    rep_time,\n",
    "                )\n",
    "                if velo_mode == \"spliced\":\n",
    "                    velo_rep = beta * mean_u_rep - gamma * mean_s_rep\n",
    "                else:\n",
    "                    velo_rep = -beta * mean_u_rep\n",
    "                mean_u_ind, mean_s_ind = self.module._get_induction_unspliced_spliced(\n",
    "                    alpha, alpha_1, lambda_alpha, beta, gamma, ind_time\n",
    "                )\n",
    "                if velo_mode == \"spliced\":\n",
    "                    velo_ind = beta * mean_u_ind - gamma * mean_s_ind\n",
    "                else:\n",
    "                    transcription_rate = alpha_1 - (alpha_1 - alpha) * torch.exp(\n",
    "                        -lambda_alpha * ind_time\n",
    "                    )\n",
    "                    velo_ind = transcription_rate - beta * mean_u_ind\n",
    "\n",
    "                if velo_mode == \"spliced\":\n",
    "                    # velo_steady = beta * u_0 - gamma * s_0\n",
    "                    velo_steady = torch.zeros_like(velo_ind)\n",
    "                else:\n",
    "                    # velo_steady = alpha - beta * u_0\n",
    "                    velo_steady = torch.zeros_like(velo_ind)\n",
    "\n",
    "                # expectation\n",
    "                if velo_statistic == \"mean\":\n",
    "                    output = (\n",
    "                        ind_prob * velo_ind\n",
    "                        + rep_prob * velo_rep\n",
    "                        + steady_prob * velo_steady\n",
    "                    )\n",
    "                # maximum\n",
    "                else:\n",
    "                    v = torch.stack(\n",
    "                        [\n",
    "                            velo_ind,\n",
    "                            velo_steady.expand(velo_ind.shape),\n",
    "                            velo_rep,\n",
    "                            torch.zeros_like(velo_rep),\n",
    "                        ],\n",
    "                        dim=2,\n",
    "                    )\n",
    "                    max_prob = torch.amax(pi, dim=-1)\n",
    "                    max_prob = torch.stack([max_prob] * 4, dim=2)\n",
    "                    max_prob_mask = pi.ge(max_prob)\n",
    "                    output = (v * max_prob_mask).sum(dim=-1)\n",
    "\n",
    "                output = output[..., gene_mask]\n",
    "                output = output.cpu().numpy()\n",
    "                minibatch_samples.append(output)\n",
    "            # samples by cells by genes\n",
    "            velos.append(np.stack(minibatch_samples, axis=0))\n",
    "            if return_mean:\n",
    "                # mean over samples axis\n",
    "                velos[-1] = np.mean(velos[-1], axis=0)\n",
    "\n",
    "        if n_samples > 1:\n",
    "            # The -2 axis correspond to cells.\n",
    "            velos = np.concatenate(velos, axis=-2)\n",
    "        else:\n",
    "            velos = np.concatenate(velos, axis=0)\n",
    "\n",
    "        spliced = self.adata_manager.get_from_registry(REGISTRY_KEYS.X_KEY)\n",
    "\n",
    "        if clip:\n",
    "            velos = np.clip(velos, -spliced[indices], None)\n",
    "\n",
    "        if return_numpy is None or return_numpy is False:\n",
    "            return pd.DataFrame(\n",
    "                velos,\n",
    "                columns=adata.var_names[gene_mask],\n",
    "                index=adata.obs_names[indices],\n",
    "            )\n",
    "        else:\n",
    "            return velos\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_velocity_from_latent(\n",
    "        self,\n",
    "        latent_representation: np.ndarray,\n",
    "        return_numpy: Optional[bool] = None,\n",
    "        velo_statistic: str = \"mean\",\n",
    "        velo_mode: Literal[\"spliced\", \"unspliced\"] = \"spliced\",\n",
    "        clip: bool = True,\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        r\"\"\"\n",
    "        Returns the normalized (decoded) gene expression.\n",
    "\n",
    "        This is denoted as :math:`\\rho_n` in the scVI paper.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        adata\n",
    "            AnnData object with equivalent structure to initial AnnData. If `None`, defaults to the\n",
    "            AnnData object used to initialize the model.\n",
    "        return_numpy\n",
    "            Return a :class:`~numpy.ndarray` instead of a :class:`~pandas.DataFrame`. DataFrame includes\n",
    "            gene names as columns. If either `n_samples=1` or `return_mean=True`, defaults to `False`.\n",
    "            Otherwise, it defaults to `True`.\n",
    "        clip\n",
    "            Clip to minus spliced value\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        If `n_samples` > 1 and `return_mean` is False, then the shape is `(samples, cells, genes)`.\n",
    "        Otherwise, shape is `(cells, genes)`. In this case, return type is :class:`~pandas.DataFrame` unless `return_numpy` is True.\n",
    "        \"\"\"\n",
    "        adata = AnnData(latent_representation)\n",
    "        data_key = \"Z\"\n",
    "        manager = AnnDataManager(\n",
    "            [LayerField(data_key, layer=None, is_count_data=False)]\n",
    "        )\n",
    "        manager.register_fields(adata)\n",
    "        scdl = AnnDataLoader(manager)\n",
    "\n",
    "        gamma, beta, alpha, alpha_1, lambda_alpha = self.module._get_rates()\n",
    "\n",
    "        velos = []\n",
    "        for tensors in scdl:\n",
    "            z = tensors[data_key]\n",
    "            generative_outputs = self.module.generative(\n",
    "                z=z,\n",
    "                gamma=gamma,\n",
    "                beta=beta,\n",
    "                alpha=alpha,\n",
    "                alpha_1=alpha_1,\n",
    "                lambda_alpha=lambda_alpha,\n",
    "            )\n",
    "            pi = generative_outputs[\"px_pi\"]\n",
    "            tau = generative_outputs[\"px_tau\"]\n",
    "            rho = generative_outputs[\"px_rho\"]\n",
    "\n",
    "            ind_prob = pi[..., 0]\n",
    "            steady_prob = pi[..., 1]\n",
    "            rep_prob = pi[..., 2]\n",
    "            switch_time = F.softplus(self.module.switch_time_unconstr)\n",
    "\n",
    "            ind_time = switch_time * rho\n",
    "            u_0, s_0 = self.module._get_induction_unspliced_spliced(\n",
    "                alpha, alpha_1, lambda_alpha, beta, gamma, switch_time\n",
    "            )\n",
    "            rep_time = (self.module.t_max - switch_time) * tau\n",
    "            mean_u_rep, mean_s_rep = self.module._get_repression_unspliced_spliced(\n",
    "                u_0,\n",
    "                s_0,\n",
    "                beta,\n",
    "                gamma,\n",
    "                rep_time,\n",
    "            )\n",
    "            if velo_mode == \"spliced\":\n",
    "                velo_rep = beta * mean_u_rep - gamma * mean_s_rep\n",
    "            else:\n",
    "                velo_rep = -beta * mean_u_rep\n",
    "            mean_u_ind, mean_s_ind = self.module._get_induction_unspliced_spliced(\n",
    "                alpha, alpha_1, lambda_alpha, beta, gamma, ind_time\n",
    "            )\n",
    "            if velo_mode == \"spliced\":\n",
    "                velo_ind = beta * mean_u_ind - gamma * mean_s_ind\n",
    "            else:\n",
    "                transcription_rate = alpha_1 - (alpha_1 - alpha) * torch.exp(\n",
    "                    -lambda_alpha * ind_time\n",
    "                )\n",
    "                velo_ind = transcription_rate - beta * mean_u_ind\n",
    "\n",
    "            if velo_mode == \"spliced\":\n",
    "                # velo_steady = beta * u_0 - gamma * s_0\n",
    "                velo_steady = torch.zeros_like(velo_ind)\n",
    "            else:\n",
    "                # velo_steady = alpha - beta * u_0\n",
    "                velo_steady = torch.zeros_like(velo_ind)\n",
    "\n",
    "            # expectation\n",
    "            if velo_statistic == \"mean\":\n",
    "                output = (\n",
    "                    ind_prob * velo_ind\n",
    "                    + rep_prob * velo_rep\n",
    "                    + steady_prob * velo_steady\n",
    "                )\n",
    "            # maximum\n",
    "            else:\n",
    "                v = torch.stack(\n",
    "                    [\n",
    "                        velo_ind,\n",
    "                        velo_steady.expand(velo_ind.shape),\n",
    "                        velo_rep,\n",
    "                        torch.zeros_like(velo_rep),\n",
    "                    ],\n",
    "                    dim=2,\n",
    "                )\n",
    "                max_prob = torch.amax(pi, dim=-1)\n",
    "                max_prob = torch.stack([max_prob] * 4, dim=2)\n",
    "                max_prob_mask = pi.ge(max_prob)\n",
    "                output = (v * max_prob_mask).sum(dim=-1)\n",
    "\n",
    "            # samples by cells by genes\n",
    "            velos.append(output.cpu().numpy())\n",
    "\n",
    "        velos = np.concatenate(velos, axis=0)\n",
    "\n",
    "        if return_numpy is None or return_numpy is False:\n",
    "            return pd.DataFrame(\n",
    "                velos,\n",
    "                columns=self.adata.var_names,\n",
    "            )\n",
    "        else:\n",
    "            return velos\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_expression_fit(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        indices: Optional[Sequence[int]] = None,\n",
    "        gene_list: Optional[Sequence[str]] = None,\n",
    "        n_samples: int = 1,\n",
    "        batch_size: Optional[int] = None,\n",
    "        return_mean: bool = True,\n",
    "        return_numpy: Optional[bool] = None,\n",
    "        restrict_to_latent_dim: Optional[int] = None,\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        r\"\"\"\n",
    "        Returns the fitted spliced and unspliced abundance (s(t) and u(t)).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        adata\n",
    "            AnnData object with equivalent structure to initial AnnData. If `None`, defaults to the\n",
    "            AnnData object used to initialize the model.\n",
    "        indices\n",
    "            Indices of cells in adata to use. If `None`, all cells are used.\n",
    "        gene_list\n",
    "            Return frequencies of expression for a subset of genes.\n",
    "            This can save memory when working with large datasets and few genes are\n",
    "            of interest.\n",
    "        n_samples\n",
    "            Number of posterior samples to use for estimation.\n",
    "        batch_size\n",
    "            Minibatch size for data loading into model. Defaults to `scvi.settings.batch_size`.\n",
    "        return_mean\n",
    "            Whether to return the mean of the samples.\n",
    "        return_numpy\n",
    "            Return a :class:`~numpy.ndarray` instead of a :class:`~pandas.DataFrame`. DataFrame includes\n",
    "            gene names as columns. If either `n_samples=1` or `return_mean=True`, defaults to `False`.\n",
    "            Otherwise, it defaults to `True`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        If `n_samples` > 1 and `return_mean` is False, then the shape is `(samples, cells, genes)`.\n",
    "        Otherwise, shape is `(cells, genes)`. In this case, return type is :class:`~pandas.DataFrame` unless `return_numpy` is True.\n",
    "        \"\"\"\n",
    "        adata = self._validate_anndata(adata)\n",
    "\n",
    "        scdl = self._make_data_loader(\n",
    "            adata=adata, indices=indices, batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        if gene_list is None:\n",
    "            gene_mask = slice(None)\n",
    "        else:\n",
    "            all_genes = adata.var_names\n",
    "            gene_mask = [True if gene in gene_list else False for gene in all_genes]\n",
    "\n",
    "        if n_samples > 1 and return_mean is False:\n",
    "            if return_numpy is False:\n",
    "                warnings.warn(\n",
    "                    \"return_numpy must be True if n_samples > 1 and return_mean is False, returning np.ndarray\"\n",
    "                )\n",
    "            return_numpy = True\n",
    "        if indices is None:\n",
    "            indices = np.arange(adata.n_obs)\n",
    "\n",
    "        fits_s = []\n",
    "        fits_u = []\n",
    "        for tensors in scdl:\n",
    "            minibatch_samples_s = []\n",
    "            minibatch_samples_u = []\n",
    "            for _ in range(n_samples):\n",
    "                inference_outputs, generative_outputs = self.module.forward(\n",
    "                    tensors=tensors,\n",
    "                    compute_loss=False,\n",
    "                    generative_kwargs=dict(latent_dim=restrict_to_latent_dim),\n",
    "                )\n",
    "\n",
    "                gamma = inference_outputs[\"gamma\"]\n",
    "                beta = inference_outputs[\"beta\"]\n",
    "                alpha = inference_outputs[\"alpha\"]\n",
    "                alpha_1 = inference_outputs[\"alpha_1\"]\n",
    "                lambda_alpha = inference_outputs[\"lambda_alpha\"]\n",
    "                px_pi = generative_outputs[\"px_pi\"]\n",
    "                scale = generative_outputs[\"scale\"]\n",
    "                px_rho = generative_outputs[\"px_rho\"]\n",
    "                px_tau = generative_outputs[\"px_tau\"]\n",
    "\n",
    "                (mixture_dist_s, mixture_dist_u, _,) = self.module.get_px(\n",
    "                    px_pi,\n",
    "                    px_rho,\n",
    "                    px_tau,\n",
    "                    scale,\n",
    "                    gamma,\n",
    "                    beta,\n",
    "                    alpha,\n",
    "                    alpha_1,\n",
    "                    lambda_alpha,\n",
    "                )\n",
    "                fit_s = mixture_dist_s.mean\n",
    "                fit_u = mixture_dist_u.mean\n",
    "\n",
    "                fit_s = fit_s[..., gene_mask]\n",
    "                fit_s = fit_s.cpu().numpy()\n",
    "                fit_u = fit_u[..., gene_mask]\n",
    "                fit_u = fit_u.cpu().numpy()\n",
    "\n",
    "                minibatch_samples_s.append(fit_s)\n",
    "                minibatch_samples_u.append(fit_u)\n",
    "\n",
    "            # samples by cells by genes\n",
    "            fits_s.append(np.stack(minibatch_samples_s, axis=0))\n",
    "            if return_mean:\n",
    "                # mean over samples axis\n",
    "                fits_s[-1] = np.mean(fits_s[-1], axis=0)\n",
    "            # samples by cells by genes\n",
    "            fits_u.append(np.stack(minibatch_samples_u, axis=0))\n",
    "            if return_mean:\n",
    "                # mean over samples axis\n",
    "                fits_u[-1] = np.mean(fits_u[-1], axis=0)\n",
    "\n",
    "        if n_samples > 1:\n",
    "            # The -2 axis correspond to cells.\n",
    "            fits_s = np.concatenate(fits_s, axis=-2)\n",
    "            fits_u = np.concatenate(fits_u, axis=-2)\n",
    "        else:\n",
    "            fits_s = np.concatenate(fits_s, axis=0)\n",
    "            fits_u = np.concatenate(fits_u, axis=0)\n",
    "\n",
    "        if return_numpy is None or return_numpy is False:\n",
    "            df_s = pd.DataFrame(\n",
    "                fits_s,\n",
    "                columns=adata.var_names[gene_mask],\n",
    "                index=adata.obs_names[indices],\n",
    "            )\n",
    "            df_u = pd.DataFrame(\n",
    "                fits_u,\n",
    "                columns=adata.var_names[gene_mask],\n",
    "                index=adata.obs_names[indices],\n",
    "            )\n",
    "            return df_s, df_u\n",
    "        else:\n",
    "            return fits_s, fits_u\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_gene_likelihood(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        indices: Optional[Sequence[int]] = None,\n",
    "        gene_list: Optional[Sequence[str]] = None,\n",
    "        n_samples: int = 1,\n",
    "        batch_size: Optional[int] = None,\n",
    "        return_mean: bool = True,\n",
    "        return_numpy: Optional[bool] = None,\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        r\"\"\"\n",
    "        Returns the likelihood per gene. Higher is better.\n",
    "\n",
    "        This is denoted as :math:`\\rho_n` in the scVI paper.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        adata\n",
    "            AnnData object with equivalent structure to initial AnnData. If `None`, defaults to the\n",
    "            AnnData object used to initialize the model.\n",
    "        indices\n",
    "            Indices of cells in adata to use. If `None`, all cells are used.\n",
    "        transform_batch\n",
    "            Batch to condition on.\n",
    "            If transform_batch is:\n",
    "\n",
    "            - None, then real observed batch is used.\n",
    "            - int, then batch transform_batch is used.\n",
    "        gene_list\n",
    "            Return frequencies of expression for a subset of genes.\n",
    "            This can save memory when working with large datasets and few genes are\n",
    "            of interest.\n",
    "        library_size\n",
    "            Scale the expression frequencies to a common library size.\n",
    "            This allows gene expression levels to be interpreted on a common scale of relevant\n",
    "            magnitude. If set to `\"latent\"`, use the latent libary size.\n",
    "        n_samples\n",
    "            Number of posterior samples to use for estimation.\n",
    "        batch_size\n",
    "            Minibatch size for data loading into model. Defaults to `scvi.settings.batch_size`.\n",
    "        return_mean\n",
    "            Whether to return the mean of the samples.\n",
    "        return_numpy\n",
    "            Return a :class:`~numpy.ndarray` instead of a :class:`~pandas.DataFrame`. DataFrame includes\n",
    "            gene names as columns. If either `n_samples=1` or `return_mean=True`, defaults to `False`.\n",
    "            Otherwise, it defaults to `True`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        If `n_samples` > 1 and `return_mean` is False, then the shape is `(samples, cells, genes)`.\n",
    "        Otherwise, shape is `(cells, genes)`. In this case, return type is :class:`~pandas.DataFrame` unless `return_numpy` is True.\n",
    "        \"\"\"\n",
    "        adata = self._validate_anndata(adata)\n",
    "        scdl = self._make_data_loader(\n",
    "            adata=adata, indices=indices, batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        if gene_list is None:\n",
    "            gene_mask = slice(None)\n",
    "        else:\n",
    "            all_genes = adata.var_names\n",
    "            gene_mask = [True if gene in gene_list else False for gene in all_genes]\n",
    "\n",
    "        if n_samples > 1 and return_mean is False:\n",
    "            if return_numpy is False:\n",
    "                warnings.warn(\n",
    "                    \"return_numpy must be True if n_samples > 1 and return_mean is False, returning np.ndarray\"\n",
    "                )\n",
    "            return_numpy = True\n",
    "        if indices is None:\n",
    "            indices = np.arange(adata.n_obs)\n",
    "\n",
    "        rls = []\n",
    "        for tensors in scdl:\n",
    "            minibatch_samples = []\n",
    "            for _ in range(n_samples):\n",
    "                inference_outputs, generative_outputs = self.module.forward(\n",
    "                    tensors=tensors,\n",
    "                    compute_loss=False,\n",
    "                )\n",
    "                spliced = tensors[REGISTRY_KEYS.X_KEY]\n",
    "                unspliced = tensors[REGISTRY_KEYS.U_KEY]\n",
    "\n",
    "                gamma = inference_outputs[\"gamma\"]\n",
    "                beta = inference_outputs[\"beta\"]\n",
    "                alpha = inference_outputs[\"alpha\"]\n",
    "                alpha_1 = inference_outputs[\"alpha_1\"]\n",
    "                lambda_alpha = inference_outputs[\"lambda_alpha\"]\n",
    "                px_pi = generative_outputs[\"px_pi\"]\n",
    "                scale = generative_outputs[\"scale\"]\n",
    "                px_rho = generative_outputs[\"px_rho\"]\n",
    "                px_tau = generative_outputs[\"px_tau\"]\n",
    "                dec_mean = generative_outputs[\"gene_recon\"]\n",
    "                dec_lat = generative_outputs[\"dec_latent\"]\n",
    "\n",
    "                (mixture_dist_s, mixture_dist_u, _,) = self.module.get_px(\n",
    "                    px_pi,\n",
    "                    px_rho,\n",
    "                    px_tau,\n",
    "                    scale,\n",
    "                    gamma,\n",
    "                    beta,\n",
    "                    alpha,\n",
    "                    alpha_1,\n",
    "                    lambda_alpha,\n",
    "                )\n",
    "                reconst_loss_s = -mixture_dist_s.log_prob(spliced)\n",
    "                reconst_loss_u = -mixture_dist_u.log_prob(unspliced)\n",
    "                output = -(reconst_loss_s + reconst_loss_u)\n",
    "                output = output[..., gene_mask]\n",
    "                output = output.cpu().numpy()\n",
    "                minibatch_samples.append(output)\n",
    "            # samples by cells by genes by four\n",
    "            rls.append(np.stack(minibatch_samples, axis=0))\n",
    "            if return_mean:\n",
    "                rls[-1] = np.mean(rls[-1], axis=0)\n",
    "\n",
    "        rls = np.concatenate(rls, axis=0)\n",
    "        return rls.shape, dec_mean.shape, dec_lat.shape\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_rates(self, mean: bool = True):\n",
    "\n",
    "        gamma, beta, alpha, alpha_1, lambda_alpha = self.module._get_rates()\n",
    "\n",
    "        return {\n",
    "            \"beta\": beta.cpu().numpy(),\n",
    "            \"gamma\": gamma.cpu().numpy(),\n",
    "            \"alpha\": alpha.cpu().numpy(),\n",
    "            \"alpha_1\": alpha_1.cpu().numpy(),\n",
    "            \"lambda_alpha\": lambda_alpha.cpu().numpy(),\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    @setup_anndata_dsp.dedent\n",
    "    def setup_anndata(\n",
    "        cls,\n",
    "        adata: AnnData,\n",
    "        spliced_layer: str,\n",
    "        unspliced_layer: str,\n",
    "        **kwargs,\n",
    "    ) -> Optional[AnnData]:\n",
    "        \"\"\"\n",
    "        %(summary)s.\n",
    "        Parameters\n",
    "        ----------\n",
    "        %(param_adata)s\n",
    "        spliced_layer\n",
    "            Layer in adata with spliced normalized expression\n",
    "        unspliced_layer\n",
    "            Layer in adata with unspliced normalized expression\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        %(returns)s\n",
    "        \"\"\"\n",
    "        setup_method_args = cls._get_setup_method_args(**locals())\n",
    "        anndata_fields = [\n",
    "            LayerField(REGISTRY_KEYS.X_KEY, spliced_layer, is_count_data=False),\n",
    "            LayerField(REGISTRY_KEYS.U_KEY, unspliced_layer, is_count_data=False),\n",
    "        ]\n",
    "        adata_manager = AnnDataManager(\n",
    "            fields=anndata_fields, setup_method_args=setup_method_args\n",
    "        )\n",
    "        adata_manager.register_fields(adata, **kwargs)\n",
    "        cls.register_manager(adata_manager)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @_doc_params(\n",
    "        doc_differential_expression=doc_differential_expression,\n",
    "    )\n",
    "    def differential_velocity(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        groupby: Optional[str] = None,\n",
    "        group1: Optional[Iterable[str]] = None,\n",
    "        group2: Optional[str] = None,\n",
    "        idx1: Optional[Union[Sequence[int], Sequence[bool], str]] = None,\n",
    "        idx2: Optional[Union[Sequence[int], Sequence[bool], str]] = None,\n",
    "        mode: Literal[\"vanilla\", \"change\"] = \"vanilla\",\n",
    "        delta: float = 0.25,\n",
    "        batch_size: Optional[int] = None,\n",
    "        all_stats: bool = True,\n",
    "        batch_correction: bool = False,\n",
    "        batchid1: Optional[Iterable[str]] = None,\n",
    "        batchid2: Optional[Iterable[str]] = None,\n",
    "        fdr_target: float = 0.05,\n",
    "        silent: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> pd.DataFrame:\n",
    "        r\"\"\"\n",
    "        A unified method for differential velocity analysis.\n",
    "\n",
    "        Implements `\"vanilla\"` DE [Lopez18]_ and `\"change\"` mode DE [Boyeau19]_.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        {doc_differential_expression}\n",
    "        **kwargs\n",
    "            Keyword args for :meth:`scvi.model.base.DifferentialComputation.get_bayes_factors`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Differential expression DataFrame.\n",
    "        \"\"\"\n",
    "        adata = self._validate_anndata(adata)\n",
    "\n",
    "        def model_fn(adata, **kwargs):\n",
    "            if \"transform_batch\" in kwargs.keys():\n",
    "                kwargs.pop(\"transform_batch\")\n",
    "            return partial(\n",
    "                self.get_velocity,\n",
    "                batch_size=batch_size,\n",
    "                n_samples=1,\n",
    "                return_numpy=True,\n",
    "                clip=False,\n",
    "            )(adata, **kwargs)\n",
    "\n",
    "        col_names = adata.var_names\n",
    "\n",
    "        result = _de_core(\n",
    "            self.get_anndata_manager(adata, required=True),\n",
    "            model_fn,\n",
    "            groupby,\n",
    "            group1,\n",
    "            group2,\n",
    "            idx1,\n",
    "            idx2,\n",
    "            all_stats,\n",
    "            scrna_raw_counts_properties,\n",
    "            col_names,\n",
    "            mode,\n",
    "            batchid1,\n",
    "            batchid2,\n",
    "            delta,\n",
    "            batch_correction,\n",
    "            fdr_target,\n",
    "            silent,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def differential_transition(\n",
    "        self,\n",
    "        groupby: str,\n",
    "        group1: str,\n",
    "        group2: str,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        batch_size: Optional[int] = None,\n",
    "        n_samples: Optional[int] = 5000,\n",
    "    ) -> pd.DataFrame:\n",
    "        adata = self._validate_anndata(adata)\n",
    "        adata_manager = self.get_anndata_manager(adata, required=True)\n",
    "\n",
    "        if not isinstance(group1, str):\n",
    "            raise ValueError(\"Group 1 must be a string\")\n",
    "\n",
    "        cell_idx1 = (adata.obs[groupby] == group1).to_numpy().ravel()\n",
    "        if group2 is None:\n",
    "            cell_idx2 = ~cell_idx1\n",
    "        else:\n",
    "            cell_idx2 = (adata.obs[groupby] == group2).to_numpy().ravel()\n",
    "\n",
    "        indices1 = np.random.choice(\n",
    "            np.asarray(np.where(cell_idx1)[0].ravel()), n_samples\n",
    "        )\n",
    "        indices2 = np.random.choice(\n",
    "            np.asarray(np.where(cell_idx2)[0].ravel()), n_samples\n",
    "        )\n",
    "\n",
    "        velo1 = self.get_velocity(\n",
    "            adata,\n",
    "            return_numpy=True,\n",
    "            indices=indices1,\n",
    "            n_samples=1,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        velo1 = velo1 - velo1.mean(1)[:, np.newaxis]\n",
    "        velo2 = self.get_velocity(\n",
    "            adata,\n",
    "            return_numpy=True,\n",
    "            indices=indices2,\n",
    "            n_samples=1,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        velo2 = velo2 - velo2.mean(1)[:, np.newaxis]\n",
    "\n",
    "        spliced = adata_manager.get_from_registry(REGISTRY_KEYS.X_KEY)\n",
    "        delta12 = spliced[indices2] - spliced[indices1]\n",
    "        delta12 = delta12 - delta12.mean(1)[:, np.newaxis]\n",
    "\n",
    "        delta21 = spliced[indices1] - spliced[indices2]\n",
    "        delta21 = delta21 - delta21.mean(1)[:, np.newaxis]\n",
    "\n",
    "        # TODO: Make more efficient\n",
    "        correlation12 = np.diagonal(cosine_similarity(velo1, delta12))\n",
    "        correlation21 = np.diagonal(cosine_similarity(velo2, delta21))\n",
    "\n",
    "        return correlation12, correlation21\n",
    "\n",
    "    def get_loadings(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract per-gene weights in the linear decoder.\n",
    "\n",
    "        Shape is genes by `n_latent`.\n",
    "        \"\"\"\n",
    "        cols = [\"Z_{}\".format(i) for i in range(self.n_latent)]\n",
    "        var_names = self.adata.var_names\n",
    "        loadings = pd.DataFrame(\n",
    "            self.module.get_loadings(), index=var_names, columns=cols\n",
    "        )\n",
    "\n",
    "        return loadings\n",
    "\n",
    "    def get_variance_explained(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        labels_key: Optional[str] = None,\n",
    "        n_samples: int = 10,\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "        if self.module.decoder.linear_decoder is False:\n",
    "            raise ValueError(\"Model not trained with linear decoder\")\n",
    "        adata = self._validate_anndata(adata)\n",
    "        adata_manager = self.get_anndata_manager(adata)\n",
    "        n_latent = self.module.n_latent\n",
    "\n",
    "        if labels_key is not None:\n",
    "            groups = np.unique(adata.obs[labels_key])\n",
    "        else:\n",
    "            groups = [None]\n",
    "\n",
    "        spliced = adata_manager.get_from_registry(REGISTRY_KEYS.X_KEY)\n",
    "        unspliced = adata_manager.get_from_registry(REGISTRY_KEYS.U_KEY)\n",
    "\n",
    "        centered_s = spliced - spliced.mean(0)\n",
    "        centered_u = unspliced - unspliced.mean(0)\n",
    "\n",
    "        def r_squared(true_s, pred_s, true_u, pred_u, centered_s, centered_u):\n",
    "            rss_s = np.sum((true_s - pred_s) ** 2)\n",
    "            tss_s = np.sum(centered_s**2)\n",
    "            rss_u = np.sum((true_u - pred_u) ** 2)\n",
    "            tss_u = np.sum(centered_u**2)\n",
    "\n",
    "            return (1 - (rss_s + rss_u) / (tss_s + tss_u)) * 100\n",
    "\n",
    "        df_out = pd.DataFrame(\n",
    "            data=np.zeros((n_latent, len(groups))),\n",
    "            index=[f\"Z_{i}\" for i in range(n_latent)],\n",
    "            columns=groups if groups[0] is not None else [\"0\"],\n",
    "        )\n",
    "\n",
    "        for i in range(n_latent):\n",
    "            fitted_s, fitted_u = self.get_expression_fit(\n",
    "                adata, restrict_to_latent_dim=i, n_samples=n_samples, return_numpy=True\n",
    "            )\n",
    "            for j, g in enumerate(groups):\n",
    "                if g is None:\n",
    "                    subset = slice(None)\n",
    "                else:\n",
    "                    subset = adata.obs[labels_key] == g\n",
    "                r_2 = r_squared(\n",
    "                    spliced[subset],\n",
    "                    fitted_s[subset],\n",
    "                    unspliced[subset],\n",
    "                    fitted_u[subset],\n",
    "                    centered_s[subset],\n",
    "                    centered_u[subset],\n",
    "                )\n",
    "                df_out.iloc[i, j] = r_2\n",
    "\n",
    "        return df_out\n",
    "\n",
    "    def get_directional_uncertainty(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        n_samples: int = 50,\n",
    "        gene_list: Iterable[str] = None,\n",
    "        n_jobs: int = -1,\n",
    "    ):\n",
    "\n",
    "        adata = self._validate_anndata(adata)\n",
    "\n",
    "        logger.info(\"Sampling from model...\")\n",
    "        velocities_all = self.get_velocity(\n",
    "            n_samples=n_samples, return_mean=False, gene_list=gene_list\n",
    "        )  # (n_samples, n_cells, n_genes)\n",
    "\n",
    "        df, cosine_sims = _compute_directional_statistics_tensor(\n",
    "            tensor=velocities_all, n_jobs=n_jobs, n_cells=adata.n_obs\n",
    "        )\n",
    "        df.index = adata.obs_names\n",
    "\n",
    "        return df, cosine_sims\n",
    "\n",
    "    def get_permutation_scores(\n",
    "        self, labels_key: str, adata: Optional[AnnData] = None\n",
    "    ) -> Tuple[pd.DataFrame, AnnData]:\n",
    "        \"\"\"\n",
    "        Compute permutation scores.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        labels_key\n",
    "            Key in adata.obs encoding cell types\n",
    "        adata\n",
    "            AnnData object with equivalent structure to initial AnnData. If `None`, defaults to the\n",
    "            AnnData object used to initialize the model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple of DataFrame and AnnData. DataFrame is genes by cell types with score per cell type.\n",
    "        AnnData is the permutated version of the original AnnData.\n",
    "        \"\"\"\n",
    "        adata = self._validate_anndata(adata)\n",
    "        adata_manager = self.get_anndata_manager(adata)\n",
    "        if labels_key not in adata.obs:\n",
    "            raise ValueError(f\"{labels_key} not found in adata.obs\")\n",
    "\n",
    "        # shuffle spliced then unspliced\n",
    "        bdata = self._shuffle_layer_celltype(\n",
    "            adata_manager, labels_key, REGISTRY_KEYS.X_KEY\n",
    "        )\n",
    "        bdata_manager = self.get_anndata_manager(bdata)\n",
    "        bdata = self._shuffle_layer_celltype(\n",
    "            bdata_manager, labels_key, REGISTRY_KEYS.U_KEY\n",
    "        )\n",
    "        bdata_manager = self.get_anndata_manager(bdata)\n",
    "\n",
    "        ms_ = adata_manager.get_from_registry(REGISTRY_KEYS.X_KEY)\n",
    "        mu_ = adata_manager.get_from_registry(REGISTRY_KEYS.U_KEY)\n",
    "\n",
    "        ms_p = bdata_manager.get_from_registry(REGISTRY_KEYS.X_KEY)\n",
    "        mu_p = bdata_manager.get_from_registry(REGISTRY_KEYS.U_KEY)\n",
    "\n",
    "        spliced_, unspliced_ = self.get_expression_fit(adata, n_samples=10)\n",
    "        root_squared_error = np.abs(spliced_ - ms_)\n",
    "        root_squared_error += np.abs(unspliced_ - mu_)\n",
    "\n",
    "        spliced_p, unspliced_p = self.get_expression_fit(bdata, n_samples=10)\n",
    "        root_squared_error_p = np.abs(spliced_p - ms_p)\n",
    "        root_squared_error_p += np.abs(unspliced_p - mu_p)\n",
    "\n",
    "        celltypes = np.unique(adata.obs[labels_key])\n",
    "\n",
    "        dynamical_df = pd.DataFrame(\n",
    "            index=adata.var_names,\n",
    "            columns=celltypes,\n",
    "            data=np.zeros((adata.shape[1], len(celltypes))),\n",
    "        )\n",
    "        N = 200\n",
    "        for ct in celltypes:\n",
    "            for g in adata.var_names.tolist():\n",
    "                x = root_squared_error_p[g][adata.obs[labels_key] == ct]\n",
    "                y = root_squared_error[g][adata.obs[labels_key] == ct]\n",
    "                ratio = ttest_ind(x[:N], y[:N])[0]\n",
    "                dynamical_df.loc[g, ct] = ratio\n",
    "\n",
    "        return dynamical_df, bdata\n",
    "\n",
    "    def _shuffle_layer_celltype(\n",
    "        self, adata_manager: AnnDataManager, labels_key: str, registry_key: str\n",
    "    ) -> AnnData:\n",
    "        \"\"\"Shuffle cells within cell types for each gene.\"\"\"\n",
    "        from scvi.data._constants import _SCVI_UUID_KEY\n",
    "\n",
    "        bdata = adata_manager.adata.copy()\n",
    "        labels = bdata.obs[labels_key]\n",
    "        del bdata.uns[_SCVI_UUID_KEY]\n",
    "        self._validate_anndata(bdata)\n",
    "        bdata_manager = self.get_anndata_manager(bdata)\n",
    "\n",
    "        # get registry info to later set data back in bdata\n",
    "        # in a way that doesn't require actual knowledge of location\n",
    "        unspliced = bdata_manager.get_from_registry(registry_key)\n",
    "        u_registry = bdata_manager.data_registry[registry_key]\n",
    "        attr_name = u_registry.attr_name\n",
    "        attr_key = u_registry.attr_key\n",
    "\n",
    "        for lab in np.unique(labels):\n",
    "            mask = np.asarray(labels == lab)\n",
    "            unspliced_ct = unspliced[mask].copy()\n",
    "            unspliced_ct = np.apply_along_axis(\n",
    "                np.random.permutation, axis=0, arr=unspliced_ct\n",
    "            )\n",
    "            unspliced[mask] = unspliced_ct\n",
    "        # e.g., if using adata.X\n",
    "        if attr_key is None:\n",
    "            setattr(bdata, attr_name, unspliced)\n",
    "        # e.g., if using a layer\n",
    "        elif attr_key is not None:\n",
    "            attribute = getattr(bdata, attr_name)\n",
    "            attribute[attr_key] = unspliced\n",
    "            setattr(bdata, attr_name, attribute)\n",
    "\n",
    "        return bdata\n",
    "\n",
    "\n",
    "def _compute_directional_statistics_tensor(\n",
    "    tensor: np.ndarray, n_jobs: int, n_cells: int\n",
    ") -> pd.DataFrame:\n",
    "    df = pd.DataFrame(index=np.arange(n_cells))\n",
    "    df[\"directional_variance\"] = np.nan\n",
    "    df[\"directional_difference\"] = np.nan\n",
    "    df[\"directional_cosine_sim_variance\"] = np.nan\n",
    "    df[\"directional_cosine_sim_difference\"] = np.nan\n",
    "    df[\"directional_cosine_sim_mean\"] = np.nan\n",
    "    logger.info(\"Computing the uncertainties...\")\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=3)(\n",
    "        delayed(_directional_statistics_per_cell)(tensor[:, cell_index, :])\n",
    "        for cell_index in range(n_cells)\n",
    "    )\n",
    "    # cells by samples\n",
    "    cosine_sims = np.stack([results[i][0] for i in range(n_cells)])\n",
    "    df.loc[:, \"directional_cosine_sim_variance\"] = [\n",
    "        results[i][1] for i in range(n_cells)\n",
    "    ]\n",
    "    df.loc[:, \"directional_cosine_sim_difference\"] = [\n",
    "        results[i][2] for i in range(n_cells)\n",
    "    ]\n",
    "    df.loc[:, \"directional_variance\"] = [results[i][3] for i in range(n_cells)]\n",
    "    df.loc[:, \"directional_difference\"] = [results[i][4] for i in range(n_cells)]\n",
    "    df.loc[:, \"directional_cosine_sim_mean\"] = [results[i][5] for i in range(n_cells)]\n",
    "\n",
    "    return df, cosine_sims\n",
    "\n",
    "\n",
    "def _directional_statistics_per_cell(\n",
    "    tensor: np.ndarray,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Internal function for parallelization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor\n",
    "        Shape of samples by genes for a given cell.\n",
    "    \"\"\"\n",
    "    n_samples = tensor.shape[0]\n",
    "    # over samples axis\n",
    "    mean_velocity_of_cell = tensor.mean(0)\n",
    "    cosine_sims = [\n",
    "        _cosine_sim(tensor[i, :], mean_velocity_of_cell) for i in range(n_samples)\n",
    "    ]\n",
    "    angle_samples = [np.arccos(el) for el in cosine_sims]\n",
    "    return (\n",
    "        cosine_sims,\n",
    "        np.var(cosine_sims),\n",
    "        np.percentile(cosine_sims, 95) - np.percentile(cosine_sims, 5),\n",
    "        np.var(angle_samples),\n",
    "        np.percentile(angle_samples, 95) - np.percentile(angle_samples, 5),\n",
    "        np.mean(cosine_sims),\n",
    "    )\n",
    "\n",
    "\n",
    "def _centered_unit_vector(vector: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns the centered unit vector of the vector.\"\"\"\n",
    "    vector = vector - np.mean(vector)\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "\n",
    "def _cosine_sim(v1: np.ndarray, v2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns cosine similarity of the vectors.\"\"\"\n",
    "    v1_u = _centered_unit_vector(v1)\n",
    "    v2_u = _centered_unit_vector(v2)\n",
    "    return np.clip(np.dot(v1_u, v2_u), -1.0, 1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scvi import train\n",
    "\n",
    "class ProxGroupLasso:\n",
    "    def __init__(self, alpha, omega=None, inplace=True):\n",
    "    # omega - vector of coefficients with size\n",
    "    # equal to the number of groups\n",
    "        if omega is None:\n",
    "            self._group_coeff = alpha\n",
    "        else:\n",
    "            self._group_coeff = (omega*alpha).view(-1)\n",
    "\n",
    "        # to check for update\n",
    "        self._alpha = alpha\n",
    "\n",
    "        self._inplace = inplace\n",
    "\n",
    "    def __call__(self, W):\n",
    "        if not self._inplace:\n",
    "            W = W.clone()\n",
    "\n",
    "        norm_vect = W.norm(p=2, dim=0)\n",
    "        norm_g_gr_vect = norm_vect>self._group_coeff\n",
    "\n",
    "        scaled_norm_vector = norm_vect/self._group_coeff\n",
    "        scaled_norm_vector+=(~(scaled_norm_vector>0)).float()\n",
    "\n",
    "        W-=W/scaled_norm_vector\n",
    "        W*=norm_g_gr_vect.float()\n",
    "\n",
    "        return W\n",
    "\n",
    "\n",
    "class ProxL1:\n",
    "    def __init__(self, alpha, I=None, inplace=True):\n",
    "        self._I = ~I.bool() if I is not None else None\n",
    "        self._alpha=alpha\n",
    "        self._inplace=inplace\n",
    "\n",
    "    def __call__(self, W):\n",
    "        if not self._inplace:\n",
    "            W = W.clone()\n",
    "\n",
    "        W_geq_alpha = W>=self._alpha\n",
    "        W_leq_neg_alpha = W<=-self._alpha\n",
    "        W_cond_joint = ~W_geq_alpha&~W_leq_neg_alpha\n",
    "\n",
    "        if self._I is not None:\n",
    "            W_geq_alpha &= self._I\n",
    "            W_leq_neg_alpha &= self._I\n",
    "            W_cond_joint &= self._I\n",
    "\n",
    "        W -= W_geq_alpha.float()*self._alpha\n",
    "        W += W_leq_neg_alpha.float()*self._alpha\n",
    "        W -= W_cond_joint.float()*W\n",
    "\n",
    "        return W\n",
    "\n",
    "class CustomTrainingPlan(train.TrainingPlan):\n",
    "    def __init__(self, \n",
    "            model,\n",
    "            alpha,\n",
    "            omega=None,\n",
    "            alpha_l1=None,\n",
    "            alpha_l1_epoch_anneal=None,\n",
    "            alpha_l1_anneal_each=5,\n",
    "            gamma_ext=None,\n",
    "            gamma_epoch_anneal=None,\n",
    "            gamma_anneal_each=5,\n",
    "            beta=1.,\n",
    "            print_stats=False,\n",
    "            **kwargs):\n",
    "        super().__init__(model, **kwargs)\n",
    "\n",
    "        self.model=model\n",
    "        self.print_stats = print_stats\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.omega = omega\n",
    "\n",
    "        if self.omega is not None:\n",
    "            self.omega = self.omega.to(self.device)\n",
    "\n",
    "        # self.gamma_ext = gamma_ext\n",
    "        # self.gamma_epoch_anneal = gamma_epoch_anneal\n",
    "        # self.gamma_anneal_each = gamma_anneal_each\n",
    "\n",
    "        self.alpha_l1 = alpha_l1\n",
    "        self.alpha_l1_epoch_anneal = alpha_l1_epoch_anneal\n",
    "        self.alpha_l1_anneal_each = alpha_l1_anneal_each\n",
    "\n",
    "        # if self.model.use_hsic:\n",
    "        #     self.beta = beta\n",
    "        # else:\n",
    "        #     self.beta = None\n",
    "\n",
    "        self.watch_lr = None\n",
    "\n",
    "        self.use_prox_ops = self.check_prox_ops()\n",
    "        self.prox_ops = {}\n",
    "\n",
    "        self.corr_coeffs = self.init_anneal()\n",
    "\n",
    "        print(f\"init corr coeffs: {self.corr_coeffs}\")\n",
    "\n",
    "    def check_prox_ops(self):\n",
    "        use_prox_ops = {}\n",
    "\n",
    "        use_main = self.model.decoder.L0.expr_L.weight.requires_grad\n",
    "\n",
    "        use_prox_ops['main_group_lasso'] = use_main and self.alpha is not None\n",
    "\n",
    "        use_mask = use_main and self.model.mask is not None\n",
    "        use_prox_ops['main_soft_mask'] = use_mask and self.alpha_l1 is not None\n",
    "\n",
    "        # use_ext_m = self.model.n_ext_m_decoder > 0 and self.alpha_l1 is not None\n",
    "        # use_ext_m = use_ext_m and self.model.decoder.L0.ext_L_m.weight.requires_grad\n",
    "        # use_prox_ops['ext_soft_mask'] = use_ext_m and self.model.ext_mask is not None\n",
    "\n",
    "        return use_prox_ops\n",
    "\n",
    "    def init_anneal(self):\n",
    "        corr_coeffs = {}\n",
    "\n",
    "        use_soft_mask = self.use_prox_ops['main_soft_mask'] #or self.use_prox_ops['ext_soft_mask']\n",
    "        if use_soft_mask and self.alpha_l1_epoch_anneal is not None:\n",
    "            corr_coeffs['alpha_l1'] = 1. / self.alpha_l1_epoch_anneal\n",
    "        else:\n",
    "            corr_coeffs['alpha_l1'] = 1.\n",
    "\n",
    "        # if self.use_prox_ops['ext_unannot_l1'] and self.gamma_epoch_anneal is not None:\n",
    "        #     corr_coeffs['gamma_ext'] = 1. / self.gamma_epoch_anneal\n",
    "        # else:\n",
    "        #     corr_coeffs['gamma_ext'] = 1.\n",
    "\n",
    "        return corr_coeffs\n",
    "\n",
    "    def anneal(self):\n",
    "        any_change = False\n",
    "\n",
    "        # if self.corr_coeffs['gamma_ext'] < 1.:\n",
    "        #     any_change = True\n",
    "        #     time_to_anneal = self.epoch > 0 and self.epoch % self.gamma_anneal_each == 0\n",
    "        #     if time_to_anneal:\n",
    "        #         self.corr_coeffs['gamma_ext'] = min(self.epoch / self.gamma_epoch_anneal, 1.)\n",
    "        #         if self.print_stats:\n",
    "        #             print('New gamma_ext anneal coefficient:', self.corr_coeffs['gamma_ext'])\n",
    "\n",
    "        if self.corr_coeffs['alpha_l1'] < 1.:\n",
    "            any_change = True\n",
    "            time_to_anneal = self.epoch > 0 and self.epoch % self.self.alpha_l1_anneal_each == 0\n",
    "            if time_to_anneal:\n",
    "                self.corr_coeffs['alpha_l1'] = min(self.epoch / self.alpha_l1_epoch_anneal, 1.)\n",
    "                if self.print_stats:\n",
    "                    print('New alpha_l1 anneal coefficient:', self.corr_coeffs['alpha_l1'])\n",
    "\n",
    "        return any_change\n",
    "\n",
    "    def init_prox_ops(self):\n",
    "        if any(self.use_prox_ops.values()) and self.watch_lr is None:\n",
    "            self.watch_lr = self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "        if 'main_group_lasso' not in self.prox_ops and self.use_prox_ops['main_group_lasso']:\n",
    "            print('Init the group lasso proximal operator for the main terms.')\n",
    "            alpha_corr = self.alpha * self.watch_lr\n",
    "            self.prox_ops['main_group_lasso'] = ProxGroupLasso(alpha_corr, self.omega)\n",
    "\n",
    "        if 'main_soft_mask' not in self.prox_ops and self.use_prox_ops['main_soft_mask']:\n",
    "            print('Init the soft mask proximal operator for the main terms.')\n",
    "            main_mask = self.model.mask.to(self.device)\n",
    "            alpha_l1_corr = self.alpha_l1 * self.watch_lr * self.corr_coeffs['alpha_l1']\n",
    "            self.prox_ops['main_soft_mask'] = ProxL1(alpha_l1_corr, main_mask)\n",
    "\n",
    "        # if 'ext_unannot_l1' not in self.prox_ops and self.use_prox_ops['ext_unannot_l1']:\n",
    "        #     print('Init the L1 proximal operator for the unannotated extension.')\n",
    "        #     gamma_ext_corr = self.gamma_ext * self.watch_lr * self.corr_coeffs['gamma_ext']\n",
    "        #     self.prox_ops['ext_unannot_l1'] = ProxL1(gamma_ext_corr)\n",
    "\n",
    "        # if 'ext_soft_mask' not in self.prox_ops and self.use_prox_ops['ext_soft_mask']:\n",
    "        #     print('Init the soft mask proximal operator for the annotated extension.')\n",
    "        #     ext_mask = self.model.ext_mask.to(self.device)\n",
    "        #     alpha_l1_corr = self.alpha_l1 * self.watch_lr * self.corr_coeffs['alpha_l1']\n",
    "        #     self.prox_ops['ext_soft_mask'] = ProxL1(alpha_l1_corr, ext_mask)\n",
    "\n",
    "    def update_prox_ops(self):\n",
    "        if 'main_group_lasso' in self.prox_ops:\n",
    "            alpha_corr = self.alpha * self.watch_lr\n",
    "            if self.prox_ops['main_group_lasso']._alpha != alpha_corr:\n",
    "                self.prox_ops['main_group_lasso'] = ProxGroupLasso(alpha_corr, self.omega)\n",
    "\n",
    "        # if 'ext_unannot_l1' in self.prox_ops:\n",
    "        #     gamma_ext_corr = self.gamma_ext * self.watch_lr * self.corr_coeffs['gamma_ext']\n",
    "        #     if self.prox_ops['ext_unannot_l1']._alpha != gamma_ext_corr:\n",
    "        #         self.prox_ops['ext_unannot_l1']._alpha = gamma_ext_corr\n",
    "\n",
    "        for mask_key in ('main_soft_mask'):#, 'ext_soft_mask'):\n",
    "            if mask_key in self.prox_ops:\n",
    "                alpha_l1_corr = self.alpha_l1 * self.watch_lr * self.corr_coeffs['alpha_l1']\n",
    "                if self.prox_ops[mask_key]._alpha != alpha_l1_corr:\n",
    "                    self.prox_ops[mask_key]._alpha = alpha_l1_corr\n",
    "\n",
    "    def apply_prox_ops(self):\n",
    "        if 'main_soft_mask' in self.prox_ops:\n",
    "            self.prox_ops['main_soft_mask'](self.model.decoder.L0.expr_L.weight.data)\n",
    "        if 'main_group_lasso' in self.prox_ops:\n",
    "            self.prox_ops['main_group_lasso'](self.model.decoder.L0.expr_L.weight.data)\n",
    "        # if 'ext_unannot_l1' in self.prox_ops:\n",
    "        #     self.prox_ops['ext_unannot_l1'](self.model.decoder.L0.ext_L.weight.data)\n",
    "        # if 'ext_soft_mask' in self.prox_ops:\n",
    "        #     self.prox_ops['ext_soft_mask'](self.model.decoder.L0.ext_L_m.weight.data)\n",
    "\n",
    "        def training_step(self, batch, batch_idx, optimizer_idx=0):\n",
    "            self.init_prox_ops()\n",
    "            super().training_step(batch, batch_idx, optimizer_idx=0)\n",
    "            self.apply_prox_ops()\n",
    "            print(\"applied prox ops\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get AnnData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = scv.datasets.pancreas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 21611 genes that are detected 30 counts (shared).\n",
      "Normalized count data: X, spliced, unspliced.\n",
      "Extracted 2000 highly variable genes.\n",
      "Logarithmized X.\n",
      "computing neighbors\n",
      "    finished (0:00:19) --> added \n",
      "    'distances' and 'connectivities', weighted adjacency matrices (adata.obsp)\n",
      "computing moments based on connectivities\n",
      "    finished (0:00:01) --> added \n",
      "    'Ms' and 'Mu', moments of un/spliced abundances (adata.layers)\n"
     ]
    }
   ],
   "source": [
    "scv.pp.filter_and_normalize(adata, min_shared_counts=30, n_top_genes=2000)\n",
    "scv.pp.moments(adata, n_pcs=30, n_neighbors=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing velocities\n",
      "    finished (0:00:00) --> added \n",
      "    'velocity', velocity vectors for each individual cell (adata.layers)\n"
     ]
    }
   ],
   "source": [
    "adata = preprocess_data(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1136LntaVr92G1MphGeMVcmpE0AqcqM6c\n",
      "To: /home/chels/thesis/repos/velo_interpret/velovi/reactome.gmt\n",
      "100%|██████████| 331k/331k [00:00<00:00, 1.84MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'reactome.gmt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "url = 'https://drive.google.com/uc?id=1136LntaVr92G1MphGeMVcmpE0AqcqM6c'\n",
    "output = 'reactome.gmt'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scarches as sca\n",
    "sca.utils.add_annotations(adata, 'reactome.gmt', min_genes=12, clean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all genes not present in annotations\n",
    "\n",
    "adata._inplace_subset_var(adata.varm['I'].sum(1)>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out terms with less than 12 genes \n",
    "\n",
    "select_terms = adata.varm['I'].sum(0)>12\n",
    "adata.uns['terms'] = np.array(adata.uns['terms'])[select_terms].tolist()\n",
    "adata.varm['I'] = adata.varm['I'][:, select_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use only highly variable genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\twith soft mask.\n",
      "Last Decoder layer: softmax\n"
     ]
    }
   ],
   "source": [
    "VELOVI.setup_anndata(adata, spliced_layer=\"Ms\", unspliced_layer=\"Mu\")\n",
    "vae = VELOVI(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init corr coeffs: {'alpha_l1': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/500:   0%|          | 2/500 [03:23<14:35:19, 105.46s/it, loss=655, v_num=1]"
     ]
    }
   ],
   "source": [
    "vae.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('thesis': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10d1bd7936c97bab4011aab90821338c12e0c1d0b82a3498f3b9832575200244"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
